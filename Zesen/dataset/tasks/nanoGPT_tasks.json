{
  "tasks": [
    {
      "task_title": "Prepare Shakespeare Character Dataset",
      "task_description": "准备莎士比亚字符数据集以供训练使用。",
      "example_code": null,
      "running_command": "python data/shakespeare_char/prepare.py",
      "expected_input": null,
      "expected_output": "Dataset prepared successfully."
    },
    {
      "task_title": "Train on Shakespeare Character Dataset",
      "task_description": "使用莎士比亚字符数据集进行模型训练。",
      "example_code": null,
      "running_command": "python train.py config/train_shakespeare_char.py",
      "expected_input": null,
      "expected_output": "Training completed."
    },
    {
      "task_title": "Sample from Trained Model",
      "task_description": "从训练好的模型中生成样本。",
      "example_code": null,
      "running_command": "python sample.py --out_dir=out-shakespeare-char",
      "expected_input": null,
      "expected_output": "Generated samples saved to out-shakespeare-char."
    },
    {
      "task_title": "Train with Custom Parameters",
      "task_description": "使用自定义参数进行莎士比亚字符数据集的训练。",
      "example_code": null,
      "running_command": "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0",
      "expected_input": null,
      "expected_output": "Training completed with custom parameters."
    },
    {
      "task_title": "Sample from Model on CPU",
      "task_description": "在CPU上从训练好的模型中生成样本。",
      "example_code": null,
      "running_command": "python sample.py --out_dir=out-shakespeare-char --device=cpu",
      "expected_input": null,
      "expected_output": "Generated samples saved to out-shakespeare-char."
    },
    {
      "task_title": "Prepare OpenWebText Dataset",
      "task_description": "准备OpenWebText数据集以供训练使用。",
      "example_code": null,
      "running_command": "python data/openwebtext/prepare.py",
      "expected_input": null,
      "expected_output": "Dataset prepared successfully."
    },
    {
      "task_title": "Distributed Training with Torchrun",
      "task_description": "在分布式环境下使用Torchrun进行训练。",
      "example_code": null,
      "running_command": "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py",
      "expected_input": null,
      "expected_output": "Distributed training started."
    },
    {
      "task_title": "Distributed Training on Multiple Nodes",
      "task_description": "在多个节点上进行分布式训练。",
      "example_code": null,
      "running_command": "torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py",
      "expected_input": null,
      "expected_output": "Distributed training started on master node."
    },
    {
      "task_title": "Evaluate GPT-2 Models",
      "task_description": "评估不同大小的GPT-2模型。",
      "example_code": null,
      "running_command": "$ python train.py config/eval_gpt2.py\n$ python train.py config/eval_gpt2_medium.py\n$ python train.py config/eval_gpt2_large.py\n$ python train.py config/eval_gpt2_xl.py",
      "expected_input": null,
      "expected_output": "Evaluation completed for all specified models."
    },
    {
      "task_title": "Fine-tune on Shakespeare Dataset",
      "task_description": "对莎士比亚数据集进行微调训练。",
      "example_code": null,
      "running_command": "python train.py config/finetune_shakespeare.py",
      "expected_input": null,
      "expected_output": "Fine-tuning completed."
    },
    {
      "task_title": "Sample from Fine-tuned Model",
      "task_description": "从微调后的模型中生成样本。",
      "example_code": null,
      "running_command": "python sample.py --init_from=gpt2-xl --start=\"What is the answer to life, the universe, and everything?\" --num_samples=5 --max_new_tokens=100",
      "expected_input": "What is the answer to life, the universe, and everything?",
      "expected_output": "Generated text based on the prompt."
    }
  ],
  "setup": {
    "setup_commands": [],
    "docker_commands": [],
    "docker_files": {},
    "docker_setup_descriptions": null
  },
  "input_to_gpt": {
    "repo_name": "karpathy/nanoGPT",
    "num_code_blocks": 11,
    "total_length": 1284,
    "code_blocks": [
      "python data/shakespeare_char/prepare.py",
      "python train.py config/train_shakespeare_char.py",
      "python sample.py --out_dir=out-shakespeare-char",
      "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0",
      "python sample.py --out_dir=out-shakespeare-char --device=cpu",
      "python data/openwebtext/prepare.py",
      "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py",
      "# Run on the first (master) node with example IP 123.456.123.456:\ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n# Run on the worker node:\ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py",
      "$ python train.py config/eval_gpt2.py\n$ python train.py config/eval_gpt2_medium.py\n$ python train.py config/eval_gpt2_large.py\n$ python train.py config/eval_gpt2_xl.py",
      "python train.py config/finetune_shakespeare.py",
      "python sample.py \\\n    --init_from=gpt2-xl \\\n    --start=\"What is the answer to life, the universe, and everything?\" \\\n    --num_samples=5 --max_new_tokens=100"
    ]
  }
}