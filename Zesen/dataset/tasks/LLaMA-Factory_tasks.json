{
  "tasks": [
    {
      "task_title": "Sync extra components",
      "task_description": "Synchronizes additional components like torch and metrics with a pre-release option.",
      "example_code": null,
      "running_command": "uv sync --extra torch --extra metrics --prerelease=allow",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Train model with LoRA pretraining",
      "task_description": "Runs the training process using a configuration file for LoRA pretraining.",
      "example_code": null,
      "running_command": "uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Train model with LoRA SFT",
      "task_description": "Runs the training process using a configuration file for LoRA SFT.",
      "example_code": null,
      "running_command": "llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Chat with the trained model",
      "task_description": "Starts a chat session using the trained model with the specified configuration.",
      "example_code": null,
      "running_command": "llamafactory-cli chat examples/inference/llama3_lora_sft.yaml",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Export trained model",
      "task_description": "Exports the trained model using the specified configuration.",
      "example_code": null,
      "running_command": "llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Start web UI",
      "task_description": "Launches the web user interface for interacting with the model.",
      "example_code": null,
      "running_command": "llamafactory-cli webui",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Run Docker for CUDA",
      "task_description": "Starts the Docker container for CUDA support.",
      "example_code": null,
      "running_command": "cd docker/docker-cuda/ && docker compose up -d && docker compose exec llamafactory bash",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Run Docker for NPU",
      "task_description": "Starts the Docker container for NPU support.",
      "example_code": null,
      "running_command": "cd docker/docker-npu/ && docker compose up -d && docker compose exec llamafactory bash",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Run Docker for ROCm",
      "task_description": "Starts the Docker container for ROCm support.",
      "example_code": null,
      "running_command": "cd docker/docker-rocm/ && docker compose up -d && docker compose exec llamafactory bash",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Run inference API",
      "task_description": "Starts an inference API with specified configuration and options.",
      "example_code": null,
      "running_command": "API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Set ModelScope Hub environment variable",
      "task_description": "Sets an environment variable to use the ModelScope Hub.",
      "example_code": null,
      "running_command": "export USE_MODELSCOPE_HUB=1",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Set OpenMind Hub environment variable",
      "task_description": "Sets an environment variable to use the OpenMind Hub.",
      "example_code": null,
      "running_command": "export USE_OPENMIND_HUB=1",
      "expected_input": null,
      "expected_output": null
    }
  ],
  "setup": {
    "setup_commands": [
      "pip install --upgrade huggingface_hub\nhuggingface-cli login",
      "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation",
      "pip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"",
      "pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl",
      "# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh",
      "# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .",
      "git clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install ."
    ],
    "docker_commands": [
      "docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest",
      "docker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash",
      "docker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash",
      "docker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash"
    ],
    "docker_files": {
      ".dockerignore": ".vscode\n.git\n.github\n.venv\ncache\ndocker\nsaves\nhf_cache\nms_cache\nom_cache\nshared_data\noutput\n.dockerignore\n.gitattributes\n.gitignore\n"
    },
    "docker_setup_descriptions": {
      ".dockerignore": "To build and run the Docker image for the `hiyouga/LLaMA-Factory` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image created from this repository is designed to facilitate the development and deployment of the LLaMA-Factory project, which is likely focused on machine learning or natural language processing tasks. The image includes necessary dependencies for running the application efficiently, particularly leveraging GPU resources for enhanced performance.\n\n### Building the Image\n1. **Prepare Your Environment**: Ensure you have Docker installed on your machine. You should also have access to a terminal or command prompt.\n\n2. **Clone the Repository**: Clone the `hiyouga/LLaMA-Factory` repository to your local machine. This will provide you with the necessary files, including the Dockerfile and the `.dockerignore` file.\n\n3. **Understand the `.dockerignore` File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This helps keep the image lightweight and avoids unnecessary files being included. The entries in this file include common development artifacts like `.git`, `.venv`, and cache directories.\n\n4. **Build the Docker Image**: Navigate to the directory containing the Dockerfile. Use the Docker build command to create the image. The build process will utilize the specified Dockerfile and any build arguments necessary for your environment (e.g., package index or additional metrics).\n\n5. **Tag the Image**: During the build process, tag the image appropriately (e.g., `llamafactory:latest`) to easily reference it later.\n\n### Running the Image\n1. **Run the Container**: Once the image is built, you can run it using Docker. Use the `docker run` command to start a new container from the image. Ensure to allocate the necessary resources, such as GPU access and specific ports for application interaction (e.g., 7860 and 8000).\n\n2. **Interactive Shell Access**: If you need to interact with the container, you can execute a bash shell within the running container using the `docker exec` command. This allows you to run commands directly in the container environment.\n\n3. **Manage Resources**: Depending on your hardware and application requirements, you may need to adjust device mappings or volume mounts to ensure the container has access to necessary resources.\n\n### Conclusion\nBy following these steps, you will successfully build and run the Docker image for the LLaMA-Factory project. This setup allows for efficient development and testing of machine learning models, leveraging Docker's capabilities to manage dependencies and resources effectively."
    }
  },
  "input_to_gpt": {
    "repo_name": "hiyouga/LLaMA-Factory",
    "num_code_blocks": 10,
    "total_length": 907,
    "code_blocks": [
      "uv sync --extra torch --extra metrics --prerelease=allow",
      "uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml",
      "llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml",
      "llamafactory-cli webui",
      "cd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash",
      "cd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash",
      "cd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash",
      "API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true",
      "export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows",
      "export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows"
    ]
  }
}