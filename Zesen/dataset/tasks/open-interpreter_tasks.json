{
  "tasks": [
    {
      "task_title": "Chat with the interpreter",
      "task_description": "Executes a single command to chat with the interpreter.",
      "example_code": "interpreter.chat(\"Plot AAPL and META's normalized stock prices\")",
      "running_command": null,
      "expected_input": "Plot AAPL and META's normalized stock prices",
      "expected_output": "Generated plot of AAPL and META's normalized stock prices."
    },
    {
      "task_title": "Start an interactive chat",
      "task_description": "Starts an interactive chat session with the interpreter.",
      "example_code": "interpreter.chat()",
      "running_command": null,
      "expected_input": null,
      "expected_output": "Interactive chat session initiated."
    },
    {
      "task_title": "Streaming chat response",
      "task_description": "Streams chat responses for a given message.",
      "example_code": "for chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)",
      "running_command": null,
      "expected_input": "What operating system are we on?",
      "expected_output": "Chunked response about the operating system."
    },
    {
      "task_title": "Add subtitles to videos",
      "task_description": "Executes a command to add subtitles to all videos in a specified directory.",
      "example_code": "interpreter.chat(\"Add subtitles to all videos in /videos.\")",
      "running_command": null,
      "expected_input": "Add subtitles to all videos in /videos.",
      "expected_output": "Subtitles added to all videos in /videos."
    },
    {
      "task_title": "Reset and resume chat messages",
      "task_description": "Resets the interpreter's messages and resumes from a saved state.",
      "example_code": "messages = interpreter.chat(\"My name is Killian.\")\ninterpreter.messages = []\ninterpreter.messages = messages",
      "running_command": null,
      "expected_input": "My name is Killian.",
      "expected_output": "Interpreter remembers the name 'Killian'."
    },
    {
      "task_title": "Update system message",
      "task_description": "Updates the system message with additional instructions.",
      "example_code": "interpreter.system_message += \"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"",
      "running_command": null,
      "expected_input": null,
      "expected_output": "System message updated."
    },
    {
      "task_title": "Set model for interpreter",
      "task_description": "Sets the model used by the interpreter.",
      "example_code": "interpreter.llm.model = \"gpt-3.5-turbo\"",
      "running_command": null,
      "expected_input": null,
      "expected_output": "Model set to gpt-3.5-turbo."
    },
    {
      "task_title": "Chat endpoint in FastAPI",
      "task_description": "Defines a chat endpoint that streams responses for a given message.",
      "example_code": "def chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")",
      "running_command": null,
      "expected_input": "What is the weather today?",
      "expected_output": "Streaming response for the weather inquiry."
    }
  ],
  "setup": {
    "setup_commands": [
      "pip install git+https://github.com/OpenInterpreter/open-interpreter.git",
      "pip install open-interpreter",
      "pip install fastapi uvicorn\nuvicorn server:app --reload"
    ],
    "docker_commands": [],
    "docker_files": {
      "Dockerfile": "###########################################################################################\n# This Dockerfile runs an LMC-compatible websocket server at / on port 8000.              #\n# To learn more about LMC, visit https://docs.openinterpreter.com/protocols/lmc-messages. #\n###########################################################################################\n\nFROM python:3.11.8\n\n# Set environment variables\n# ENV OPENAI_API_KEY ...\n\nENV HOST 0.0.0.0\n# ^ Sets the server host to 0.0.0.0, Required for the server to be accessible outside the container\n\n# Copy required files into container\nRUN mkdir -p interpreter scripts\nCOPY interpreter/ interpreter/\nCOPY scripts/ scripts/\nCOPY poetry.lock pyproject.toml README.md ./\n\n# Expose port 8000\nEXPOSE 8000\n\n# Install server dependencies\nRUN pip install \".[server]\"\n\n# Start the server\nENTRYPOINT [\"interpreter\", \"--server\"]"
    },
    "docker_setup_descriptions": {
      "Dockerfile": "### Building and Running Docker for openinterpreter/open-interpreter\n\nTo build and run the Docker container for the openinterpreter/open-interpreter project, follow these steps:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/openinterpreter/open-interpreter.git\n   cd open-interpreter\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image from the provided `Dockerfile`:\n   ```bash\n   docker build -t open-interpreter .\n   ```\n\n3. **Run the Docker Container**:\n   Start the container, mapping port 8000 of the container to port 8000 on your host:\n   ```bash\n   docker run -p 8000:8000 open-interpreter\n   ```\n\n### Semantic Meaning of the Dockerfile\n\n- **Base Image**: The Dockerfile starts with `FROM python:3.11.8`, indicating that it uses Python 3.11.8 as the base image, which is essential for running Python applications.\n\n- **Environment Variables**: \n  - `ENV HOST 0.0.0.0` sets the server host to `0.0.0.0`, allowing the server to be accessed from outside the Docker container.\n\n- **Directory Structure**: \n  - `RUN mkdir -p interpreter scripts` creates necessary directories within the container.\n  - `COPY` commands transfer files from the host into the container, ensuring that the application code and dependencies are available.\n\n- **Port Exposure**: \n  - `EXPOSE 8000` informs Docker that the container listens on port 8000 at runtime, which is crucial for accessing the websocket server.\n\n- **Dependency Installation**: \n  - `RUN pip install \".[server]\"` installs the server dependencies specified in the `pyproject.toml` and `poetry.lock` files, ensuring that the application has all necessary libraries.\n\n- **Entry Point**: \n  - `ENTRYPOINT [\"interpreter\", \"--server\"]` specifies the command that runs when the container starts, launching the LMC-compatible websocket server.\n\nThis Dockerfile effectively sets up an environment for running a websocket server that adheres to the LMC protocol, making it accessible for external connections."
    }
  },
  "input_to_gpt": {
    "repo_name": "openinterpreter/open-interpreter",
    "num_code_blocks": 21,
    "total_length": 2640,
    "code_blocks": [
      "interpreter",
      "interpreter",
      "from interpreter import interpreter\n\ninterpreter.chat(\"Plot AAPL and META's normalized stock prices\") # Executes a single command\ninterpreter.chat() # Starts an interactive chat",
      "message = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)",
      "interpreter",
      "interpreter.chat()",
      "message = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)",
      "interpreter.chat(\"Add subtitles to all videos in /videos.\")\n\n# ... Streams output to your terminal, completes task ...\n\ninterpreter.chat(\"These look great but can you make the subtitles bigger?\")\n\n# ...",
      "interpreter.messages = []",
      "messages = interpreter.chat(\"My name is Killian.\") # Save messages to 'messages'\ninterpreter.messages = [] # Reset interpreter (\"Killian\" will be forgotten)\n\ninterpreter.messages = messages # Resume chat from 'messages' (\"Killian\" will be remembered)",
      "interpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)",
      "interpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly",
      "interpreter.llm.model = \"gpt-3.5-turbo\"",
      "interpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"",
      "interpreter --local",
      "from interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires this\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI compatible server\n\ninterpreter.chat()",
      "interpreter --local --max_tokens 1000 --context_window 3000",
      "$ interpreter\n...\n> %verbose true <- Turns on verbose mode\n\n> %verbose false <- Turns off verbose mode",
      "# server.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom interpreter import interpreter\n\napp = FastAPI()\n\n@app.get(\"/chat\")\ndef chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\n@app.get(\"/history\")\ndef history_endpoint():\n    return interpreter.messages",
      "npm i -g mintlify@latest",
      "# Assuming you're at the project's root directory\ncd ./docs\n\n# Run the documentation server\nmintlify dev"
    ]
  }
}