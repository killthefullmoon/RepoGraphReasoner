{
  "tasks": [
    {
      "task_title": "运行图形用户界面",
      "task_description": "启动g4f库的图形用户界面。",
      "example_code": "from g4f.gui import run_gui\nrun_gui()",
      "running_command": null,
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "启动CLI工具图形界面",
      "task_description": "通过CLI启动g4f库的图形用户界面，指定端口和调试模式。",
      "example_code": null,
      "running_command": "python -m g4f.cli gui --port 8080 --debug",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "启动CLI工具",
      "task_description": "通过CLI启动g4f库，指定端口和调试模式。",
      "example_code": null,
      "running_command": "python -m g4f --port 8080 --debug",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "聊天生成",
      "task_description": "使用g4f库的客户端生成聊天回复，指定模型和用户消息。",
      "example_code": "from g4f.client import Client\n\nclient = Client()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    web_search=False\n)\nprint(response.choices[0].message.content)",
      "running_command": null,
      "expected_input": "Hello, how are you?",
      "expected_output": "..."
    },
    {
      "task_title": "图像生成",
      "task_description": "使用g4f库的客户端生成图像，指定模型和提示内容。",
      "example_code": "from g4f.client import Client\n\nclient = Client()\nresponse = client.images.generate(\n    model=\"flux\",\n    prompt=\"a white siamese cat\",\n    response_format=\"url\"\n)\nprint(f\"Generated image URL: {response.data[0].url}\")",
      "running_command": null,
      "expected_input": "a white siamese cat",
      "expected_output": "Generated image URL: ..."
    },
    {
      "task_title": "异步聊天生成",
      "task_description": "使用g4f库的异步客户端生成聊天回复，指定模型和用户消息。",
      "example_code": "from g4f.client import AsyncClient\nimport asyncio\n\nasync def main():\n    client = AsyncClient()\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing briefly\"}],\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())",
      "running_command": null,
      "expected_input": "Explain quantum computing briefly",
      "expected_output": "..."
    }
  ],
  "setup": {
    "setup_commands": [
      "pip install -U g4f[all]",
      "git clone https://github.com/xtekky/gpt4free.git\ncd gpt4free\npip install -r requirements.txt\npip install -e .",
      "pip install -U g4f[all]"
    ],
    "docker_commands": [
      "   docker pull hlohaus789/g4f",
      "   docker run -p 8080:8080 -p 7900:7900 \\\n     --shm-size=\"2g\" \\\n     -v ${PWD}/har_and_cookies:/app/har_and_cookies \\\n     -v ${PWD}/generated_media:/app/generated_media \\\n     hlohaus789/g4f:latest",
      "mkdir -p ${PWD}/har_and_cookies ${PWD}/generated_media\nchown -R 1000:1000 ${PWD}/har_and_cookies ${PWD}/generated_media\n\ndocker run \\\n  -p 1337:8080 -p 8080:8080 \\\n  -v ${PWD}/har_and_cookies:/app/har_and_cookies \\\n  -v ${PWD}/generated_media:/app/generated_media \\\n  hlohaus789/g4f:latest-slim"
    ],
    "docker_files": {
      "docker-compose.yml": "version: '3'\n\nservices:\n  gpt4free:\n    image: hlohaus789/g4f:latest\n    shm_size: 2gb\n    build:\n      context: .\n      dockerfile: docker/Dockerfile\n    volumes:\n      - .:/app\n    ports:\n      - '8080:8080'\n      - '1337:8080'\n      - '7900:7900'\n    environment:\n      - OLLAMA_HOST=host.docker.internal\n"
    },
    "docker_setup_descriptions": {
      "docker-compose.yml": "### Building and Running Docker with `docker-compose.yml` for `xtekky/gpt4free`\n\nTo build and run the Docker container for the `gpt4free` service using the provided `docker-compose.yml` file, follow these steps:\n\n1. **Understanding the `docker-compose.yml` File:**\n   - **Version:** Specifies the Docker Compose file format version (3 in this case).\n   - **Services:** Defines the services that will be run. Here, we have a single service named `gpt4free`.\n     - **Image:** Specifies the Docker image to use (`hlohaus789/g4f:latest`).\n     - **shm_size:** Allocates shared memory size (2GB) for the container, which is important for applications that require significant memory.\n     - **Build:** Indicates that the service should be built from a Dockerfile located in the `docker` directory of the current context.\n     - **Volumes:** Mounts the current directory (`.`) to `/app` in the container, allowing for persistent data storage and access to local files.\n     - **Ports:** Maps host ports to container ports:\n       - `8080` on the host to `8080` in the container (main service).\n       - `1337` on the host to `8080` in the container (alternative access).\n       - `7900` on the host to `7900` in the container (additional service).\n     - **Environment:** Sets the environment variable `OLLAMA_HOST` to `host.docker.internal`, enabling the container to access the host machine.\n\n2. **Building and Running the Docker Container:**\n   - **Prerequisites:** Ensure you have Docker and Docker Compose installed on your machine.\n   - **Clone the Repository:**\n     ```bash\n     git clone https://github.com/xtekky/gpt4free.git\n     cd gpt4free\n     ```\n   - **Create Required Directories:**\n     ```bash\n     mkdir -p ${PWD}/har_and_cookies ${PWD}/generated_media\n     chown -R 1000:1000 ${PWD}/har_and_cookies ${PWD}/generated_media\n     ```\n   - **Build and Start the Service:**\n     ```bash\n     docker-compose up --build\n     ```\n\n3. **Accessing the Application:**\n   - Once the container is running, you can access the application via:\n     - `http://localhost:8080` for the main service.\n     - `http://localhost:1337` for the alternative access.\n     - `http://localhost:7900` for the additional service.\n\nBy following these steps, you will successfully build and run the `gpt4free` service using Docker Compose, allowing you to leverage the functionalities provided by the application."
    }
  },
  "input_to_gpt": {
    "repo_name": "xtekky/gpt4free",
    "num_code_blocks": 8,
    "total_length": 1137,
    "code_blocks": [
      "     mkdir -p ${PWD}/har_and_cookies ${PWD}/generated_media\n     sudo chown -R 1200:1201 ${PWD}/har_and_cookies ${PWD}/generated_media",
      "from g4f.gui import run_gui\nrun_gui()",
      "python -m g4f.cli gui --port 8080 --debug",
      "python -m g4f --port 8080 --debug",
      "python -m g4f.cli gui --port 8080 --debug",
      "from g4f.client import Client\n\nclient = Client()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    web_search=False\n)\nprint(response.choices[0].message.content)",
      "from g4f.client import Client\n\nclient = Client()\nresponse = client.images.generate(\n    model=\"flux\",\n    prompt=\"a white siamese cat\",\n    response_format=\"url\"\n)\nprint(f\"Generated image URL: {response.data[0].url}\")",
      "from g4f.client import AsyncClient\nimport asyncio\n\nasync def main():\n    client = AsyncClient()\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing briefly\"}],\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())"
    ]
  }
}