{"repo_name": "Significant-Gravitas/AutoGPT", "stars": 179320, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†", "task_description": "ä½¿ç”¨ agent å‘½ä»¤æ¥åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†å®ä¾‹ã€‚", "example_code": null, "running_command": "cli.py agent", "expected_input": null, "expected_output": "ä»£ç†å·²åˆ›å»º/å¯åŠ¨/åœæ­¢çš„ç›¸å…³ä¿¡æ¯"}, {"task_title": "åŸºå‡†æµ‹è¯•", "task_description": "ä½¿ç”¨ benchmark å‘½ä»¤æ¥å¯åŠ¨åŸºå‡†æµ‹è¯•å¹¶åˆ—å‡ºæµ‹è¯•å’Œç±»åˆ«ã€‚", "example_code": null, "running_command": "cli.py benchmark", "expected_input": null, "expected_output": "åŸºå‡†æµ‹è¯•ç»“æœå’Œç±»åˆ«ä¿¡æ¯"}, {"task_title": "å®‰è£…ä¾èµ–", "task_description": "ä½¿ç”¨ setup å‘½ä»¤æ¥å®‰è£…ç³»ç»Ÿæ‰€éœ€çš„ä¾èµ–ã€‚", "example_code": null, "running_command": "cli.py setup", "expected_input": null, "expected_output": "ä¾èµ–å®‰è£…æˆåŠŸçš„ç›¸å…³ä¿¡æ¯"}], "setup": {"setup_commands": ["curl -fsSL https://setup.agpt.co/install.sh -o install.sh && bash install.sh", "powershell -c \"iwr https://setup.agpt.co/install.bat -o install.bat; ./install.bat\""], "docker_commands": [], "has_docker_files": true}, "timestamp": "2025-10-27T21:05:44.926477"}
{"repo_name": "langflow-ai/langflow", "stars": 136272, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒLangflow", "task_description": "ä½¿ç”¨Langflowåº“çš„å‘½ä»¤è¡Œå·¥å…·æ¥å¯åŠ¨åº”ç”¨ã€‚", "example_code": null, "running_command": "uv run langflow run", "expected_input": null, "expected_output": "åº”ç”¨å·²å¯åŠ¨å¹¶è¿è¡Œ"}], "setup": {"setup_commands": ["uv pip install langflow -U"], "docker_commands": [], "has_docker_files": true}, "timestamp": "2025-10-27T21:06:05.431915"}
{"repo_name": "yt-dlp/yt-dlp", "stars": 132886, "language": "Python", "tasks": [{"task_title": "Always extract audio", "task_description": "è¿™ä¸ªä»»åŠ¡æå–è§†é¢‘çš„éŸ³é¢‘éƒ¨åˆ†ã€‚", "example_code": null, "running_command": "yt-dlp -x", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Extracting audio..."}, {"task_title": "Copy the mtime", "task_description": "è¿™ä¸ªä»»åŠ¡åœ¨ä¸‹è½½æ—¶ä¿ç•™æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´ã€‚", "example_code": null, "running_command": "yt-dlp --mtime", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "File mtime preserved."}, {"task_title": "Use a proxy", "task_description": "è¿™ä¸ªä»»åŠ¡é€šè¿‡æŒ‡å®šçš„ä»£ç†æœåŠ¡å™¨ä¸‹è½½è§†é¢‘ã€‚", "example_code": null, "running_command": "yt-dlp --proxy 127.0.0.1:3128", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Using proxy 127.0.0.1:3128"}, {"task_title": "Save videos in a specific directory", "task_description": "è¿™ä¸ªä»»åŠ¡å°†ä¸‹è½½çš„è§†é¢‘ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ã€‚", "example_code": null, "running_command": "yt-dlp -o ~/YouTube/%(title)s.%(ext)s", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Video saved to ~/YouTube/VideoTitle.ext"}, {"task_title": "Parse metadata for title", "task_description": "è¿™ä¸ªä»»åŠ¡è§£æè§†é¢‘çš„å…ƒæ•°æ®ï¼Œå°†æ ‡é¢˜æ ¼å¼åŒ–ä¸º 'Artist - Title'ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"title:%(artist)s - %(title)s\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Title set to 'Artist - Title'"}, {"task_title": "Regex for artist in description", "task_description": "è¿™ä¸ªä»»åŠ¡ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»æè¿°ä¸­æå–è‰ºæœ¯å®¶ä¿¡æ¯ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"description:Artist - (?P<artist>.+)\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Artist extracted from description"}, {"task_title": "Set series name in title", "task_description": "è¿™ä¸ªä»»åŠ¡å°†æ ‡é¢˜æ ¼å¼åŒ–ä¸º 'Series name S01E05'ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"%{series}s S%(season_number)02dE%(episode_number)02d:%(title)s\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Title set to 'Series name S01E05: Title'"}, {"task_title": "Prioritize uploader as artist", "task_description": "è¿™ä¸ªä»»åŠ¡å°†ä¸Šä¼ è€…ä¿¡æ¯ä½œä¸ºè§†é¢‘å…ƒæ•°æ®ä¸­çš„è‰ºæœ¯å®¶å­—æ®µã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"%{uploader|}s:%(meta_artist)s\" --embed-metadata", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Uploader prioritized as artist"}, {"task_title": "Set comment field from description", "task_description": "è¿™ä¸ªä»»åŠ¡å°†æè¿°ä¿¡æ¯è®¾ç½®ä¸ºè§†é¢‘å…ƒæ•°æ®ä¸­çš„è¯„è®ºå­—æ®µã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"description:(?s)(?P<meta_comment>.+)\" --embed-metadata", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Comment field set from description"}, {"task_title": "Do not set synopsis", "task_description": "è¿™ä¸ªä»»åŠ¡åœ¨è§†é¢‘å…ƒæ•°æ®ä¸­ä¸è®¾ç½®ä»»ä½•æ¦‚è¦ä¿¡æ¯ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \":(?P<meta_synopsis>)\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "No synopsis set in metadata"}, {"task_title": "Remove formats field from infojson", "task_description": "è¿™ä¸ªä»»åŠ¡å°†ä¿¡æ¯JSONä¸­çš„æ ¼å¼å­—æ®µè®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"video::(?P<formats>)\" --write-info-json", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Formats field removed from infojson"}, {"task_title": "Replace spaces in title and uploader", "task_description": "è¿™ä¸ªä»»åŠ¡å°†æ ‡é¢˜å’Œä¸Šä¼ è€…ä¸­çš„ç©ºæ ¼å’Œä¸‹åˆ’çº¿æ›¿æ¢ä¸ºçŸ­æ¨ªçº¿ã€‚", "example_code": null, "running_command": "yt-dlp --replace-in-metadata \"title,uploader\" \"[ _]\" \"-\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Spaces and underscores replaced in title and uploader"}], "setup": {"setup_commands": ["INSTALLATION", "curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import\ngpg --verify SHA2-256SUMS.sig SHA2-256SUMS\ngpg --verify SHA2-512SUMS.sig SHA2-512SUMS", "# To update to nightly from stable executable/binary:\nyt-dlp --update-to nightly\n\n# To install nightly with pip:\npython3 -m pip install -U --pre \"yt-dlp[default]\"", "$ yt-dlp --print filename -o \"test video.%(ext)s\" BaW_jenozKc\ntest video.webm    # Literal name with correct extension\n\n$ yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc\nyoutube-dl test video ''_Ã¤â†­ğ•.webm    # All kinds of weird characters\n\n$ yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc --restrict-filenames\nyoutube-dl_test_video_.webm    # Restricted file name\n\n# Download YouTube playlist videos in separate directory indexed by video order in a playlist\n$ yt-dlp -o \"%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n# Download YouTube playlist videos in separate directories according to their uploaded year\n$ yt-dlp -o \"%(upload_date>%Y)s/%(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n# Prefix playlist index with \" - \" separator, but only if it is available\n$ yt-dlp -o \"%(playlist_index&{} - |)s%(title)s.%(ext)s\" BaW_jenozKc \"https://www.youtube.com/user/TheLinuxFoundation/playlists\"\n\n# Download all playlists of YouTube channel/user keeping each playlist in separate directory:\n$ yt-dlp -o \"%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/user/TheLinuxFoundation/playlists\"\n\n# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home\n$ yt-dlp -u user -p password -P \"~/MyVideos\" -o \"%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s\" \"https://www.udemy.com/java-tutorial\"\n\n# Download entire series season keeping each series and each season in separate directory under C:/MyVideos\n$ yt-dlp -P \"C:/MyVideos\" -o \"%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s\" \"https://videomore.ru/kino_v_detalayah/5_sezon/367617\"\n\n# Download video as \"C:\\MyVideos\\uploader\\title.ext\", subtitles as \"C:\\MyVideos\\subs\\uploader\\title.ext\"\n# and put all temporary files in \"C:\\MyVideos\\tmp\"\n$ yt-dlp -P \"C:/MyVideos\" -P \"temp:tmp\" -P \"subtitle:subs\" -o \"%(uploader)s/%(title)s.%(ext)s\" BaW_jenozKc --write-subs\n\n# Download video as \"C:\\MyVideos\\uploader\\title.ext\" and subtitles as \"C:\\MyVideos\\uploader\\subs\\title.ext\"\n$ yt-dlp -P \"C:/MyVideos\" -o \"%(uploader)s/%(title)s.%(ext)s\" -o \"subtitle:%(uploader)s/subs/%(title)s.%(ext)s\" BaW_jenozKc --write-subs\n\n# Stream the video being downloaded to stdout\n$ yt-dlp -o - BaW_jenozKc", "# Download and merge the best video-only format and the best audio-only format,\n# or download the best combined format if video-only format is not available\n$ yt-dlp -f \"bv+ba/b\"\n\n# Download best format that contains video,\n# and if it doesn't already have an audio stream, merge it with best audio-only format\n$ yt-dlp -f \"bv*+ba/b\"\n\n# Same as above\n$ yt-dlp\n\n# Download the best video-only format and the best audio-only format without merging them\n# For this case, an output template should be used since\n# by default, bestvideo and bestaudio will have the same file name.\n$ yt-dlp -f \"bv,ba\" -o \"%(title)s.f%(format_id)s.%(ext)s\"\n\n# Download and merge the best format that has a video stream,\n# and all audio-only formats into one file\n$ yt-dlp -f \"bv*+mergeall[vcodec=none]\" --audio-multistreams\n\n# Download and merge the best format that has a video stream,\n# and the best 2 audio-only formats into one file\n$ yt-dlp -f \"bv*+ba+ba.2\" --audio-multistreams\n\n\n# The following examples show the old method (without -S) of format selection\n# and how to use -S to achieve a similar but (generally) better result\n\n# Download the worst video available (old method)\n$ yt-dlp -f \"wv*+wa/w\"\n\n# Download the best video available but with the smallest resolution\n$ yt-dlp -S \"+res\"\n\n# Download the smallest video available\n$ yt-dlp -S \"+size,+br\"\n\n\n\n# Download the best mp4 video available, or the best video if no mp4 available\n$ yt-dlp -f \"bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b\"\n\n# Download the best video with the best extension\n# (For video, mp4 > mov > webm > flv. For audio, m4a > aac > mp3 ...)\n$ yt-dlp -S \"ext\"\n\n\n\n# Download the best video available but no better than 480p,\n# or the worst video if there is no video under 480p\n$ yt-dlp -f \"bv*[height<=480]+ba/b[height<=480] / wv*+ba/w\"\n\n# Download the best video available with the largest height but no better than 480p,\n# or the best video with the smallest resolution if there is no video under 480p\n$ yt-dlp -S \"height:480\"\n\n# Download the best video available with the largest resolution but no better than 480p,\n# or the best video with the smallest resolution if there is no video under 480p\n# Resolution is determined by using the smallest dimension.\n# So this works correctly for vertical videos as well\n$ yt-dlp -S \"res:480\"\n\n\n\n# Download the best video (that also has audio) but no bigger than 50 MB,\n# or the worst video (that also has audio) if there is no video under 50 MB\n$ yt-dlp -f \"b[filesize<50M] / w\"\n\n# Download the largest video (that also has audio) but no bigger than 50 MB,\n# or the smallest video (that also has audio) if there is no video under 50 MB\n$ yt-dlp -f \"b\" -S \"filesize:50M\"\n\n# Download the best video (that also has audio) that is closest in size to 50 MB\n$ yt-dlp -f \"b\" -S \"filesize~50M\"\n\n\n\n# Download best video available via direct link over HTTP/HTTPS protocol,\n# or the best video available via any protocol if there is no such video\n$ yt-dlp -f \"(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)\"\n\n# Download best video available via the best protocol\n# (https/ftps > http/ftp > m3u8_native > m3u8 > http_dash_segments ...)\n$ yt-dlp -S \"proto\"\n\n\n\n# Download the best video with either h264 or h265 codec,\n# or the best video if there is no such video\n$ yt-dlp -f \"(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)\"\n\n# Download the best video with best codec no better than h264,\n# or the best video with worst codec if there is no such video\n$ yt-dlp -S \"codec:h264\"\n\n# Download the best video with worst codec no worse than h264,\n# or the best video with best codec if there is no such video\n$ yt-dlp -S \"+codec:h264\"\n\n\n\n# More complex examples\n\n# Download the best video no better than 720p preferring framerate greater than 30,\n# or the worst video (still preferring framerate greater than 30) if there is no such video\n$ yt-dlp -f \"((bv*[fps>30]/bv*)[height<=720]/(wv*[fps>30]/wv*)) + ba / (b[fps>30]/b)[height<=720]/(w[fps>30]/w)\"\n\n# Download the video with the largest resolution no better than 720p,\n# or the video with the smallest resolution available if there is no such video,\n# preferring larger framerate for formats with the same resolution\n$ yt-dlp -S \"res:720,fps\"\n\n\n\n# Download the video with smallest resolution no worse than 480p,\n# or the video with the largest resolution available if there is no such video,\n# preferring better codec and then larger total bitrate for the same resolution\n$ yt-dlp -S \"+res:480,codec,br\"", "from yt_dlp import YoutubeDL\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\nwith YoutubeDL() as ydl:\n    ydl.download(URLS)", "import json\nimport yt_dlp\n\nURL = 'https://www.youtube.com/watch?v=BaW_jenozKc'\n\n# â„¹ï¸ See help(yt_dlp.YoutubeDL) for a list of available options and public functions\nydl_opts = {}\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    info = ydl.extract_info(URL, download=False)\n\n    # â„¹ï¸ ydl.sanitize_info makes the info json-serializable\n    print(json.dumps(ydl.sanitize_info(info)))", "import yt_dlp\n\nINFO_FILE = 'path/to/video.info.json'\n\nwith yt_dlp.YoutubeDL() as ydl:\n    error_code = ydl.download_with_info_file(INFO_FILE)\n\nprint('Some videos failed to download' if error_code\n      else 'All videos successfully downloaded')", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\nydl_opts = {\n    'format': 'm4a/bestaudio/best',\n    # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n    'postprocessors': [{  # Extract audio using ffmpeg\n        'key': 'FFmpegExtractAudio',\n        'preferredcodec': 'm4a',\n    }]\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\ndef longer_than_a_minute(info, *, incomplete):\n    \"\"\"Download only videos longer than a minute (or with unknown duration)\"\"\"\n    duration = info.get('duration')\n    if duration and duration < 60:\n        return 'The video is too short'\n\nydl_opts = {\n    'match_filter': longer_than_a_minute,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\nclass MyLogger:\n    def debug(self, msg):\n        # For compatibility with youtube-dl, both debug and info are passed into debug\n        # You can distinguish them by the prefix '[debug] '\n        if msg.startswith('[debug] '):\n            pass\n        else:\n            self.info(msg)\n\n    def info(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        print(msg)\n\n\n# â„¹ï¸ See \"progress_hooks\" in help(yt_dlp.YoutubeDL)\ndef my_hook(d):\n    if d['status'] == 'finished':\n        print('Done downloading, now post-processing ...')\n\n\nydl_opts = {\n    'logger': MyLogger(),\n    'progress_hooks': [my_hook],\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\n# â„¹ï¸ See help(yt_dlp.postprocessor.PostProcessor)\nclass MyCustomPP(yt_dlp.postprocessor.PostProcessor):\n    def run(self, info):\n        self.to_screen('Doing stuff')\n        return [], info\n\n\nwith yt_dlp.YoutubeDL() as ydl:\n    # â„¹ï¸ \"when\" can take any value in yt_dlp.utils.POSTPROCESS_WHEN\n    ydl.add_post_processor(MyCustomPP(), when='pre_process')\n    ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\ndef format_selector(ctx):\n    \"\"\" Select the best video and the best audio that won't result in an mkv.\n    NOTE: This is just an example and does not handle all cases \"\"\"\n\n    # formats are already sorted worst to best\n    formats = ctx.get('formats')[::-1]\n\n    # acodec='none' means there is no audio\n    best_video = next(f for f in formats\n                      if f['vcodec'] != 'none' and f['acodec'] == 'none')\n\n    # find compatible audio extension\n    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]\n    # vcodec='none' means there is no video\n    best_audio = next(f for f in formats if (\n        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))\n\n    # These are the minimum required fields for a merged format\n    yield {\n        'format_id': f'{best_video[\"format_id\"]}+{best_audio[\"format_id\"]}',\n        'ext': best_video['ext'],\n        'requested_formats': [best_video, best_audio],\n        # Must be + separated list of protocols\n        'protocol': f'{best_video[\"protocol\"]}+{best_audio[\"protocol\"]}'\n    }\n\n\nydl_opts = {\n    'format': format_selector,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)"], "docker_commands": [], "has_docker_files": false}, "timestamp": "2025-10-27T21:06:43.990827"}
{"repo_name": "deepseek-ai/DeepSeek-V3", "stars": 100008, "language": "Python", "tasks": [{"task_title": "FP8 to BF16 Weight Conversion", "task_description": "å°†FP8æƒé‡è½¬æ¢ä¸ºBF16æƒé‡ï¼Œä½¿ç”¨äº†fp8_cast_bf16.pyè„šæœ¬è¿›è¡Œå¤„ç†ã€‚", "example_code": null, "running_command": "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights", "expected_input": "/path/to/fp8_weights, /path/to/bf16_weights", "expected_output": "è½¬æ¢å®Œæˆï¼Œè¾“å‡ºBF16æƒé‡æ–‡ä»¶"}, {"task_title": "Chat with DeepSeek-V3", "task_description": "ä¸DeepSeek-V3è¿›è¡Œå¯¹è¯ï¼Œä½¿ç”¨äº†èŠå¤©åŠŸèƒ½ã€‚", "example_code": null, "running_command": null, "expected_input": "ç”¨æˆ·è¾“å…¥çš„å¯¹è¯å†…å®¹", "expected_output": "DeepSeek-V3çš„å›å¤å†…å®¹"}, {"task_title": "Batch Inference", "task_description": "å¯¹ç»™å®šæ–‡ä»¶è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œå¤„ç†å¤šä¸ªè¾“å…¥æ•°æ®ã€‚", "example_code": null, "running_command": null, "expected_input": "åŒ…å«å¤šä¸ªå¾…æ¨ç†æ•°æ®çš„æ–‡ä»¶", "expected_output": "æ¨ç†ç»“æœçš„æ±‡æ€»è¾“å‡º"}], "setup": {"setup_commands": ["\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.\n", "\nDownload the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert Hugging Face model weights to a specific format:\n", "\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nSGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.\n\nMulti-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. \n\n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Inference with LightLLM (recommended)\n\n[LightLLM](https://github.com/ModelTC/lightllm/tree/main) v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to [LightLLM instructions](https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html). Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.\n\n### 6.7 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.8 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation"], "docker_commands": [], "has_docker_files": false}, "timestamp": "2025-10-27T21:06:56.259096"}
{"repo_name": "nvbn/thefuck", "stars": 94490, "language": "Python", "tasks": [{"task_title": "Fix Git Push Error", "task_description": "Automatically fix the error when trying to push to a git repository without an upstream branch by suggesting the correct command.", "example_code": null, "running_command": "fuck", "expected_input": "git push", "expected_output": "git push --set-upstream origin master"}, {"task_title": "Correct Misspelled Command", "task_description": "Suggest the correct command when a misspelled command is entered.", "example_code": null, "running_command": "fuck", "expected_input": "puthon", "expected_output": "python"}, {"task_title": "Fix Git Branch Command", "task_description": "Automatically suggest the correct git command when an incorrect command is typed.", "example_code": null, "running_command": "fuck", "expected_input": "git brnch", "expected_output": "git branch"}, {"task_title": "Fix Lein Command", "task_description": "Suggest the correct Lein command when an incorrect task is used.", "example_code": null, "running_command": "fuck", "expected_input": "lein rpl", "expected_output": "lein repl"}, {"task_title": "Match Command Function", "task_description": "Check if the output of a command contains permission errors.", "example_code": "def match(command):\n    return ('permission denied' in command.output.lower()\n            or 'EACCES' in command.output)", "running_command": null, "expected_input": "Command output containing 'permission denied'", "expected_output": "True"}, {"task_title": "Get New Command Function", "task_description": "Generate a new command by adding 'sudo' to the existing command.", "example_code": "def get_new_command(command):\n    return 'sudo {}'.format(command.script)", "running_command": null, "expected_input": "existing command script", "expected_output": "sudo existing command"}, {"task_title": "Side Effect Function", "task_description": "Change permissions of the current directory as a side effect of running a command.", "example_code": "def side_effect(command, fixed_command):\n    subprocess.call('chmod 777 .', shell=True)", "running_command": null, "expected_input": "Command with permission issues", "expected_output": "Permissions changed"}, {"task_title": "Set Rules for The Fuck", "task_description": "Define rules for how The Fuck should operate, including which commands to exclude.", "example_code": null, "running_command": "export THEFUCK_RULES='sudo:no_command'", "expected_input": null, "expected_output": null}, {"task_title": "Enable Experimental Mode", "task_description": "Enable experimental instant mode for The Fuck.", "example_code": null, "running_command": "eval $(thefuck --alias --enable-experimental-instant-mode)", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim [enter/â†‘/â†“/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...", "âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim\n[sudo] password for nvbn:\nReading package lists... Done\n...", "brew install thefuck", "sudo apt update\nsudo apt install python3-dev python3-pip python3-setuptools\npip3 install thefuck --user", "pip install thefuck", "pip3 install thefuck --upgrade", "thefuck_contrib_foo\n  thefuck_contrib_foo\n    rules\n      __init__.py\n      *third-party rules*\n    __init__.py\n    *third-party-utils*\n  setup.py"], "docker_commands": [], "has_docker_files": false}, "timestamp": "2025-10-27T21:07:14.727991"}
{"repo_name": "pytorch/pytorch", "stars": 94294, "language": "Python", "tasks": [{"task_title": "Build for ROCm", "task_description": "This task compiles the PyTorch library specifically for the ROCm platform.", "example_code": null, "running_command": "python tools/amd_build/build_amd.py", "expected_input": null, "expected_output": "Compilation output for ROCm"}, {"task_title": "Build Docker Image", "task_description": "This task builds a Docker image for PyTorch using the provided Makefile.", "example_code": null, "running_command": "make -f docker.Makefile", "expected_input": null, "expected_output": "Docker image tagged as docker.io/${your_docker_username}/pytorch"}, {"task_title": "Generate LaTeX PDF", "task_description": "This task generates a PDF document from LaTeX sources.", "example_code": null, "running_command": "make latexpdf", "expected_input": null, "expected_output": "LaTeX PDF document generated"}, {"task_title": "Generate LaTeX PDF with Nonstop Mode", "task_description": "This task generates a PDF document from LaTeX sources with nonstop mode enabled to avoid stopping for errors.", "example_code": null, "running_command": "make LATEXOPTS=\"-interaction=nonstopmode\"", "expected_input": null, "expected_output": "LaTeX PDF document generated with nonstop mode"}], "setup": {"setup_commands": ["$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>", "$ source <CONDA_INSTALL_DIR>\\Scripts\\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call \"C:\\Program Files\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64", "git clone https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive", "# Run this command from the PyTorch directory after cloning the source code using the â€œGet the PyTorch Sourceâ€œ section above\npip install --group dev", "pip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\n# magma installation: run with active conda environment. specify CUDA version to install\n.ci/docker/common/install_magma_conda.sh 12.4\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton", "# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv", "pip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.51", "export CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\npython -m pip install --no-build-isolation -v -e .", "python -m pip install --no-build-isolation -v -e .", "python -m pip install --no-build-isolation -v -e .", "cmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython -m pip install --no-build-isolation -v -e .", ":: CMD Commands:\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\n\nif defined CMAKE_PREFIX_PATH (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library;%CMAKE_PREFIX_PATH%\"\n) else (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library\"\n)\n\npython -m pip install --no-build-isolation -v -e .", "export CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nCMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build", "export CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nMACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build", "cd docs/\npip install -r requirements.txt\nmake html\nmake serve", "brew install --cask mactex"], "docker_commands": ["docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest"], "has_docker_files": true}, "timestamp": "2025-10-27T21:07:29.697580"}
{"repo_name": "comfyanonymous/ComfyUI", "stars": 92097, "language": "Python", "tasks": [{"task_title": "Run ComfyUI for AMD cards", "task_description": "è¿è¡ŒComfyUIä»¥æ”¯æŒAMDæ˜¾å¡ï¼Œç‰¹åˆ«æ˜¯RDNA2å’ŒRDNA3æ¶æ„çš„æ˜¾å¡ï¼Œé€šè¿‡è®¾ç½®HSA_OVERRIDE_GFX_VERSIONç¯å¢ƒå˜é‡ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py", "expected_input": null, "expected_output": null}, {"task_title": "Run ComfyUI for RDNA3 cards", "task_description": "è¿è¡ŒComfyUIä»¥æ”¯æŒAMD RDNA3æ˜¾å¡ï¼Œè®¾ç½®HSA_OVERRIDE_GFX_VERSIONç¯å¢ƒå˜é‡ä»¥æé«˜å…¼å®¹æ€§ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py", "expected_input": null, "expected_output": null}, {"task_title": "Enable experimental memory efficient attention", "task_description": "åœ¨ComfyUIä¸­ä¸ºæŸäº›AMD GPUå¯ç”¨å®éªŒæ€§å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›åŠŸèƒ½ï¼Œä»¥æé«˜é€Ÿåº¦ã€‚", "example_code": null, "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "Set front-end version to latest", "task_description": "ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ComfyUIå‰ç«¯ã€‚", "example_code": null, "running_command": "--front-end-version Comfy-Org/ComfyUI_frontend@latest", "expected_input": null, "expected_output": null}, {"task_title": "Set front-end version to specific version", "task_description": "ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„ComfyUIå‰ç«¯ã€‚", "example_code": null, "running_command": "--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2", "expected_input": null, "expected_output": null}, {"task_title": "Set legacy front-end version to latest", "task_description": "ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ComfyUIé—ç•™å‰ç«¯ã€‚", "example_code": null, "running_command": "--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["pip install comfy-cli\ncomfy install"], "docker_commands": [], "has_docker_files": false}, "timestamp": "2025-10-27T21:07:44.208686"}
{"repo_name": "fastapi/fastapi", "stars": 91237, "language": "Python", "tasks": [{"task_title": "Read root endpoint", "task_description": "This task defines a GET endpoint at the root path that returns a greeting message.", "example_code": "@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "Read item by ID", "task_description": "This task defines a GET endpoint that accepts an item ID as a path parameter and an optional query parameter, returning the item ID and query value.", "example_code": "@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": "item_id=1&q=test", "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "Read root endpoint (async)", "task_description": "This task defines an asynchronous GET endpoint at the root path that returns a greeting message.", "example_code": "@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "Read item by ID (async)", "task_description": "This task defines an asynchronous GET endpoint that accepts an item ID as a path parameter and an optional query parameter, returning the item ID and query value.", "example_code": "@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": "item_id=1&q=test", "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "Run FastAPI development server", "task_description": "This command starts the FastAPI development server, allowing the application to serve HTTP requests.", "example_code": null, "running_command": "fastapi dev main.py", "expected_input": null, "expected_output": "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)"}], "setup": {"setup_commands": ["$ pip install \"fastapi[standard]\"\n\n---> 100%"], "docker_commands": [], "has_docker_files": false}, "timestamp": "2025-10-27T21:07:59.475542"}
{"repo_name": "openai/whisper", "stars": 90010, "language": "Python", "tasks": [{"task_title": "Transcribe audio file", "task_description": "ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å°†éŸ³é¢‘æ–‡ä»¶è½¬å½•ä¸ºæ–‡æœ¬ã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])", "running_command": null, "expected_input": "audio.mp3", "expected_output": "è½¬å½•åçš„æ–‡æœ¬å†…å®¹"}, {"task_title": "Detect language of audio", "task_description": "æ£€æµ‹éŸ³é¢‘ä¸­æ‰€è¯´çš„è¯­è¨€ï¼Œå¹¶è¾“å‡ºæ£€æµ‹ç»“æœã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Detected language: è¯­è¨€åç§°"}, {"task_title": "Translate audio file", "task_description": "ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å°†éŸ³é¢‘æ–‡ä»¶ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚", "example_code": null, "running_command": "whisper japanese.wav --model medium --language Japanese --task translate", "expected_input": "japanese.wav", "expected_output": "ç¿»è¯‘åçš„æ–‡æœ¬å†…å®¹"}, {"task_title": "Specify language for transcription", "task_description": "åœ¨è½¬å½•æ—¶æŒ‡å®šéŸ³é¢‘çš„è¯­è¨€ã€‚", "example_code": null, "running_command": "whisper japanese.wav --language Japanese", "expected_input": "japanese.wav", "expected_output": "è½¬å½•åçš„æ–‡æœ¬å†…å®¹"}, {"task_title": "Display help information", "task_description": "æ˜¾ç¤ºå‘½ä»¤è¡Œå·¥å…·çš„å¸®åŠ©ä¿¡æ¯ã€‚", "example_code": null, "running_command": "whisper --help", "expected_input": null, "expected_output": "å¸®åŠ©ä¿¡æ¯å†…å®¹"}], "setup": {"setup_commands": ["# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg", "pip install setuptools-rust"], "docker_commands": [], "has_docker_files": false}, "timestamp": "2025-10-27T21:08:14.560976"}
{"repo_name": "microsoft/markitdown", "stars": 82164, "language": "Python", "tasks": [{"task_title": "Convert PDF to Markdown", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æ¡£ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf > document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF to Markdown with Output File", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownå¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF from Standard Input", "task_description": "é€šè¿‡æ ‡å‡†è¾“å…¥å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "cat path-to-file.pdf | markitdown", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "List Available Plugins", "task_description": "åˆ—å‡ºå¯ç”¨çš„æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --list-plugins", "expected_input": null, "expected_output": "å¯ç”¨æ’ä»¶åˆ—è¡¨"}, {"task_title": "Use Plugins for Conversion", "task_description": "ä½¿ç”¨æ’ä»¶å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "markitdown --use-plugins path-to-file.pdf", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint", "task_description": "ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"", "expected_input": "path-to-file.pdf, <document_intelligence_endpoint>", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert Excel to Markdown without Plugins", "task_description": "å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œç¦ç”¨æ’ä»¶ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False)\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.xlsx", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint in Code", "task_description": "ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„ä»£ç ç¤ºä¾‹ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.pdf, <document_intelligence_endpoint>", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert Image with LLM Client", "task_description": "ä½¿ç”¨LLMå®¢æˆ·ç«¯å°†å›¾åƒè½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": "from markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)", "running_command": null, "expected_input": "example.jpg", "expected_output": "Markdownå†…å®¹"}], "setup": {"setup_commands": ["python -m venv .venv\nsource .venv/bin/activate", "uv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment", "conda create -n markitdown python=3.12\nconda activate markitdown", "git clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'", "pip install 'markitdown[pdf, docx, pptx]'", "  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test"], "docker_commands": ["docker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"], "has_docker_files": true}, "timestamp": "2025-10-27T21:08:39.409134"}
{"repo_name": "Significant-Gravitas/AutoGPT", "stars": 179320, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†", "task_description": "ä½¿ç”¨ agent å‘½ä»¤æ¥åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†ã€‚", "example_code": null, "running_command": "cli.py agent", "expected_input": null, "expected_output": null}, {"task_title": "åŸºå‡†æµ‹è¯•", "task_description": "ä½¿ç”¨ benchmark å‘½ä»¤æ¥å¯åŠ¨åŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ—å‡ºæµ‹è¯•å’Œç±»åˆ«ã€‚", "example_code": null, "running_command": "cli.py benchmark", "expected_input": null, "expected_output": null}, {"task_title": "å®‰è£…ä¾èµ–", "task_description": "ä½¿ç”¨ setup å‘½ä»¤æ¥å®‰è£…ç³»ç»Ÿæ‰€éœ€çš„ä¾èµ–ã€‚", "example_code": null, "running_command": "cli.py setup", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["curl -fsSL https://setup.agpt.co/install.sh -o install.sh && bash install.sh", "powershell -c \"iwr https://setup.agpt.co/install.bat -o install.bat; ./install.bat\""], "docker_commands": [], "has_docker_files": true}, "input_to_gpt": {"repo_name": "Significant-Gravitas/AutoGPT", "num_code_blocks": 1, "total_length": 297, "code_blocks": ["$ ./run\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent      Commands to create, start and stop agents\n  benchmark  Commands to start the benchmark and list tests and categories\n  setup      Installs dependencies needed for your system."]}, "timestamp": "2025-10-27T21:19:40.773260"}
{"repo_name": "langflow-ai/langflow", "stars": 136272, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒLangflow", "task_description": "ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è¿è¡ŒLangflowåº”ç”¨ï¼Œå¯åŠ¨ç›¸å…³æœåŠ¡ã€‚", "example_code": null, "running_command": "uv run langflow run", "expected_input": null, "expected_output": "æœåŠ¡å¯åŠ¨æˆåŠŸï¼Œç›¸å…³ä¿¡æ¯è¾“å‡º"}], "setup": {"setup_commands": ["uv pip install langflow -U"], "docker_commands": [], "has_docker_files": true}, "input_to_gpt": {"repo_name": "langflow-ai/langflow", "num_code_blocks": 1, "total_length": 19, "code_blocks": ["uv run langflow run"]}, "timestamp": "2025-10-27T21:20:00.861358"}
{"repo_name": "yt-dlp/yt-dlp", "stars": 132886, "language": "Python", "tasks": [{"task_title": "Always extract audio", "task_description": "ä½¿ç”¨-xé€‰é¡¹æå–éŸ³é¢‘ã€‚", "example_code": null, "running_command": "yt-dlp -x", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Extracting audio..."}, {"task_title": "Copy the mtime", "task_description": "ä½¿ç”¨--mtimeé€‰é¡¹ä¿å­˜è§†é¢‘çš„ä¿®æ”¹æ—¶é—´ã€‚", "example_code": null, "running_command": "yt-dlp --mtime", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Video downloaded with original mtime."}, {"task_title": "Use a proxy", "task_description": "ä½¿ç”¨--proxyé€‰é¡¹é€šè¿‡ä»£ç†ä¸‹è½½è§†é¢‘ã€‚", "example_code": null, "running_command": "yt-dlp --proxy 127.0.0.1:3128", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Downloading through proxy..."}, {"task_title": "Save videos in a specific directory", "task_description": "ä½¿ç”¨-oé€‰é¡¹å°†è§†é¢‘ä¿å­˜åˆ°æŒ‡å®šç›®å½•ã€‚", "example_code": null, "running_command": "yt-dlp -o ~/YouTube/%(title)s.%(ext)s", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Video saved to ~/YouTube/VideoTitle.ext."}, {"task_title": "Parse metadata for title", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹è§£æå…ƒæ•°æ®å¹¶è®¾ç½®æ ‡é¢˜æ ¼å¼ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"title:%(artist)s - %(title)s\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Video title set to 'Artist - Title'."}, {"task_title": "Regex for description metadata", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£ææè¿°ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"description:Artist - (?P<artist>.+)\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Artist metadata extracted."}, {"task_title": "Set series name in title", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹å°†æ ‡é¢˜è®¾ç½®ä¸ºç³»åˆ—åç§°æ ¼å¼ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"%((series)s S%(season_number)02dE%(episode_number)02d:%(title)s\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Title set to 'Series name S01E05: Title'."}, {"task_title": "Prioritize uploader as artist", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹å°†ä¸Šä¼ è€…ä½œä¸ºè‰ºæœ¯å®¶å­—æ®µï¼Œå¹¶åµŒå…¥å…ƒæ•°æ®ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"%((uploader|)s:%(meta_artist)s\" --embed-metadata", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Uploader set as artist in metadata."}, {"task_title": "Set comment field using description", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹ç”¨æè¿°è®¾ç½®è¯„è®ºå­—æ®µã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"description:(?s)(?P<meta_comment>.+)\" --embed-metadata", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Comment field set in metadata."}, {"task_title": "Do not set synopsis in metadata", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹ä¸è®¾ç½®ä»»ä½•ç®€ä»‹ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \":(?P<meta_synopsis>)\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "No synopsis set in metadata."}, {"task_title": "Remove formats field from infojson", "task_description": "ä½¿ç”¨--parse-metadataé€‰é¡¹å°†formatså­—æ®µè®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²ä»¥ç§»é™¤ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"video::(?P<formats>)\" --write-info-json", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Formats field removed from infojson."}, {"task_title": "Replace spaces in metadata", "task_description": "ä½¿ç”¨--replace-in-metadataé€‰é¡¹æ›¿æ¢æ ‡é¢˜å’Œä¸Šä¼ è€…ä¸­çš„ç©ºæ ¼å’Œä¸‹åˆ’çº¿ã€‚", "example_code": null, "running_command": "yt-dlp --replace-in-metadata \"title,uploader\" \"[ _]\" \"-\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Spaces and underscores replaced with '-'."}], "setup": {"setup_commands": ["INSTALLATION", "curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import\ngpg --verify SHA2-256SUMS.sig SHA2-256SUMS\ngpg --verify SHA2-512SUMS.sig SHA2-512SUMS", "# To update to nightly from stable executable/binary:\nyt-dlp --update-to nightly\n\n# To install nightly with pip:\npython3 -m pip install -U --pre \"yt-dlp[default]\"", "$ yt-dlp --print filename -o \"test video.%(ext)s\" BaW_jenozKc\ntest video.webm    # Literal name with correct extension\n\n$ yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc\nyoutube-dl test video ''_Ã¤â†­ğ•.webm    # All kinds of weird characters\n\n$ yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc --restrict-filenames\nyoutube-dl_test_video_.webm    # Restricted file name\n\n# Download YouTube playlist videos in separate directory indexed by video order in a playlist\n$ yt-dlp -o \"%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n# Download YouTube playlist videos in separate directories according to their uploaded year\n$ yt-dlp -o \"%(upload_date>%Y)s/%(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n# Prefix playlist index with \" - \" separator, but only if it is available\n$ yt-dlp -o \"%(playlist_index&{} - |)s%(title)s.%(ext)s\" BaW_jenozKc \"https://www.youtube.com/user/TheLinuxFoundation/playlists\"\n\n# Download all playlists of YouTube channel/user keeping each playlist in separate directory:\n$ yt-dlp -o \"%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/user/TheLinuxFoundation/playlists\"\n\n# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home\n$ yt-dlp -u user -p password -P \"~/MyVideos\" -o \"%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s\" \"https://www.udemy.com/java-tutorial\"\n\n# Download entire series season keeping each series and each season in separate directory under C:/MyVideos\n$ yt-dlp -P \"C:/MyVideos\" -o \"%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s\" \"https://videomore.ru/kino_v_detalayah/5_sezon/367617\"\n\n# Download video as \"C:\\MyVideos\\uploader\\title.ext\", subtitles as \"C:\\MyVideos\\subs\\uploader\\title.ext\"\n# and put all temporary files in \"C:\\MyVideos\\tmp\"\n$ yt-dlp -P \"C:/MyVideos\" -P \"temp:tmp\" -P \"subtitle:subs\" -o \"%(uploader)s/%(title)s.%(ext)s\" BaW_jenozKc --write-subs\n\n# Download video as \"C:\\MyVideos\\uploader\\title.ext\" and subtitles as \"C:\\MyVideos\\uploader\\subs\\title.ext\"\n$ yt-dlp -P \"C:/MyVideos\" -o \"%(uploader)s/%(title)s.%(ext)s\" -o \"subtitle:%(uploader)s/subs/%(title)s.%(ext)s\" BaW_jenozKc --write-subs\n\n# Stream the video being downloaded to stdout\n$ yt-dlp -o - BaW_jenozKc", "# Download and merge the best video-only format and the best audio-only format,\n# or download the best combined format if video-only format is not available\n$ yt-dlp -f \"bv+ba/b\"\n\n# Download best format that contains video,\n# and if it doesn't already have an audio stream, merge it with best audio-only format\n$ yt-dlp -f \"bv*+ba/b\"\n\n# Same as above\n$ yt-dlp\n\n# Download the best video-only format and the best audio-only format without merging them\n# For this case, an output template should be used since\n# by default, bestvideo and bestaudio will have the same file name.\n$ yt-dlp -f \"bv,ba\" -o \"%(title)s.f%(format_id)s.%(ext)s\"\n\n# Download and merge the best format that has a video stream,\n# and all audio-only formats into one file\n$ yt-dlp -f \"bv*+mergeall[vcodec=none]\" --audio-multistreams\n\n# Download and merge the best format that has a video stream,\n# and the best 2 audio-only formats into one file\n$ yt-dlp -f \"bv*+ba+ba.2\" --audio-multistreams\n\n\n# The following examples show the old method (without -S) of format selection\n# and how to use -S to achieve a similar but (generally) better result\n\n# Download the worst video available (old method)\n$ yt-dlp -f \"wv*+wa/w\"\n\n# Download the best video available but with the smallest resolution\n$ yt-dlp -S \"+res\"\n\n# Download the smallest video available\n$ yt-dlp -S \"+size,+br\"\n\n\n\n# Download the best mp4 video available, or the best video if no mp4 available\n$ yt-dlp -f \"bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b\"\n\n# Download the best video with the best extension\n# (For video, mp4 > mov > webm > flv. For audio, m4a > aac > mp3 ...)\n$ yt-dlp -S \"ext\"\n\n\n\n# Download the best video available but no better than 480p,\n# or the worst video if there is no video under 480p\n$ yt-dlp -f \"bv*[height<=480]+ba/b[height<=480] / wv*+ba/w\"\n\n# Download the best video available with the largest height but no better than 480p,\n# or the best video with the smallest resolution if there is no video under 480p\n$ yt-dlp -S \"height:480\"\n\n# Download the best video available with the largest resolution but no better than 480p,\n# or the best video with the smallest resolution if there is no video under 480p\n# Resolution is determined by using the smallest dimension.\n# So this works correctly for vertical videos as well\n$ yt-dlp -S \"res:480\"\n\n\n\n# Download the best video (that also has audio) but no bigger than 50 MB,\n# or the worst video (that also has audio) if there is no video under 50 MB\n$ yt-dlp -f \"b[filesize<50M] / w\"\n\n# Download the largest video (that also has audio) but no bigger than 50 MB,\n# or the smallest video (that also has audio) if there is no video under 50 MB\n$ yt-dlp -f \"b\" -S \"filesize:50M\"\n\n# Download the best video (that also has audio) that is closest in size to 50 MB\n$ yt-dlp -f \"b\" -S \"filesize~50M\"\n\n\n\n# Download best video available via direct link over HTTP/HTTPS protocol,\n# or the best video available via any protocol if there is no such video\n$ yt-dlp -f \"(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)\"\n\n# Download best video available via the best protocol\n# (https/ftps > http/ftp > m3u8_native > m3u8 > http_dash_segments ...)\n$ yt-dlp -S \"proto\"\n\n\n\n# Download the best video with either h264 or h265 codec,\n# or the best video if there is no such video\n$ yt-dlp -f \"(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)\"\n\n# Download the best video with best codec no better than h264,\n# or the best video with worst codec if there is no such video\n$ yt-dlp -S \"codec:h264\"\n\n# Download the best video with worst codec no worse than h264,\n# or the best video with best codec if there is no such video\n$ yt-dlp -S \"+codec:h264\"\n\n\n\n# More complex examples\n\n# Download the best video no better than 720p preferring framerate greater than 30,\n# or the worst video (still preferring framerate greater than 30) if there is no such video\n$ yt-dlp -f \"((bv*[fps>30]/bv*)[height<=720]/(wv*[fps>30]/wv*)) + ba / (b[fps>30]/b)[height<=720]/(w[fps>30]/w)\"\n\n# Download the video with the largest resolution no better than 720p,\n# or the video with the smallest resolution available if there is no such video,\n# preferring larger framerate for formats with the same resolution\n$ yt-dlp -S \"res:720,fps\"\n\n\n\n# Download the video with smallest resolution no worse than 480p,\n# or the video with the largest resolution available if there is no such video,\n# preferring better codec and then larger total bitrate for the same resolution\n$ yt-dlp -S \"+res:480,codec,br\"", "from yt_dlp import YoutubeDL\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\nwith YoutubeDL() as ydl:\n    ydl.download(URLS)", "import json\nimport yt_dlp\n\nURL = 'https://www.youtube.com/watch?v=BaW_jenozKc'\n\n# â„¹ï¸ See help(yt_dlp.YoutubeDL) for a list of available options and public functions\nydl_opts = {}\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    info = ydl.extract_info(URL, download=False)\n\n    # â„¹ï¸ ydl.sanitize_info makes the info json-serializable\n    print(json.dumps(ydl.sanitize_info(info)))", "import yt_dlp\n\nINFO_FILE = 'path/to/video.info.json'\n\nwith yt_dlp.YoutubeDL() as ydl:\n    error_code = ydl.download_with_info_file(INFO_FILE)\n\nprint('Some videos failed to download' if error_code\n      else 'All videos successfully downloaded')", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\nydl_opts = {\n    'format': 'm4a/bestaudio/best',\n    # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n    'postprocessors': [{  # Extract audio using ffmpeg\n        'key': 'FFmpegExtractAudio',\n        'preferredcodec': 'm4a',\n    }]\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\ndef longer_than_a_minute(info, *, incomplete):\n    \"\"\"Download only videos longer than a minute (or with unknown duration)\"\"\"\n    duration = info.get('duration')\n    if duration and duration < 60:\n        return 'The video is too short'\n\nydl_opts = {\n    'match_filter': longer_than_a_minute,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\nclass MyLogger:\n    def debug(self, msg):\n        # For compatibility with youtube-dl, both debug and info are passed into debug\n        # You can distinguish them by the prefix '[debug] '\n        if msg.startswith('[debug] '):\n            pass\n        else:\n            self.info(msg)\n\n    def info(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        print(msg)\n\n\n# â„¹ï¸ See \"progress_hooks\" in help(yt_dlp.YoutubeDL)\ndef my_hook(d):\n    if d['status'] == 'finished':\n        print('Done downloading, now post-processing ...')\n\n\nydl_opts = {\n    'logger': MyLogger(),\n    'progress_hooks': [my_hook],\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\n# â„¹ï¸ See help(yt_dlp.postprocessor.PostProcessor)\nclass MyCustomPP(yt_dlp.postprocessor.PostProcessor):\n    def run(self, info):\n        self.to_screen('Doing stuff')\n        return [], info\n\n\nwith yt_dlp.YoutubeDL() as ydl:\n    # â„¹ï¸ \"when\" can take any value in yt_dlp.utils.POSTPROCESS_WHEN\n    ydl.add_post_processor(MyCustomPP(), when='pre_process')\n    ydl.download(URLS)", "import yt_dlp\n\nURLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']\n\ndef format_selector(ctx):\n    \"\"\" Select the best video and the best audio that won't result in an mkv.\n    NOTE: This is just an example and does not handle all cases \"\"\"\n\n    # formats are already sorted worst to best\n    formats = ctx.get('formats')[::-1]\n\n    # acodec='none' means there is no audio\n    best_video = next(f for f in formats\n                      if f['vcodec'] != 'none' and f['acodec'] == 'none')\n\n    # find compatible audio extension\n    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]\n    # vcodec='none' means there is no video\n    best_audio = next(f for f in formats if (\n        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))\n\n    # These are the minimum required fields for a merged format\n    yield {\n        'format_id': f'{best_video[\"format_id\"]}+{best_audio[\"format_id\"]}',\n        'ext': best_video['ext'],\n        'requested_formats': [best_video, best_audio],\n        # Must be + separated list of protocols\n        'protocol': f'{best_video[\"protocol\"]}+{best_audio[\"protocol\"]}'\n    }\n\n\nydl_opts = {\n    'format': format_selector,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)"], "docker_commands": [], "has_docker_files": false}, "input_to_gpt": {"repo_name": "yt-dlp/yt-dlp", "num_code_blocks": 8, "total_length": 1869, "code_blocks": ["python3 devscripts/install_deps.py --include pyinstaller\npython3 devscripts/make_lazy_extractors.py\npython3 -m bundle.pyinstaller", "# Lines starting with # are comments\n\n# Always extract audio\n-x\n\n# Copy the mtime\n--mtime\n\n# Use this proxy\n--proxy 127.0.0.1:3128\n\n# Save all videos under YouTube directory in your home directory\n-o ~/YouTube/%(title)s.%(ext)s", "touch ${HOME}/.netrc\nchmod a-rwx,u+rw ${HOME}/.netrc", "machine <extractor> login <username> password <password>", "machine youtube login myaccount@gmail.com password my_youtube_password\nmachine twitch login my_twitch_account_name password my_twitch_password", "yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' 'https://www.youtube.com/watch?v=BaW_jenozKc'", "%(name[.keys][addition][>strf][,alternate][&replacement][|default])[flags][width][.precision][length]type", "# Interpret the title as \"Artist - Title\"\n$ yt-dlp --parse-metadata \"title:%(artist)s - %(title)s\"\n\n# Regex example\n$ yt-dlp --parse-metadata \"description:Artist - (?P<artist>.+)\"\n\n# Set title as \"Series name S01E05\"\n$ yt-dlp --parse-metadata \"%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s\"\n\n# Prioritize uploader as the \"artist\" field in video metadata\n$ yt-dlp --parse-metadata \"%(uploader|)s:%(meta_artist)s\" --embed-metadata\n\n# Set \"comment\" field in video metadata using description instead of webpage_url,\n# handling multiple lines correctly\n$ yt-dlp --parse-metadata \"description:(?s)(?P<meta_comment>.+)\" --embed-metadata\n\n# Do not set any \"synopsis\" in the video metadata\n$ yt-dlp --parse-metadata \":(?P<meta_synopsis>)\"\n\n# Remove \"formats\" field from the infojson by setting it to an empty string\n$ yt-dlp --parse-metadata \"video::(?P<formats>)\" --write-info-json\n\n# Replace all spaces and \"_\" in title and uploader with a `-`\n$ yt-dlp --replace-in-metadata \"title,uploader\" \"[ _]\" \"-\"\n"]}, "timestamp": "2025-10-27T21:20:36.012691"}
{"repo_name": "deepseek-ai/DeepSeek-V3", "stars": 100009, "language": "Python", "tasks": [{"task_title": "FP8 to BF16 Weight Conversion", "task_description": "å°†FP8æ ¼å¼çš„æ¨¡å‹æƒé‡è½¬æ¢ä¸ºBF16æ ¼å¼ï¼Œä½¿ç”¨äº†Pythonè„šæœ¬è¿›è¡Œè½¬æ¢ã€‚", "example_code": "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights", "running_command": null, "expected_input": "/path/to/fp8_weights, /path/to/bf16_weights", "expected_output": "æƒé‡è½¬æ¢æˆåŠŸï¼Œè¾“å‡ºè·¯å¾„ä¸º/path/to/bf16_weights"}, {"task_title": "Chat with DeepSeek-V3", "task_description": "ä¸DeepSeek-V3è¿›è¡Œå¯¹è¯ï¼Œä½¿ç”¨äº†CLIå·¥å…·è¿›è¡Œäº¤äº’ã€‚", "example_code": null, "running_command": "python chat_with_deepseek.py", "expected_input": "ç”¨æˆ·è¾“å…¥çš„å¯¹è¯å†…å®¹", "expected_output": "DeepSeek-V3çš„å›å¤å†…å®¹"}, {"task_title": "Batch Inference", "task_description": "å¯¹ç»™å®šæ–‡ä»¶è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œä½¿ç”¨CLIå·¥å…·å¤„ç†å¤šä¸ªè¾“å…¥ã€‚", "example_code": null, "running_command": "python batch_inference.py --input-file /path/to/input_file", "expected_input": "/path/to/input_file", "expected_output": "æ‰¹é‡æ¨ç†ç»“æœ"}], "setup": {"setup_commands": ["\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.\n", "\nDownload the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert Hugging Face model weights to a specific format:\n", "\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nSGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.\n\nMulti-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. \n\n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Inference with LightLLM (recommended)\n\n[LightLLM](https://github.com/ModelTC/lightllm/tree/main) v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to [LightLLM instructions](https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html). Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.\n\n### 6.7 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.8 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation"], "docker_commands": [], "has_docker_files": false}, "input_to_gpt": {"repo_name": "deepseek-ai/DeepSeek-V3", "num_code_blocks": 4, "total_length": 318, "code_blocks": ["cd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights", "#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n", "\n#### Run\n\nThen you can chat with DeepSeek-V3:\n", "\nOr batch inference on a given file:\n"]}, "timestamp": "2025-10-27T21:20:52.805149"}
{"repo_name": "nvbn/thefuck", "stars": 94490, "language": "Python", "tasks": [{"task_title": "Fix Git Push Command", "task_description": "Automatically suggests the correct command to set the upstream branch for git push.", "example_code": null, "running_command": "fuck", "expected_input": "git push", "expected_output": "git push --set-upstream origin master"}, {"task_title": "Fix Misspelled Python Command", "task_description": "Automatically suggests the correct command when a misspelled command is entered.", "example_code": null, "running_command": "fuck", "expected_input": "puthon", "expected_output": "python"}, {"task_title": "Fix Misspelled Git Command", "task_description": "Automatically suggests the correct command when an invalid git command is entered.", "example_code": null, "running_command": "fuck", "expected_input": "git brnch", "expected_output": "git branch"}, {"task_title": "Fix Lein Command", "task_description": "Automatically suggests the correct task for Lein when an invalid task is specified.", "example_code": null, "running_command": "fuck", "expected_input": "lein rpl", "expected_output": "lein repl"}, {"task_title": "Set Custom Alias for TheFuck", "task_description": "Sets a custom alias for TheFuck to use it more conveniently.", "example_code": null, "running_command": "eval $(thefuck --alias)", "expected_input": null, "expected_output": null}, {"task_title": "Run TheFuck with Confirmation", "task_description": "Runs TheFuck with confirmation enabled.", "example_code": null, "running_command": "fuck --yeah", "expected_input": null, "expected_output": null}, {"task_title": "Run TheFuck with Retry", "task_description": "Runs TheFuck with a retry option.", "example_code": null, "running_command": "fuck -r", "expected_input": null, "expected_output": null}, {"task_title": "Match Command Example", "task_description": "Demonstrates how to match a command based on its output.", "example_code": "def match(command):\n    return ('permission denied' in command.output.lower()\n            or 'EACCES' in command.output)", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "Get New Command Example", "task_description": "Demonstrates how to generate a new command based on the matched command.", "example_code": "def get_new_command(command):\n    return 'sudo {}'.format(command.script)", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "Side Effect Example", "task_description": "Demonstrates how to perform a side effect action after fixing a command.", "example_code": "def side_effect(command, fixed_command):\n    subprocess.call('chmod 777 .', shell=True)", "running_command": null, "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim [enter/â†‘/â†“/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...", "âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim\n[sudo] password for nvbn:\nReading package lists... Done\n...", "brew install thefuck", "sudo apt update\nsudo apt install python3-dev python3-pip python3-setuptools\npip3 install thefuck --user", "pip install thefuck", "pip3 install thefuck --upgrade", "thefuck_contrib_foo\n  thefuck_contrib_foo\n    rules\n      __init__.py\n      *third-party rules*\n    __init__.py\n    *third-party-utils*\n  setup.py"], "docker_commands": [], "has_docker_files": false}, "input_to_gpt": {"repo_name": "nvbn/thefuck", "num_code_blocks": 16, "total_length": 2405, "code_blocks": ["âœ git push\nfatal: The current branch master has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin master\n\n\nâœ fuck\ngit push --set-upstream origin master [enter/â†‘/â†“/ctrl+c]\nCounting objects: 9, done.\n...", "âœ puthon\nNo command 'puthon' found, did you mean:\n Command 'python' from package 'python-minimal' (main)\n Command 'python' from package 'python3' (main)\nzsh: command not found: puthon\n\nâœ fuck\npython [enter/â†‘/â†“/ctrl+c]\nPython 3.4.2 (default, Oct  8 2014, 13:08:17)\n...", "âœ git brnch\ngit: 'brnch' is not a git command. See 'git --help'.\n\nDid you mean this?\n    branch\n\nâœ fuck\ngit branch [enter/â†‘/â†“/ctrl+c]\n* master", "âœ lein rpl\n'rpl' is not a task. See 'lein help'.\n\nDid you mean this?\n         repl\n\nâœ fuck\nlein repl [enter/â†‘/â†“/ctrl+c]\nnREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848\nREPL-y 0.3.1\n...", "pkg install thefuck", "crew install thefuck", "sudo pacman -S thefuck", "eval $(thefuck --alias)\n# You can use whatever you want as an alias, like for Mondays:\neval $(thefuck --alias FUCK)", "fuck --yeah", "fuck -r", "match(command: Command) -> bool\nget_new_command(command: Command) -> str | list[str]", "side_effect(old_command: Command, fixed_command: str) -> None", "def match(command):\n    return ('permission denied' in command.output.lower()\n            or 'EACCES' in command.output)\n\n\ndef get_new_command(command):\n    return 'sudo {}'.format(command.script)\n\n# Optional:\nenabled_by_default = True\n\ndef side_effect(command, fixed_command):\n    subprocess.call('chmod 777 .', shell=True)\n\npriority = 1000  # Lower first, default is 1000\n\nrequires_output = True", "rules = ['sudo', 'no_command']\nexclude_rules = ['git_push']\nrequire_confirmation = True\nwait_command = 10\nno_colors = False\npriority = {'sudo': 100, 'no_command': 9999}\ndebug = False\nhistory_limit = 9999\nwait_slow_command = 20\nslow_commands = ['react-native', 'gradle']\nnum_close_matches = 5", "export THEFUCK_RULES='sudo:no_command'\nexport THEFUCK_EXCLUDE_RULES='git_pull:git_push'\nexport THEFUCK_REQUIRE_CONFIRMATION='true'\nexport THEFUCK_WAIT_COMMAND=10\nexport THEFUCK_NO_COLORS='false'\nexport THEFUCK_PRIORITY='no_command=9999:apt_get=100'\nexport THEFUCK_HISTORY_LIMIT='2000'\nexport THEFUCK_NUM_CLOSE_MATCHES='5'", "eval $(thefuck --alias --enable-experimental-instant-mode)"]}, "timestamp": "2025-10-27T21:21:11.891361"}
{"repo_name": "pytorch/pytorch", "stars": 94295, "language": "Python", "tasks": [{"task_title": "Build for ROCm", "task_description": "Compile the PyTorch library specifically for the ROCm platform.", "example_code": null, "running_command": "python tools/amd_build/build_amd.py", "expected_input": null, "expected_output": null}, {"task_title": "Build Docker Images", "task_description": "Build Docker images for PyTorch using the provided Makefile.", "example_code": null, "running_command": "make -f docker.Makefile", "expected_input": null, "expected_output": "images are tagged as docker.io/${your_docker_username}/pytorch"}, {"task_title": "Build Docker Images Again", "task_description": "Another command to build Docker images for PyTorch using the Makefile.", "example_code": null, "running_command": "make -f docker.Makefile", "expected_input": null, "expected_output": null}, {"task_title": "Generate LaTeX PDF", "task_description": "Generate a PDF document from LaTeX files for PyTorch documentation.", "example_code": null, "running_command": "make latexpdf", "expected_input": null, "expected_output": null}, {"task_title": "Generate LaTeX PDF with Nonstop Mode", "task_description": "Generate a PDF document from LaTeX files with nonstop mode enabled.", "example_code": null, "running_command": "make LATEXOPTS=\"-interaction=nonstopmode\"", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>", "$ source <CONDA_INSTALL_DIR>\\Scripts\\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call \"C:\\Program Files\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64", "git clone https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive", "# Run this command from the PyTorch directory after cloning the source code using the â€œGet the PyTorch Sourceâ€œ section above\npip install --group dev", "pip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\n# magma installation: run with active conda environment. specify CUDA version to install\n.ci/docker/common/install_magma_conda.sh 12.4\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton", "# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv", "pip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.51", "export CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\npython -m pip install --no-build-isolation -v -e .", "python -m pip install --no-build-isolation -v -e .", "python -m pip install --no-build-isolation -v -e .", "cmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython -m pip install --no-build-isolation -v -e .", ":: CMD Commands:\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\n\nif defined CMAKE_PREFIX_PATH (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library;%CMAKE_PREFIX_PATH%\"\n) else (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library\"\n)\n\npython -m pip install --no-build-isolation -v -e .", "export CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nCMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build", "export CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nMACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build", "cd docs/\npip install -r requirements.txt\nmake html\nmake serve", "brew install --cask mactex"], "docker_commands": ["docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest"], "has_docker_files": true}, "input_to_gpt": {"repo_name": "pytorch/pytorch", "num_code_blocks": 5, "total_length": 279, "code_blocks": ["# Only run this if you're compiling for ROCm\npython tools/amd_build/build_amd.py", "make -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch", "make -f docker.Makefile", "   make latexpdf", "   make LATEXOPTS=\"-interaction=nonstopmode\""]}, "timestamp": "2025-10-27T21:21:27.610266"}
{"repo_name": "comfyanonymous/ComfyUI", "stars": 92097, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒComfyUIä»¥æ”¯æŒAMDæ˜¾å¡", "task_description": "é€šè¿‡è®¾ç½®HSA_OVERRIDE_GFX_VERSIONç¯å¢ƒå˜é‡æ¥è¿è¡ŒComfyUIï¼Œä»¥æ”¯æŒAMDæ˜¾å¡çš„ç‰¹å®šç‰ˆæœ¬ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py", "expected_input": null, "expected_output": null}, {"task_title": "è¿è¡ŒComfyUIä»¥æ”¯æŒAMD RDNA3æ˜¾å¡", "task_description": "ä¸ºAMD RDNA3æ˜¾å¡è®¾ç½®HSA_OVERRIDE_GFX_VERSIONç¯å¢ƒå˜é‡ä»¥è¿è¡ŒComfyUIã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py", "expected_input": null, "expected_output": null}, {"task_title": "å¯ç”¨å®éªŒæ€§å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›", "task_description": "åœ¨ComfyUIä¸­ä¸ºæŸäº›AMD GPUå¯ç”¨å®éªŒæ€§å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä»¥æé«˜é€Ÿåº¦ã€‚", "example_code": null, "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "æŒ‡å®šå‰ç«¯ç‰ˆæœ¬", "task_description": "ä½¿ç”¨ComfyUIçš„ç‰¹å®šå‰ç«¯ç‰ˆæœ¬è¿›è¡Œè¿è¡Œã€‚", "example_code": null, "running_command": "--front-end-version Comfy-Org/ComfyUI_frontend@latest", "expected_input": null, "expected_output": null}, {"task_title": "æŒ‡å®šç‰¹å®šå‰ç«¯ç‰ˆæœ¬", "task_description": "ä½¿ç”¨ComfyUIçš„ç‰¹å®šå‰ç«¯ç‰ˆæœ¬1.2.2è¿›è¡Œè¿è¡Œã€‚", "example_code": null, "running_command": "--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨é—ç•™å‰ç«¯ç‰ˆæœ¬", "task_description": "ä½¿ç”¨ComfyUIçš„é—ç•™å‰ç«¯ç‰ˆæœ¬è¿›è¡Œè¿è¡Œã€‚", "example_code": null, "running_command": "--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["pip install comfy-cli\ncomfy install"], "docker_commands": [], "has_docker_files": false}, "input_to_gpt": {"repo_name": "comfyanonymous/ComfyUI", "num_code_blocks": 4, "total_length": 800, "code_blocks": ["\n### For AMD cards not officially supported by ROCm\n\nTry running it with this command if you have issues:\n\nFor 6700, 6600 and maybe other RDNA2 or older: ```HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py```\n\nFor AMD 7600 and maybe other RDNA3 cards: ```HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py```\n\n### AMD ROCm Tips\n\nYou can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.\n", "   --front-end-version Comfy-Org/ComfyUI_frontend@latest", "   --front-end-version Comfy-Org/ComfyUI_frontend@1.2.2", "--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest"]}, "timestamp": "2025-10-27T21:21:45.085298"}
{"repo_name": "fastapi/fastapi", "stars": 91238, "language": "Python", "tasks": [{"task_title": "åŸºæœ¬çš„GETè¯·æ±‚å¤„ç†", "task_description": "åˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨ï¼Œå¤„ç†æ ¹è·¯å¾„çš„GETè¯·æ±‚ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "å¸¦è·¯å¾„å‚æ•°çš„GETè¯·æ±‚å¤„ç†", "task_description": "åˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨ï¼Œå¤„ç†å¸¦æœ‰è·¯å¾„å‚æ•°çš„GETè¯·æ±‚ï¼Œè¿”å›è¯¥å‚æ•°çš„å€¼å’Œä¸€ä¸ªå¯é€‰çš„æŸ¥è¯¢å‚æ•°ã€‚", "example_code": "from typing import Union\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": {"item_id": 1, "q": "test"}, "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "å¼‚æ­¥å¤„ç†GETè¯·æ±‚", "task_description": "ä½¿ç”¨asyncå…³é”®å­—å®šä¹‰å¼‚æ­¥çš„GETè¯·æ±‚å¤„ç†å‡½æ•°ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "å¼‚æ­¥å¤„ç†å¸¦è·¯å¾„å‚æ•°çš„GETè¯·æ±‚", "task_description": "ä½¿ç”¨asyncå…³é”®å­—å®šä¹‰å¼‚æ­¥çš„GETè¯·æ±‚å¤„ç†å‡½æ•°ï¼Œå¤„ç†å¸¦æœ‰è·¯å¾„å‚æ•°çš„è¯·æ±‚ï¼Œè¿”å›è¯¥å‚æ•°çš„å€¼å’Œä¸€ä¸ªå¯é€‰çš„æŸ¥è¯¢å‚æ•°ã€‚", "example_code": "from typing import Union\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": {"item_id": 1, "q": "test"}, "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "å¯åŠ¨FastAPIå¼€å‘æœåŠ¡å™¨", "task_description": "ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·å¯åŠ¨FastAPIå¼€å‘æœåŠ¡å™¨ï¼Œæä¾›APIæœåŠ¡ã€‚", "example_code": null, "running_command": "fastapi dev main.py", "expected_input": null, "expected_output": "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)"}], "setup": {"setup_commands": ["$ pip install \"fastapi[standard]\"\n\n---> 100%"], "docker_commands": [], "has_docker_files": false}, "input_to_gpt": {"repo_name": "fastapi/fastapi", "num_code_blocks": 9, "total_length": 6703, "code_blocks": ["from typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "from typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "$ fastapi dev main.py\n\n â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI CLI - Development mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n â”‚                                                     â”‚\n â”‚  Serving at: http://127.0.0.1:8000                  â”‚\n â”‚                                                     â”‚\n â”‚  API docs: http://127.0.0.1:8000/docs               â”‚\n â”‚                                                     â”‚\n â”‚  Running in development mode, for production use:   â”‚\n â”‚                                                     â”‚\n â”‚  fastapi run                                        â”‚\n â”‚                                                     â”‚\n â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n\nINFO:     Will watch for changes in these directories: ['/home/user/code/awesomeapp']\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [2248755] using WatchFiles\nINFO:     Started server process [2248757]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.", "\nYou already created an API that:\n\n* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.\n* Both _paths_ take `GET` <em>operations</em> (also known as HTTP _methods_).\n* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.\n* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.\n\n### Interactive API docs\n\nNow go to <a href=\"http://127.0.0.1:8000/docs\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/docs</a>.\n\nYou will see the automatic interactive API documentation (provided by <a href=\"https://github.com/swagger-api/swagger-ui\" class=\"external-link\" target=\"_blank\">Swagger UI</a>):\n\n![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)\n\n### Alternative API docs\n\nAnd now, go to <a href=\"http://127.0.0.1:8000/redoc\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/redoc</a>.\n\nYou will see the alternative automatic documentation (provided by <a href=\"https://github.com/Rebilly/ReDoc\" class=\"external-link\" target=\"_blank\">ReDoc</a>):\n\n![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)\n\n## Example upgrade\n\nNow modify the file `main.py` to receive a body from a `PUT` request.\n\nDeclare the body using standard Python types, thanks to Pydantic.\n", "\nThe `fastapi dev` server should reload automatically.\n\n### Interactive API docs upgrade\n\nNow go to <a href=\"http://127.0.0.1:8000/docs\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/docs</a>.\n\n* The interactive API documentation will be automatically updated, including the new body:\n\n![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)\n\n* Click on the button \"Try it out\", it allows you to fill the parameters and directly interact with the API:\n\n![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)\n\n* Then click on the \"Execute\" button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:\n\n![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)\n\n### Alternative API docs upgrade\n\nAnd now, go to <a href=\"http://127.0.0.1:8000/redoc\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/redoc</a>.\n\n* The alternative documentation will also reflect the new query parameter and body:\n\n![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)\n\n### Recap\n\nIn summary, you declare **once** the types of parameters, body, etc. as function parameters.\n\nYou do that with standard modern Python types.\n\nYou don't have to learn a new syntax, the methods or classes of a specific library, etc.\n\nJust standard **Python**.\n\nFor example, for an `int`:\n", "\nor for a more complex `Item` model:\n", "\n...and with that single declaration you get:\n\n* Editor support, including:\n    * Completion.\n    * Type checks.\n* Validation of data:\n    * Automatic and clear errors when the data is invalid.\n    * Validation even for deeply nested JSON objects.\n* <abbr title=\"also known as: serialization, parsing, marshalling\">Conversion</abbr> of input data: coming from the network to Python data and types. Reading from:\n    * JSON.\n    * Path parameters.\n    * Query parameters.\n    * Cookies.\n    * Headers.\n    * Forms.\n    * Files.\n* <abbr title=\"also known as: serialization, parsing, marshalling\">Conversion</abbr> of output data: converting from Python data and types to network data (as JSON):\n    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).\n    * `datetime` objects.\n    * `UUID` objects.\n    * Database models.\n    * ...and many more.\n* Automatic interactive API documentation, including 2 alternative user interfaces:\n    * Swagger UI.\n    * ReDoc.\n\n---\n\nComing back to the previous code example, **FastAPI** will:\n\n* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.\n* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.\n    * If it is not, the client will see a useful, clear error.\n* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.\n    * As the `q` parameter is declared with `= None`, it is optional.\n    * Without the `None` it would be required (as is the body in the case with `PUT`).\n* For `PUT` requests to `/items/{item_id}`, read the body as JSON:\n    * Check that it has a required attribute `name` that should be a `str`.\n    * Check that it has a required attribute `price` that has to be a `float`.\n    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.\n    * All this would also work for deeply nested JSON objects.\n* Convert from and to JSON automatically.\n* Document everything with OpenAPI, that can be used by:\n    * Interactive documentation systems.\n    * Automatic client code generation systems, for many languages.\n* Provide 2 interactive documentation web interfaces directly.\n\n---\n\nWe just scratched the surface, but you already get the idea of how it all works.\n\nTry changing the line with:\n", "\n...from:\n", "\n...to:\n"]}, "timestamp": "2025-10-27T21:22:01.265767"}
{"repo_name": "openai/whisper", "stars": 90010, "language": "Python", "tasks": [{"task_title": "Transcribe audio to text", "task_description": "This task transcribes the audio file into text using the 'turbo' model.", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Transcribed text from audio.mp3"}, {"task_title": "Specify language for transcription", "task_description": "This task transcribes an audio file while specifying the spoken language as Japanese.", "example_code": null, "running_command": "whisper japanese.wav --language Japanese", "expected_input": "japanese.wav", "expected_output": "Transcribed text in Japanese"}, {"task_title": "Translate audio to text", "task_description": "This task translates the spoken language in an audio file to text while specifying the model as medium.", "example_code": null, "running_command": "whisper japanese.wav --model medium --language Japanese --task translate", "expected_input": "japanese.wav", "expected_output": "Translated text from Japanese audio"}, {"task_title": "Display help information", "task_description": "This task displays the help information for the whisper CLI tool.", "example_code": null, "running_command": "whisper --help", "expected_input": null, "expected_output": "Help information for the whisper CLI"}, {"task_title": "Load and process audio for transcription", "task_description": "This task loads an audio file, processes it, detects the language, and decodes it to get the recognized text.", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\nprint(result.text)", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Detected language: <language>\nRecognized text"}], "setup": {"setup_commands": ["# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg", "pip install setuptools-rust"], "docker_commands": [], "has_docker_files": false}, "input_to_gpt": {"repo_name": "openai/whisper", "num_code_blocks": 6, "total_length": 926, "code_blocks": ["whisper audio.flac audio.mp3 audio.wav --model turbo", "whisper japanese.wav --language Japanese", "whisper japanese.wav --model medium --language Japanese --task translate", "whisper --help", "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])", "import whisper\n\nmodel = whisper.load_model(\"turbo\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)"]}, "timestamp": "2025-10-27T21:22:20.313542"}
{"repo_name": "microsoft/markitdown", "stars": 82164, "language": "Python", "tasks": [{"task_title": "Convert PDF to Markdown", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æ¡£ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf > document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF to Markdown with Output File", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼å¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF from Standard Input", "task_description": "é€šè¿‡æ ‡å‡†è¾“å…¥å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "cat path-to-file.pdf | markitdown", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "List Available Plugins", "task_description": "åˆ—å‡ºå¯ç”¨çš„æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --list-plugins", "expected_input": null, "expected_output": "å¯ç”¨æ’ä»¶åˆ—è¡¨"}, {"task_title": "Use Plugins for Conversion", "task_description": "ä½¿ç”¨æ’ä»¶å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "markitdown --use-plugins path-to-file.pdf", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼å¹¶ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"", "expected_input": "path-to-file.pdf, <document_intelligence_endpoint>", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert Excel to Markdown without Plugins", "task_description": "å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œç¦ç”¨æ’ä»¶ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False)\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.xlsx", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint in Code", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼å¹¶ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.pdf, <document_intelligence_endpoint>", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert Image to Markdown with OpenAI", "task_description": "å°†å›¾åƒæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œä½¿ç”¨OpenAIçš„LLMã€‚", "example_code": "from markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)", "running_command": null, "expected_input": "example.jpg", "expected_output": "Markdownå†…å®¹"}], "setup": {"setup_commands": ["python -m venv .venv\nsource .venv/bin/activate", "uv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment", "conda create -n markitdown python=3.12\nconda activate markitdown", "git clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'", "pip install 'markitdown[pdf, docx, pptx]'", "  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test"], "docker_commands": ["docker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"], "has_docker_files": true}, "input_to_gpt": {"repo_name": "microsoft/markitdown", "num_code_blocks": 11, "total_length": 978, "code_blocks": ["markitdown path-to-file.pdf > document.md", "markitdown path-to-file.pdf -o document.md", "cat path-to-file.pdf | markitdown", "markitdown --list-plugins", "markitdown --use-plugins path-to-file.pdf", "markitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"", "from markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) # Set to True to enable plugins\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)", "from markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)", "from markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)", "  cd packages/markitdown", "  # Reopen the project in Devcontainer and run:\n  hatch test"]}, "timestamp": "2025-10-27T21:22:49.107912"}
