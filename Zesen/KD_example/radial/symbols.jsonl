{"symbol_id": "v2.generation_functions::Fast_dLLM_QwenForCausalLM", "path": "v2/generation_functions.py", "module": "v2.generation_functions", "name": "Fast_dLLM_QwenForCausalLM", "kind": "Class", "parent": null, "start_line": 14, "end_line": 323, "docstring": null, "calls": ["no_grad", "no_grad", "zeros", "arange", "range", "range", "decode", "forward", "all", "all", "clone", "any", "len", "range", "len", "forward", "cat", "clone", "decode", "append", "item", "argmax", "cat", "range", "all", "range", "range", "item", "squeeze", "argmax"], "decorators": ["auto_docstring"], "num_calls": 99}
{"symbol_id": "v2.generation_functions::setup_model_with_custom_generation", "path": "v2/generation_functions.py", "module": "v2.generation_functions", "name": "setup_model_with_custom_generation", "kind": "Function", "parent": null, "start_line": 326, "end_line": 332, "docstring": "Set up custom generation functions for the model", "calls": ["MethodType"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.generation_functions::batch_sample", "path": "v2/generation_functions.py", "module": "v2.generation_functions", "name": "batch_sample", "kind": "Function", "parent": "Fast_dLLM_QwenForCausalLM", "start_line": 17, "end_line": 168, "docstring": null, "calls": ["no_grad", "zeros", "arange", "range", "forward", "all", "all", "clone", "any", "len", "range", "len", "item", "argmax", "cat", "range", "all", "range", "range", "item", "squeeze", "cat", "squeeze", "ones", "sum", "range", "all", "forward", "argmax", "cat"], "decorators": [null], "num_calls": 54}
{"symbol_id": "v2.generation_functions::mdm_sample_with_visualization", "path": "v2/generation_functions.py", "module": "v2.generation_functions", "name": "mdm_sample_with_visualization", "kind": "Function", "parent": "Fast_dLLM_QwenForCausalLM", "start_line": 171, "end_line": 323, "docstring": "MDM sampling function with visualization\nwith intermediate state output for Gradio visualization", "calls": ["no_grad", "range", "decode", "forward", "cat", "clone", "decode", "append", "argmax", "cat", "len", "range", "range", "ones", "range", "append", "append", "sum", "forward", "argmax", "cat", "decode", "append", "nonzero", "item", "sum", "cat", "sample_with_top_p", "squeeze", "where"], "decorators": [null], "num_calls": 45}
{"symbol_id": "v2.app::fix_seed", "path": "v2/app.py", "module": "v2.app", "name": "fix_seed", "kind": "Function", "parent": null, "start_line": 19, "end_line": 24, "docstring": null, "calls": ["manual_seed", "manual_seed", "manual_seed_all", "seed", "seed"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.app::format_chat_history", "path": "v2/app.py", "module": "v2.app", "name": "format_chat_history", "kind": "Function", "parent": null, "start_line": 52, "end_line": 68, "docstring": "Format chat history for the LLaDA model\n\nArgs:\n    history: List of [user_message, assistant_message] pairs\n    \nReturns:\n    Formatted conversation for the model", "calls": ["append", "append"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.app::generate_response_with_visualization_fast_dllm", "path": "v2/app.py", "module": "v2.app", "name": "generate_response_with_visualization_fast_dllm", "kind": "Function", "parent": null, "start_line": 73, "end_line": 121, "docstring": "Generate text with Fast_dLLM model with visualization using custom generation function\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content'\n    max_new_tokens: Maximum number of tokens to generate\n    temperature: Sampling temperature\n    block_length: Block size for generation\n    threshold: Threshold for generation\n    top_p: Top-p sampling parameter\n    \nYields:\n    Visualization states showing the progression and final text", "calls": ["no_grad", "apply_chat_template", "to", "mdm_sample_with_visualization", "isinstance", "tokenizer", "append"], "decorators": [null], "num_calls": 7}
{"symbol_id": "v2.app::create_chatbot_demo", "path": "v2/app.py", "module": "v2.app", "name": "create_chatbot_demo", "kind": "Function", "parent": null, "start_line": 149, "end_line": 404, "docstring": null, "calls": ["Blocks", "Markdown", "Markdown", "State", "Markdown", "Examples", "Textbox", "click", "submit", "click", "then", "then", "Group", "Row", "Accordion", "copy", "append", "add_message", "copy", "Row", "Textbox", "Button", "Button", "Column", "Chatbot", "Column", "HighlightedText", "HighlightedText", "Row", "Slider"], "decorators": [], "num_calls": 55}
{"symbol_id": "v2.app::add_message", "path": "v2/app.py", "module": "v2.app", "name": "add_message", "kind": "Function", "parent": null, "start_line": 252, "end_line": 256, "docstring": "Add a message pair to the history and return the updated history", "calls": ["copy", "append"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.app::user_message_submitted", "path": "v2/app.py", "module": "v2.app", "name": "user_message_submitted", "kind": "Function", "parent": null, "start_line": 258, "end_line": 276, "docstring": "Process a submitted user message", "calls": ["add_message", "copy", "strip", "copy"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.app::accelerated_response", "path": "v2/app.py", "module": "v2.app", "name": "accelerated_response", "kind": "Function", "parent": null, "start_line": 280, "end_line": 341, "docstring": "Generate accelerated model response independently", "calls": ["format_chat_history", "append", "time", "encode", "len", "no_grad", "generate_response_with_visualization_fast_dllm", "time", "print", "isinstance", "sleep", "append", "str"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.app::clear_conversation", "path": "v2/app.py", "module": "v2.app", "name": "clear_conversation", "kind": "Function", "parent": null, "start_line": 343, "end_line": 359, "docstring": "Clear the conversation history", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.run_chatbot::fix_seed", "path": "v2/run_chatbot.py", "module": "v2.run_chatbot", "name": "fix_seed", "kind": "Function", "parent": null, "start_line": 20, "end_line": 25, "docstring": null, "calls": ["manual_seed", "manual_seed", "manual_seed_all", "seed", "seed"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.eval::set_seed", "path": "v2/eval.py", "module": "v2.eval", "name": "set_seed", "kind": "Function", "parent": null, "start_line": 40, "end_line": 46, "docstring": null, "calls": ["manual_seed", "seed", "seed"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.eval::Fast_dLLM_v2EvalHarness", "path": "v2/eval.py", "module": "v2.eval", "name": "Fast_dLLM_v2EvalHarness", "kind": "Class", "parent": null, "start_line": 50, "end_line": 304, "docstring": null, "calls": ["register_model", "no_grad", "no_grad", "__init__", "Accelerator", "from_pretrained", "eval", "MethodType", "device", "from_pretrained", "int", "apply_chat_template", "len", "cat", "cat", "_forward_process", "get_logits", "cat", "cross_entropy", "sum", "append", "from_list", "map", "with_format", "empty_cache", "time", "sort", "enumerate", "time", "update"], "decorators": [null], "num_calls": 95}
{"symbol_id": "v2.eval::__init__", "path": "v2/eval.py", "module": "v2.eval", "name": "__init__", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 51, "end_line": 109, "docstring": null, "calls": ["__init__", "Accelerator", "from_pretrained", "eval", "MethodType", "device", "from_pretrained", "int", "update", "prepare", "device", "to", "super"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.eval::rank", "path": "v2/eval.py", "module": "v2.eval", "name": "rank", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 112, "end_line": 113, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.eval::world_size", "path": "v2/eval.py", "module": "v2.eval", "name": "world_size", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 116, "end_line": 117, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.eval::tokenizer_name", "path": "v2/eval.py", "module": "v2.eval", "name": "tokenizer_name", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 120, "end_line": 121, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.eval::apply_chat_template", "path": "v2/eval.py", "module": "v2.eval", "name": "apply_chat_template", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 123, "end_line": 124, "docstring": null, "calls": ["apply_chat_template"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.eval::loglikelihood_rolling", "path": "v2/eval.py", "module": "v2.eval", "name": "loglikelihood_rolling", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 126, "end_line": 127, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.eval::_encode_pair", "path": "v2/eval.py", "module": "v2.eval", "name": "_encode_pair", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 129, "end_line": 136, "docstring": null, "calls": ["len", "tokenizer", "tokenizer"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.eval::_forward_process", "path": "v2/eval.py", "module": "v2.eval", "name": "_forward_process", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 139, "end_line": 148, "docstring": null, "calls": ["cat", "sum", "to", "full"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.eval::get_logits", "path": "v2/eval.py", "module": "v2.eval", "name": "get_logits", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 151, "end_line": 154, "docstring": null, "calls": ["no_grad", "cat", "model"], "decorators": [null], "num_calls": 3}
{"symbol_id": "v2.eval::get_loglikelihood", "path": "v2/eval.py", "module": "v2.eval", "name": "get_loglikelihood", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 157, "end_line": 174, "docstring": null, "calls": ["no_grad", "_forward_process", "get_logits", "cat", "cross_entropy", "sum", "append", "concatenate", "arange", "len", "clone", "item", "len", "to", "full", "sum"], "decorators": [null], "num_calls": 16}
{"symbol_id": "v2.eval::loglikelihood", "path": "v2/eval.py", "module": "v2.eval", "name": "loglikelihood", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 177, "end_line": 205, "docstring": null, "calls": ["from_list", "map", "with_format", "empty_cache", "_encode_pair", "max", "no_grad", "tqdm", "len", "len", "get_loglikelihood", "append"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.eval::generate_until", "path": "v2/eval.py", "module": "v2.eval", "name": "generate_until", "kind": "Function", "parent": "Fast_dLLM_v2EvalHarness", "start_line": 207, "end_line": 304, "docstring": null, "calls": ["time", "sort", "enumerate", "time", "len", "append", "append", "tqdm", "cat", "to", "enumerate", "print", "print", "print", "enumerate", "len", "append", "startswith", "to", "append", "max", "min", "append", "cat", "no_grad", "decode", "print", "print", "print", "print"], "decorators": [], "num_calls": 42}
{"symbol_id": "v2.eval::_tokenize", "path": "v2/eval.py", "module": "v2.eval", "name": "_tokenize", "kind": "Function", "parent": "loglikelihood", "start_line": 178, "end_line": 185, "docstring": null, "calls": ["_encode_pair"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.train_scripts.finetune::main", "path": "v2/train_scripts/finetune.py", "module": "v2.train_scripts.finetune", "name": "main", "kind": "Function", "parent": null, "start_line": 33, "end_line": 60, "docstring": null, "calls": ["get_pipeline_args_class", "HfArgumentParser", "get_pipeline", "Dataset", "get_model", "tune", "endswith", "parse_json_file", "parse_args_into_dataclasses", "encode", "len", "abspath"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.args::OptimizerNames", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "OptimizerNames", "kind": "Class", "parent": null, "start_line": 34, "end_line": 54, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.args::ModelArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "ModelArguments", "kind": "Class", "parent": null, "start_line": 57, "end_line": 385, "docstring": "Define a class ModelArguments using the dataclass decorator. \nThe class contains several optional parameters that can be used to configure a model. \n\nmodel_name_or_path : str\n    a string representing the path or name of a pretrained\n    model checkpoint for weights initialization. If None, a model will be trained from scratch.\n\nmodel_type :  str\n    a string representing the type of model to use if training from\n    scratch. If not provided, a pretrained model will be used.\n\nconfig_overrides :  str\n    a string representing the default config settings to override\n    when training a model from scratch.\n\nconfig_name : str\n    a string representing the name or path of the pretrained config to\n    use, if different from the model_name_or_path.\n\ntokenizer_name :  str\n    a string representing the name or path of the pretrained tokenizer\n    to use, if different from the model_name_or_path.\n\ncache_dir :  str\n    a string representing the path to the directory where pretrained models\n    downloaded from huggingface.co will be stored.\n\nuse_fast_tokenizer : bool\n    a boolean indicating whether to use a fast tokenizer (backed by the\n    tokenizers library) or not.\n\nmodel_revision :  str\n    a string representing the specific model version to use (can be a\n    branch name, tag name, or commit id).\n\ntoken : Optional[str]\n    Necessary when accessing a private model/dataset.\n\ntorch_dtype :  str\n    a string representing the dtype to load the model under. If auto is\n    passed, the dtype will be automatically derived from the model's weights.\n\nuse_ram_optimized_load : bool\n    a boolean indicating whether to use disk mapping when memory is not\n    enough.\n    \nuse_int8 : bool\n    a boolean indicating whether to load int8 quantization for inference.\n    \nload_in_4bit : bool\n    whether to load the model in 4bit\n    \nmodel_max_length : int\n    The maximum length of the model.\n    \ntruncation_side : str\n    The side on which the model should have truncation applied.\n    \narch_type : str\n    Model architecture type.\npadding_side : str\n    The side on which the tokenizer should have padding applied.\neos_padding : bool\n    whether to pad with eos token instead of pad token.\nignore_bias_buffers : bool\n    fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 45}
{"symbol_id": "v2.src.lmflow.args::VisModelArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "VisModelArguments", "kind": "Class", "parent": null, "start_line": 389, "end_line": 459, "docstring": null, "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.args::DatasetArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "DatasetArguments", "kind": "Class", "parent": null, "start_line": 463, "end_line": 652, "docstring": "Define a class DatasetArguments using the dataclass decorator.\nThe class contains several optional parameters that can be used to configure a dataset for a language model.\n\n\ndataset_path : str\n    a string representing the path of the dataset to use.\n\ndataset_name : str\n    a string representing the name of the dataset to use. The default value is \"customized\".\n\nis_custom_dataset : bool\n    a boolean indicating whether to use custom data. The default value is False.\n\ncustomized_cache_dir : str\n    a string representing the path to the directory where customized dataset caches will be stored.\n\ndataset_config_name : str\n    a string representing the configuration name of the dataset to use (via the datasets library).\n\ntrain_file : str\n    a string representing the path to the input training data file (a text file).\n\nvalidation_file : str\n    a string representing the path to the input evaluation data file to evaluate the perplexity on (a text file).\n\nmax_train_samples : int\n    an integer indicating the maximum number of training examples to use for debugging or quicker training.\n    If set, the training dataset will be truncated to this number.\n\nmax_eval_samples: int\n    an integer indicating the maximum number of evaluation examples to use for debugging or quicker training.\n    If set, the evaluation dataset will be truncated to this number.\n\nstreaming : bool\n    a boolean indicating whether to enable streaming mode.\n\nblock_size: int\n    an integer indicating the optional input sequence length after tokenization. The training dataset will be\n    truncated in blocks of this size for training.\n\ntrain_on_prompt: bool\n    a boolean indicating whether to train on prompt for conversation datasets such as ShareGPT.\n\nconversation_template: str\n    a string representing the template for conversation datasets.\n\ndataset_cache_dir: str\n    a string representing the path to the dataset cache directory. Useful when the default cache dir\n    (`~/.cache/huggingface/datasets`) has limited space.\n\nThe class also includes some additional parameters that can be used to configure the dataset further, such as `overwrite_cache`,\n`validation_split_percentage`, `preprocessing_num_workers`, `disable_group_texts`, `demo_example_in_prompt`, `explanation_in_prompt`,\n`keep_linebreaks`, and `prompt_structure`.\n\nThe field function is used to set default values and provide help messages for each parameter. The Optional type hint is\nused to indicate that a parameter is optional. The metadata argument is used to provide additional information about\neach parameter, such as a help message.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "require_version", "ValueError", "split", "split"], "decorators": ["dataclass"], "num_calls": 26}
{"symbol_id": "v2.src.lmflow.args::MultiModalDatasetArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "MultiModalDatasetArguments", "kind": "Class", "parent": null, "start_line": 656, "end_line": 671, "docstring": null, "calls": ["field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.args::FinetunerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "FinetunerArguments", "kind": "Class", "parent": null, "start_line": 675, "end_line": 794, "docstring": "Adapt transformers.TrainingArguments", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.args::RewardModelTunerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "RewardModelTunerArguments", "kind": "Class", "parent": null, "start_line": 797, "end_line": 801, "docstring": "Arguments for reward modeling.", "calls": [], "decorators": ["dataclass"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.args::EvaluatorArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "EvaluatorArguments", "kind": "Class", "parent": null, "start_line": 805, "end_line": 963, "docstring": "Define a class EvaluatorArguments using the dataclass decorator. The class contains several optional\nparameters that can be used to configure a evaluator.\n\nlocal_rank : str\n    For distributed training: local_rank\n\nrandom_shuffle : bool\n\nuse_wandb : bool\n\nrandom_seed : int, default = 1\n\noutput_dir : str, default = './output_dir',\n\nmixed_precision : str, choice from [\"bf16\",\"fp16\"].\n    mixed precision mode, whether to use bf16 or fp16\n\ndeepspeed :\n    Enable deepspeed and pass the path to deepspeed json config file (e.g. ds_config.json) or an already\n    loaded json file as a dict\n\ntemperature : float\n    An argument of model.generate in huggingface to control the diversity of generation.\n\nrepetition_penalty : float\n    An argument of model.generate in huggingface to penalize repetitions.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.args::InferencerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "InferencerArguments", "kind": "Class", "parent": null, "start_line": 967, "end_line": 1160, "docstring": "Define a class InferencerArguments using the dataclass decorator. The class contains several optional\nparameters that can be used to configure a inferencer.\n\nlocal_rank : str\n    For distributed training: local_rank\nrandom_seed : int, default = 1\ninference_batch_size : int, default = 1\ndeepspeed :\n    Enable deepspeed and pass the path to deepspeed json config file (e.g. ds_config.json) or an already\n    loaded json file as a dict\nmixed_precision : str, choice from [\"bf16\",\"fp16\"].\n    mixed precision mode, whether to use bf16 or fp16\ntemperature : float\n    An argument of model.generate in huggingface to control the diversity of generation.\nrepetition_penalty : float\n    An argument of model.generate in huggingface to penalize repetitions.\nuse_beam_search : Optional[bool]\n    Whether to use beam search during inference, By default False.\nnum_output_sequences : Optional[int]\n    Number of output sequences to return for the given prompt, \n    currently only used in vllm inference, By default 8.\ntop_p : Optional[float]\n    top_p for sampling, By default 1.0.\ntop_k : Optional[int]\n    top_k for sampling, By default -1 (no top_k).\nadditional_stop_token_ids : Optional[List[int]]\n    the ids of the end of sentence tokens, By default [].\napply_chat_template : Optional[bool]\n    Whether to apply chat template, By default True.\nsave_results : Optional[bool]\n    Whether to save inference results, By default False.\nresults_path : Optional[str]\n    The **json file** path of inference results, By default None.\nenable_decode_inference_result : Optional[bool]\n    Whether to detokenize the inference results. \n\n    NOTE: For iterative align pipelines, whether to detokenize depends on \n    the homogeneity of the policy model and the reward model \n    (i.e., if they have the same tokenizer).\nuse_vllm: bool, optional\n    Whether to use VLLM for inference, By default False.\nvllm_tensor_parallel_size: int, optional\n    The tensor parallel size for VLLM inference.\nvllm_gpu_memory_utilization: float, optional\n    The GPU memory utilization for VLLM inference. The proportion of GPU\n    memory (per GPU) to use for VLLM inference.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "ValueError", "endswith", "ValueError"], "decorators": ["dataclass"], "num_calls": 32}
{"symbol_id": "v2.src.lmflow.args::RaftAlignerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "RaftAlignerArguments", "kind": "Class", "parent": null, "start_line": 1164, "end_line": 1234, "docstring": "Define a class RaftAlignerArguments to configure raft aligner.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.args::BenchmarkingArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "BenchmarkingArguments", "kind": "Class", "parent": null, "start_line": 1238, "end_line": 1253, "docstring": null, "calls": ["field", "field"], "decorators": ["dataclass"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.args::DPOAlignerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "DPOAlignerArguments", "kind": "Class", "parent": null, "start_line": 1257, "end_line": 1403, "docstring": "The arguments for the DPO training script.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 25}
{"symbol_id": "v2.src.lmflow.args::DPOv2AlignerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "DPOv2AlignerArguments", "kind": "Class", "parent": null, "start_line": 1407, "end_line": 1427, "docstring": "The arguments for the DPOv2 training script.", "calls": ["field", "field", "field", "field", "field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.args::IterativeAlignerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "IterativeAlignerArguments", "kind": "Class", "parent": null, "start_line": 1431, "end_line": 1442, "docstring": "Arguments for iterative aligners.", "calls": ["field", "field"], "decorators": ["dataclass"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.args::IterativeDPOAlignerArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "IterativeDPOAlignerArguments", "kind": "Class", "parent": null, "start_line": 1447, "end_line": 1474, "docstring": "Arguments for iterative DPO aligners.", "calls": ["field", "field", "field", "field", "field", "field"], "decorators": ["dataclass"], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.args::AutoArguments", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "AutoArguments", "kind": "Class", "parent": null, "start_line": 1491, "end_line": 1497, "docstring": "Automatically choose arguments from FinetunerArguments or EvaluatorArguments.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.args::split_args", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "split_args", "kind": "Function", "parent": null, "start_line": 1500, "end_line": 1501, "docstring": null, "calls": ["isinstance", "strip", "split"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.args::__post_init__", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "__post_init__", "kind": "Function", "parent": "ModelArguments", "start_line": 368, "end_line": 385, "docstring": null, "calls": ["ValueError", "split_args", "warning", "is_flash_attn_available", "warning"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.args::__post_init__", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "__post_init__", "kind": "Function", "parent": "DatasetArguments", "start_line": 640, "end_line": 652, "docstring": null, "calls": ["require_version", "ValueError", "split", "split"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.args::__post_init__", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "__post_init__", "kind": "Function", "parent": "InferencerArguments", "start_line": 1152, "end_line": 1160, "docstring": null, "calls": ["ValueError", "endswith", "ValueError", "mkdir", "Path"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.args::get_pipeline_args_class", "path": "v2/src/lmflow/args.py", "module": "v2.src.lmflow.args", "name": "get_pipeline_args_class", "kind": "Function", "parent": "AutoArguments", "start_line": 1496, "end_line": 1497, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::get_paired_dataset", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "get_paired_dataset", "kind": "Function", "parent": null, "start_line": 24, "end_line": 71, "docstring": "Load dataset and convert it to the necessary format.\n\n    The dataset is converted to a dictionary with the following structure:\n    {\n        'prompt': List[str],\n        'chosen': List[str],\n        'rejected': List[str],\n    }\n\n    Prompts are structured as follows:\n      \"Question: \" + <prompt> + \"\n\nAnswer: \"\n    ", "calls": ["load_dataset", "map", "Path", "as_posix", "select", "glob", "range", "absolute", "min", "len"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::DPOAligner", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "DPOAligner", "kind": "Class", "parent": null, "start_line": 74, "end_line": 170, "docstring": null, "calls": ["LoraConfig", "TrainingArguments", "DPOTrainer", "get_paired_dataset", "filter", "get_tokenizer", "_load_dataset", "get_backend_model", "_initialize_trainer", "train", "save_model", "join", "save_pretrained", "get_paired_dataset", "filter", "len", "len", "len", "len", "len", "len", "len", "len"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::return_prompt_and_responses", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "return_prompt_and_responses", "kind": "Function", "parent": "get_paired_dataset", "start_line": 59, "end_line": 64, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::__init__", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "__init__", "kind": "Function", "parent": "DPOAligner", "start_line": 75, "end_line": 80, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::_initialize_trainer", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "_initialize_trainer", "kind": "Function", "parent": "DPOAligner", "start_line": 82, "end_line": 134, "docstring": null, "calls": ["LoraConfig", "TrainingArguments", "DPOTrainer"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::_load_dataset", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "_load_dataset", "kind": "Function", "parent": "DPOAligner", "start_line": 136, "end_line": 153, "docstring": null, "calls": ["get_paired_dataset", "filter", "get_paired_dataset", "filter", "len", "len", "len", "len", "len", "len", "len", "len"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.dpo_aligner::align", "path": "v2/src/lmflow/pipeline/dpo_aligner.py", "module": "v2.src.lmflow.pipeline.dpo_aligner", "name": "align", "kind": "Function", "parent": "DPOAligner", "start_line": 155, "end_line": 170, "docstring": null, "calls": ["get_tokenizer", "_load_dataset", "get_backend_model", "_initialize_trainer", "train", "save_model", "join", "save_pretrained"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::IterativeDPOAligner", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "IterativeDPOAligner", "kind": "Class", "parent": null, "start_line": 29, "end_line": 254, "docstring": null, "calls": ["Path", "len", "tqdm", "str", "MemorySafeVLLMInferencer", "inference", "mkdir", "dump", "RewardModelInferencer", "inference", "mkdir", "save", "MemorySafeDPOv2Aligner", "align", "__filter_args", "__filter_args", "__filter_args", "target_cls", "range", "_align_single_iteration", "print_banner", "HFDecoderModel", "_do_target_model_inference", "print_banner", "HFTextRegressionModel", "deepcopy", "str", "Dataset", "_do_reward_model_inference", "print_banner"], "decorators": [], "num_calls": 50}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::__init__", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "__init__", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 30, "end_line": 44, "docstring": null, "calls": ["Path"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::align", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "align", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 47, "end_line": 70, "docstring": null, "calls": ["len", "tqdm", "range", "_align_single_iteration", "deepcopy", "str"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_align_single_iteration", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_align_single_iteration", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 73, "end_line": 125, "docstring": null, "calls": ["print_banner", "HFDecoderModel", "_do_target_model_inference", "print_banner", "HFTextRegressionModel", "deepcopy", "str", "Dataset", "_do_reward_model_inference", "print_banner", "deepcopy", "str", "_do_single_dpo_align", "str", "str", "str"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_do_target_model_inference", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_do_target_model_inference", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 128, "end_line": 154, "docstring": null, "calls": ["str", "MemorySafeVLLMInferencer", "inference", "mkdir", "dump", "Path", "open", "_parse_target_model_inference_args", "str", "Path"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_do_reward_model_inference", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_do_reward_model_inference", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 157, "end_line": 180, "docstring": null, "calls": ["RewardModelInferencer", "inference", "mkdir", "save", "Path", "str", "_parse_reward_model_inference_args"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_do_single_dpo_align", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_do_single_dpo_align", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 183, "end_line": 201, "docstring": null, "calls": ["MemorySafeDPOv2Aligner", "align", "_parse_dpo_aligner_args"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_parse_target_model_inference_args", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_parse_target_model_inference_args", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 204, "end_line": 216, "docstring": null, "calls": ["__filter_args"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_parse_reward_model_inference_args", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_parse_reward_model_inference_args", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 219, "end_line": 228, "docstring": null, "calls": ["__filter_args"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::_parse_dpo_aligner_args", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "_parse_dpo_aligner_args", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 231, "end_line": 244, "docstring": null, "calls": ["__filter_args"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.iterative_dpo_aligner::__filter_args", "path": "v2/src/lmflow/pipeline/iterative_dpo_aligner.py", "module": "v2.src.lmflow.pipeline.iterative_dpo_aligner", "name": "__filter_args", "kind": "Function", "parent": "IterativeDPOAligner", "start_line": 247, "end_line": 254, "docstring": null, "calls": ["target_cls", "getattr", "fields", "hasattr"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.base_tuner::BaseTuner", "path": "v2/src/lmflow/pipeline/base_tuner.py", "module": "v2.src.lmflow.pipeline.base_tuner", "name": "BaseTuner", "kind": "Class", "parent": null, "start_line": 9, "end_line": 20, "docstring": "A subclass of BasePipeline which is tunable.\n    ", "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.base_tuner::__init__", "path": "v2/src/lmflow/pipeline/base_tuner.py", "module": "v2.src.lmflow.pipeline.base_tuner", "name": "__init__", "kind": "Function", "parent": "BaseTuner", "start_line": 12, "end_line": 13, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.base_tuner::_check_if_tunable", "path": "v2/src/lmflow/pipeline/base_tuner.py", "module": "v2.src.lmflow.pipeline.base_tuner", "name": "_check_if_tunable", "kind": "Function", "parent": "BaseTuner", "start_line": 15, "end_line": 17, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.base_tuner::tune", "path": "v2/src/lmflow/pipeline/base_tuner.py", "module": "v2.src.lmflow.pipeline.base_tuner", "name": "tune", "kind": "Function", "parent": "BaseTuner", "start_line": 19, "end_line": 20, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::InferencerWithOffloading", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "InferencerWithOffloading", "kind": "Class", "parent": null, "start_line": 48, "end_line": 67, "docstring": null, "calls": ["NotImplementedError", "NotImplementedError", "NotImplementedError", "from_pretrained"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::VLLMInferencer", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "VLLMInferencer", "kind": "Class", "parent": null, "start_line": 70, "end_line": 312, "docstring": null, "calls": ["__init__", "parse_to_sampling_params", "SamplingParams", "prepare_inputs_for_inference", "inference", "map_batches", "to_pandas", "info", "info", "warning", "parse_to_sampling_params", "_distributed_inference", "_inference", "save_inference_results", "placement_group", "dict", "open", "dump", "open", "load", "super", "deepcopy", "activate_model_for_inference", "inference", "iterrows", "get", "get", "PlacementGroupSchedulingStrategy", "head"], "decorators": [], "num_calls": 29}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::MemorySafeVLLMInferencer", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "MemorySafeVLLMInferencer", "kind": "Class", "parent": null, "start_line": 315, "end_line": 365, "docstring": null, "calls": ["__init__", "make_shell_args_from_dataclass", "copy", "run", "info", "load_inference_results", "info", "files", "pop", "warning", "super", "RuntimeError", "str"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::__init__", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "__init__", "kind": "Function", "parent": "InferencerWithOffloading", "start_line": 49, "end_line": 58, "docstring": null, "calls": ["from_pretrained"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::inference", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "inference", "kind": "Function", "parent": "InferencerWithOffloading", "start_line": 60, "end_line": 61, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::save_inference_results", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "save_inference_results", "kind": "Function", "parent": "InferencerWithOffloading", "start_line": 63, "end_line": 64, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::load_inference_results", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "load_inference_results", "kind": "Function", "parent": "InferencerWithOffloading", "start_line": 66, "end_line": 67, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::__init__", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "__init__", "kind": "Function", "parent": "VLLMInferencer", "start_line": 71, "end_line": 79, "docstring": null, "calls": ["__init__", "parse_to_sampling_params", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::parse_to_sampling_params", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "parse_to_sampling_params", "kind": "Function", "parent": "VLLMInferencer", "start_line": 82, "end_line": 95, "docstring": null, "calls": ["SamplingParams"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::inference", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "inference", "kind": "Function", "parent": "VLLMInferencer", "start_line": 98, "end_line": 175, "docstring": "Perform inference using the provided model and dataset. Will save inference results if\n`save_results` is set to True in `inferencer_args`.\n\nParameters\n----------\nmodel : HFDecoderModel\n    LMFlow HFDecoderModel object\ndataset : Dataset\n    LMFlow Dataset object\napply_chat_template : bool, optional\n    Whether to apply chat template to the input, by default True.\nenable_decode_inference_result : bool, optional\n    Whether to decode after generation, by default False.\nrelease_gpu : bool, optional\n    Whether to release gpu resources, by default False. \ninference_args : InferencerArguments, optional\n    by default None\n\nReturns\n-------\nList[VLLMInferenceResultWithInput]\n    Return a list of VLLMInferenceResultWithInput, where each\n    element contains the input prompt and the corresponding output.\n    \n    When `enable_decode_inference_result = True`, the output would be a list of strings,\n    contains sampling_params.n samples for the corresponding prompt.\n    \n    When `enable_decode_inference_result = False`, return a list of list of ints \n    (token ids, no decoding after generation).", "calls": ["prepare_inputs_for_inference", "warning", "parse_to_sampling_params", "_distributed_inference", "_inference", "save_inference_results", "get", "get"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::_inference", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "_inference", "kind": "Function", "parent": "VLLMInferencer", "start_line": 178, "end_line": 194, "docstring": null, "calls": ["inference"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::_distributed_inference", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "_distributed_inference", "kind": "Function", "parent": "VLLMInferencer", "start_line": 197, "end_line": 291, "docstring": null, "calls": ["map_batches", "to_pandas", "info", "placement_group", "dict", "deepcopy", "activate_model_for_inference", "inference", "iterrows", "PlacementGroupSchedulingStrategy", "head"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::save_inference_results", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "save_inference_results", "kind": "Function", "parent": "VLLMInferencer", "start_line": 294, "end_line": 302, "docstring": null, "calls": ["info", "open", "dump"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::load_inference_results", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "load_inference_results", "kind": "Function", "parent": "VLLMInferencer", "start_line": 305, "end_line": 312, "docstring": null, "calls": ["open", "load"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::__init__", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "__init__", "kind": "Function", "parent": "MemorySafeVLLMInferencer", "start_line": 316, "end_line": 324, "docstring": null, "calls": ["__init__", "files", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::inference", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "inference", "kind": "Function", "parent": "MemorySafeVLLMInferencer", "start_line": 327, "end_line": 365, "docstring": null, "calls": ["make_shell_args_from_dataclass", "copy", "run", "info", "load_inference_results", "info", "pop", "warning", "RuntimeError", "str"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::scheduling_strategy_fn", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "scheduling_strategy_fn", "kind": "Function", "parent": "_distributed_inference", "start_line": 209, "end_line": 222, "docstring": null, "calls": ["placement_group", "dict", "PlacementGroupSchedulingStrategy"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::DistributedPredictor", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "DistributedPredictor", "kind": "Class", "parent": "_distributed_inference", "start_line": 236, "end_line": 267, "docstring": null, "calls": ["deepcopy", "activate_model_for_inference", "inference"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::__init__", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "__init__", "kind": "Function", "parent": "DistributedPredictor", "start_line": 237, "end_line": 252, "docstring": null, "calls": ["deepcopy", "activate_model_for_inference"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.vllm_inferencer::__call__", "path": "v2/src/lmflow/pipeline/vllm_inferencer.py", "module": "v2.src.lmflow.pipeline.vllm_inferencer", "name": "__call__", "kind": "Function", "parent": "DistributedPredictor", "start_line": 254, "end_line": 267, "docstring": "batch: Dict[str, np.ndarray], {\"item\": array(['...', '...', '...', ...])}\n                ", "calls": ["inference"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::rstrip_partial_utf8", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "rstrip_partial_utf8", "kind": "Function", "parent": null, "start_line": 33, "end_line": 34, "docstring": null, "calls": ["replace"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::Inferencer", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "Inferencer", "kind": "Class", "parent": null, "start_line": 43, "end_line": 321, "docstring": "Initializes the `Inferencer` class with given arguments.\n\nParameters\n------------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\ninferencer_args : InferencerArguments object.\n    Contains the arguments required to perform inference.", "calls": ["set_random_seed", "int", "int", "from_pretrained", "len", "range", "batchlize", "create_dataloader", "enumerate", "Dataset", "from_dict", "getenv", "getenv", "set_device", "init_distributed", "init_process_group", "Accelerator", "wait_for_everyone", "get_type", "to_dict", "append", "get_type", "NotImplementedError", "isinstance", "append", "DatasetArguments", "stream_chat", "range", "print", "get_type"], "decorators": [], "num_calls": 81}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::SpeculativeInferencer", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "SpeculativeInferencer", "kind": "Class", "parent": null, "start_line": 324, "end_line": 577, "docstring": "Ref: [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192)\n\nParameters\n------------\ntarget_model_args : ModelArguments object.\n    Contains the arguments required to load the target model.\n    \ndraft_model_args : ModelArguments object.\n    Contains the arguments required to load the draft model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\ninferencer_args : InferencerArguments object.\n    Contains the arguments required to perform inference.", "calls": ["__init__", "from_pretrained", "multinomial", "inference", "range", "len", "decode", "NotImplementedError", "float", "gather", "predict_next_token", "score_to_prob", "sample", "append", "cat", "to", "debug", "autoregressive_sampling", "debug", "debug", "debug", "range", "debug", "concat", "debug", "debug", "speculative_sampling", "debug", "debug", "super"], "decorators": [], "num_calls": 64}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::ToolInferencer", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "ToolInferencer", "kind": "Class", "parent": null, "start_line": 579, "end_line": 655, "docstring": "Initializes the `ToolInferencer` class with given arguments.\n\nParameters\n------------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\ninferencer_args : InferencerArguments object.\n    Contains the arguments required to perform inference.", "calls": ["__init__", "HFDecoderModel", "debug", "inference", "decode", "replace", "run", "to", "print", "print", "print", "print", "super", "to", "encode", "encode"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::__init__", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "__init__", "kind": "Function", "parent": "Inferencer", "start_line": 60, "end_line": 88, "docstring": null, "calls": ["set_random_seed", "int", "int", "from_pretrained", "getenv", "getenv", "set_device", "init_distributed", "init_process_group", "Accelerator", "wait_for_everyone", "print"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::create_dataloader", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "create_dataloader", "kind": "Function", "parent": "Inferencer", "start_line": 91, "end_line": 121, "docstring": "Batchlize dataset and format it to dataloader.\n\nArgs:\n    dataset (Dataset): the dataset object\n\nOutput:\n    dataloader (batchlize): the dataloader object\n    dataset_size (int): the length of the dataset", "calls": ["len", "range", "batchlize", "get_type", "to_dict", "append", "get_type", "to_list"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::inference", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "inference", "kind": "Function", "parent": "Inferencer", "start_line": 124, "end_line": 274, "docstring": "Perform inference for a model\n\nParameters\n------------\nmodel : TunableModel object.\n    TunableModel to perform inference\n\ndataset : Dataset object.\n\n\nReturns:\n\noutput_dataset: Dataset object.", "calls": ["create_dataloader", "enumerate", "Dataset", "from_dict", "get_type", "NotImplementedError", "isinstance", "append", "DatasetArguments", "format", "format", "format", "isinstance", "array", "split", "deepcopy", "range", "cat", "cat", "inference", "decode", "len", "decode", "len", "len", "to", "append", "append", "append", "len"], "decorators": [], "num_calls": 49}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::stream_inference", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "stream_inference", "kind": "Function", "parent": "Inferencer", "start_line": 276, "end_line": 321, "docstring": null, "calls": ["stream_chat", "range", "get_tokenizer", "rstrip_partial_utf8", "inference", "rstrip_partial_utf8", "to_dict", "from_dict", "get_backend_model", "index", "index", "to_dict"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::__init__", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "__init__", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 344, "end_line": 353, "docstring": null, "calls": ["__init__", "from_pretrained", "super", "print"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::score_to_prob", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "score_to_prob", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 357, "end_line": 400, "docstring": "Convert scores (NOT softmaxed tensor) to probabilities with support for temperature, top-p sampling, and argmax.\n\nParameters\n----------\nscores : torch.Tensor\n    Input scores.\ntemperature : float, optional\n    Temperature parameter for controlling randomness. Higher values make the distribution more uniform, \n    lower values make it peakier. When temperature <= 1e-6, argmax is used. by default 0.0\ntop_p : float, optional\n    Top-p sampling parameter for controlling the cumulative probability threshold, by default 1.0 (no threshold)\n\nReturns\n-------\ntorch.Tensor\n    Probability distribution after adjustments.", "calls": ["float", "sort", "softmax", "cumsum", "any", "softmax", "one_hot", "zeros_like", "scatter_add_", "softmax", "argmax", "sum", "argsort", "size"], "decorators": ["staticmethod"], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::sample", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "sample", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 404, "end_line": 408, "docstring": "Sample from a tensor of probabilities\n        ", "calls": ["multinomial", "gather"], "decorators": ["staticmethod"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::predict_next_token", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "predict_next_token", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 412, "end_line": 422, "docstring": "Predict the next token given the input_ids.\n        ", "calls": ["inference"], "decorators": ["staticmethod"], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::autoregressive_sampling", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "autoregressive_sampling", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 425, "end_line": 442, "docstring": "Ref: [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192) Section 2.2\n        ", "calls": ["range", "predict_next_token", "score_to_prob", "sample", "append", "cat"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::inference", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "inference", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 445, "end_line": 573, "docstring": "Perform inference for a model\n\nParameters\n------------\nmodel : HFDecoderModel object.\n    TunableModel to verify tokens generated by the draft model.\n    \ndraft_model : HFDecoderModel object.\n    TunableModel that provides approximations of the target model.\n\ninput : str.\n    The input text (i.e., the prompt) for the model.\n    \ngamma : int.\n    The number of tokens to be generated by the draft model within each iter.\n    \nmax_new_tokens : int.\n    The maximum number of tokens to be generated by the target model.\n    \n\nReturns\n-------\noutput: str.\n    The output text generated by the model.", "calls": ["len", "decode", "to", "debug", "autoregressive_sampling", "debug", "debug", "debug", "range", "debug", "concat", "debug", "debug", "speculative_sampling", "debug", "debug", "to", "NotImplementedError", "get_backend_model", "all", "score_to_prob", "max", "score_to_prob", "sample", "len", "len", "encode", "score_to_prob", "rand_like", "zeros_like"], "decorators": [], "num_calls": 36}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::stream_inference", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "stream_inference", "kind": "Function", "parent": "SpeculativeInferencer", "start_line": 576, "end_line": 577, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::__init__", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "__init__", "kind": "Function", "parent": "ToolInferencer", "start_line": 596, "end_line": 599, "docstring": null, "calls": ["__init__", "HFDecoderModel", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::inference", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "inference", "kind": "Function", "parent": "ToolInferencer", "start_line": 601, "end_line": 641, "docstring": "Perform inference for a model\n\nParameters\n------------\nmodel : HFDecoderModel object.\n    TunableModel to perform inference\n\ninput : str.\n    The input text (i.e., the prompt) for the model. \n    \nmax_new_tokens : int.\n    The maximum number of tokens to be generated by the model.\n\nReturns:\n\noutput : str.\n    The output text generated by the model.", "calls": ["debug", "inference", "decode", "replace", "to", "to", "encode", "encode"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::code_exec", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "code_exec", "kind": "Function", "parent": "ToolInferencer", "start_line": 643, "end_line": 655, "docstring": null, "calls": ["run", "print", "print", "print", "print"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.inferencer::speculative_sampling", "path": "v2/src/lmflow/pipeline/inferencer.py", "module": "v2.src.lmflow.pipeline.inferencer", "name": "speculative_sampling", "kind": "Function", "parent": "inference", "start_line": 492, "end_line": 553, "docstring": "Ref: [arXiv:2211.17192v2](https://arxiv.org/abs/2211.17192)\n\nParameters\n----------\ninput_ids : torch.Tensor\ndraft_model : TunableModel object\nmodel_list : List[TunableModel object]\n\nReturns\n-------\ntorch.Tensor", "calls": ["debug", "autoregressive_sampling", "debug", "debug", "debug", "range", "debug", "concat", "get_backend_model", "all", "score_to_prob", "max", "score_to_prob", "sample", "score_to_prob", "rand_like", "zeros_like", "sum", "decode", "sum", "sum"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::RaftAligner", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "RaftAligner", "kind": "Class", "parent": null, "start_line": 37, "end_line": 693, "docstring": "Initializes the `RaftAligner` class with given arguments.\n\nParameters\n------------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\nraft_aligner_args : RaftAlignerArguments object.\n    Contains the arguments required to perform alignment.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.", "calls": ["basicConfig", "setLevel", "RaftTrainer", "get_logger", "get_backend_dataset", "map", "filter", "set_format", "strip", "time", "len", "enumerate", "int", "all_gather_object", "range", "append", "append", "info", "info", "all_gather_object", "range", "DatasetDict", "time", "len", "enumerate", "range", "int", "all_gather_object", "all_gather_object", "all_gather_object"], "decorators": [], "num_calls": 215}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::__init__", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "__init__", "kind": "Function", "parent": "RaftAligner", "start_line": 59, "end_line": 80, "docstring": null, "calls": ["basicConfig", "setLevel", "makedirs", "dirname", "remove", "StreamHandler"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_initialize_trainer", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_initialize_trainer", "kind": "Function", "parent": "RaftAligner", "start_line": 83, "end_line": 97, "docstring": "This function takes the model and tokenizer as the input and initialize the trainer.", "calls": ["RaftTrainer", "from_dict", "from_dict"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_load_dataset", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_load_dataset", "kind": "Function", "parent": "RaftAligner", "start_line": 100, "end_line": 220, "docstring": "This function prepares the dataset for every iteration.", "calls": ["get_logger", "list", "list", "main_process_first", "min", "len", "copy", "main_process_first", "CaptureLogger", "tokenizer", "warning", "map", "map", "warning", "warning", "list", "map", "map", "ValueError", "min", "select", "chain", "keys", "items", "len", "range", "list", "range", "keys"], "decorators": [], "num_calls": 29}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_load_input_dataset", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_load_input_dataset", "kind": "Function", "parent": "RaftAligner", "start_line": 223, "end_line": 247, "docstring": "Load input dataset (i.e. prompt/question dataset) for training.\n\nArgs:\n    dataset: A Dataset object.\n        The dataset to be loaded.\n\nReturns:\n    dataloader (`torch.utils.data.DataLoader`):\n        The dataloader for the dataset.", "calls": ["get_backend_dataset", "map", "filter", "set_format", "encode", "decode", "len"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_clean_text", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_clean_text", "kind": "Function", "parent": "RaftAligner", "start_line": 249, "end_line": 253, "docstring": null, "calls": ["strip", "len", "split", "strip"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_discard_sample", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_discard_sample", "kind": "Function", "parent": "RaftAligner", "start_line": 255, "end_line": 260, "docstring": null, "calls": ["len"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_get_batch_dataset_top", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_get_batch_dataset_top", "kind": "Function", "parent": "RaftAligner", "start_line": 262, "end_line": 389, "docstring": ":param batch_input: input prompts", "calls": ["time", "len", "enumerate", "int", "all_gather_object", "range", "append", "append", "info", "info", "all_gather_object", "range", "DatasetDict", "append", "getenv", "extend", "extend", "mean", "mean", "plot", "plot", "legend", "savefig", "close", "randint", "to", "batch_decode", "create_from_dict", "inference", "extend"], "decorators": [], "num_calls": 60}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::_get_batch_dataset_local", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "_get_batch_dataset_local", "kind": "Function", "parent": "RaftAligner", "start_line": 391, "end_line": 546, "docstring": ":param batch_input: input prompts", "calls": ["time", "len", "enumerate", "range", "int", "all_gather_object", "all_gather_object", "all_gather_object", "range", "info", "append", "append", "all_gather_object", "range", "info", "DatasetDict", "randint", "to", "batch_decode", "create_from_dict", "inference", "append", "range", "argmax", "append", "len", "append", "getenv", "extend", "extend"], "decorators": [], "num_calls": 68}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::align", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "align", "kind": "Function", "parent": "RaftAligner", "start_line": 549, "end_line": 693, "docstring": "Perform alignment for a model\n\nParameters\n------------\nmodel : BaseModel object.\ndataset: Dataset object.\n    Input dataset for model to generate outputs. The input and output\n        will then be feed into reward model to get the reward for\n        alignment.\nreward_model: RegressionModel object.", "calls": ["get_tokenizer", "_load_input_dataset", "set_caching_enabled", "get_backend_model", "int", "set_seed", "print", "len", "arange", "shuffle", "_initialize_trainer", "train", "range", "getenv", "int", "print", "set_seed", "min", "select", "gradient_checkpointing_disable", "time", "time", "info", "_load_dataset", "info", "time", "gradient_checkpointing_enable", "train", "time", "info"], "decorators": [], "num_calls": 37}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::tokenize_function", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "tokenize_function", "kind": "Function", "parent": "_load_dataset", "start_line": 123, "end_line": 132, "docstring": null, "calls": ["CaptureLogger", "tokenizer", "warning"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::group_texts", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "group_texts", "kind": "Function", "parent": "_load_dataset", "start_line": 169, "end_line": 183, "docstring": null, "calls": ["len", "copy", "list", "chain", "keys", "items", "list", "range", "keys"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.pipeline.raft_aligner::tokenize", "path": "v2/src/lmflow/pipeline/raft_aligner.py", "module": "v2.src.lmflow.pipeline.raft_aligner", "name": "tokenize", "kind": "Function", "parent": "_load_input_dataset", "start_line": 237, "end_line": 240, "docstring": null, "calls": ["encode", "decode"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.auto_pipeline::AutoPipeline", "path": "v2/src/lmflow/pipeline/auto_pipeline.py", "module": "v2.src.lmflow.pipeline.auto_pipeline", "name": "AutoPipeline", "kind": "Class", "parent": null, "start_line": 53, "end_line": 84, "docstring": "The class designed to return a pipeline automatically based on its name.", "calls": ["NotImplementedError", "NotImplementedError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.auto_pipeline::get_pipeline", "path": "v2/src/lmflow/pipeline/auto_pipeline.py", "module": "v2.src.lmflow.pipeline.auto_pipeline", "name": "get_pipeline", "kind": "Function", "parent": "AutoPipeline", "start_line": 58, "end_line": 84, "docstring": null, "calls": ["NotImplementedError", "NotImplementedError"], "decorators": ["classmethod"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::RewardModelInferencer", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "RewardModelInferencer", "kind": "Class", "parent": null, "start_line": 51, "end_line": 359, "docstring": "Initializes the `Inferencer` class with given arguments.\n\nParameters\n------------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\ninferencer_args : InferencerArguments object.\n    Contains the arguments required to perform inference.", "calls": ["set_random_seed", "int", "int", "isinstance", "prepare_inputs_for_inference", "batchlize", "len", "tqdm", "map_batches", "to_pandas", "info", "NotImplementedError", "tolist", "getenv", "getenv", "set_device", "init_distributed", "init_process_group", "get", "warning", "deepcopy", "__vllm_inference", "_inference", "postprocess_distributed_inference_outputs", "postprocess_inference_outputs", "__distributed_inference", "__inference", "get_type", "flatten_list", "to"], "decorators": [], "num_calls": 74}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__init__", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__init__", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 66, "end_line": 90, "docstring": null, "calls": ["set_random_seed", "int", "int", "getenv", "getenv", "set_device", "init_distributed", "init_process_group", "get", "Accelerator"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::inference", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "inference", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 93, "end_line": 142, "docstring": null, "calls": ["isinstance", "prepare_inputs_for_inference", "warning", "deepcopy", "__vllm_inference", "_inference", "postprocess_distributed_inference_outputs", "postprocess_inference_outputs"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::_inference", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "_inference", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 145, "end_line": 168, "docstring": null, "calls": ["__distributed_inference", "__inference", "is_ray_available", "ImportError", "get", "get"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__inference", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__inference", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 171, "end_line": 215, "docstring": null, "calls": ["batchlize", "len", "tqdm", "get_type", "flatten_list", "to", "__post_process_model_output", "extend", "get_type", "compress_list", "get_backend_dataset", "enumerate", "inference", "get_backend_dataset", "LongTensor", "autocast", "inference"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__distributed_inference", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__distributed_inference", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 218, "end_line": 317, "docstring": null, "calls": ["map_batches", "to_pandas", "info", "placement_group", "dict", "HFTextRegressionModel", "activate_model_for_inference", "len", "to", "tolist", "iterrows", "PlacementGroupSchedulingStrategy", "inference", "tolist", "head", "flatten", "squeeze", "range", "LongTensor", "reshape", "range", "len", "to", "list", "range"], "decorators": [], "num_calls": 25}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__vllm_inference", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__vllm_inference", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 320, "end_line": 326, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__post_process_model_output", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__post_process_model_output", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 329, "end_line": 335, "docstring": null, "calls": ["tolist", "reshape", "to"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::flatten_list", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "flatten_list", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 338, "end_line": 344, "docstring": null, "calls": ["len"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::compress_list", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "compress_list", "kind": "Function", "parent": "RewardModelInferencer", "start_line": 347, "end_line": 359, "docstring": null, "calls": ["sum", "len", "append"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::scheduling_strategy_fn", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "scheduling_strategy_fn", "kind": "Function", "parent": "__distributed_inference", "start_line": 225, "end_line": 238, "docstring": null, "calls": ["placement_group", "dict", "PlacementGroupSchedulingStrategy"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::DistributedPredictor", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "DistributedPredictor", "kind": "Class", "parent": "__distributed_inference", "start_line": 252, "end_line": 297, "docstring": null, "calls": ["HFTextRegressionModel", "activate_model_for_inference", "len", "to", "tolist", "inference", "tolist", "flatten", "squeeze", "range", "LongTensor", "reshape", "range", "len", "to", "list", "range"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__init__", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__init__", "kind": "Function", "parent": "DistributedPredictor", "start_line": 253, "end_line": 262, "docstring": null, "calls": ["HFTextRegressionModel", "activate_model_for_inference"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.rm_inferencer::__call__", "path": "v2/src/lmflow/pipeline/rm_inferencer.py", "module": "v2.src.lmflow.pipeline.rm_inferencer", "name": "__call__", "kind": "Function", "parent": "DistributedPredictor", "start_line": 264, "end_line": 297, "docstring": "batch: Dict[str, np.ndarray]\nExample (batch size=2):\n{'input': array(['...','...'], dtype=object),\n 'output': array([array([\"...\", \"...\"], dtype=object), array(['...','...'], dtype=object)], dtype=object),\n 'input_ids': array([[[128000, 128006,    882, ..., 128256, 128256, 128256],\n         [128000, 128006,    882, ..., 128256, 128256, 128256]],\n        [[128000, 128006,    882, ..., 128256, 128256, 128256],\n         [128000, 128006,    882, ..., 128256, 128256, 128256]]])}", "calls": ["len", "to", "tolist", "inference", "tolist", "flatten", "squeeze", "range", "LongTensor", "reshape", "range", "len", "to", "list", "range"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::RewardModelTuner", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "RewardModelTuner", "kind": "Class", "parent": null, "start_line": 26, "end_line": 233, "docstring": "Initializes the `RewardModelTuner` class.\n\nParameters\n----------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\nfinetuner_args : RewardModelTunerArguments object.\n    Contains the arguments required to perform finetuning.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.", "calls": ["__init__", "get_backend_dataset", "info", "RewardModelingTrainer", "deepcopy", "main_process_first", "tokenize", "min", "select", "deepcopy", "Dataset", "get_backend_dataset", "info", "RewardDataCollatorWithPadding", "DynamicLayerActivationCallback", "append", "train", "save_model", "min", "log_metrics", "save_metrics", "save_state", "push_to_hub", "create_model_card", "super", "group_text", "len", "range", "main_process_first", "tokenize"], "decorators": [], "num_calls": 58}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::__init__", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "__init__", "kind": "Function", "parent": "RewardModelTuner", "start_line": 46, "end_line": 54, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::tune", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "tune", "kind": "Function", "parent": "RewardModelTuner", "start_line": 57, "end_line": 233, "docstring": null, "calls": ["get_backend_dataset", "info", "RewardModelingTrainer", "deepcopy", "main_process_first", "tokenize", "min", "select", "deepcopy", "Dataset", "get_backend_dataset", "info", "RewardDataCollatorWithPadding", "DynamicLayerActivationCallback", "append", "train", "save_model", "min", "log_metrics", "save_metrics", "save_state", "push_to_hub", "create_model_card", "group_text", "len", "range", "main_process_first", "tokenize", "get_backend_model", "get_tokenizer"], "decorators": [], "num_calls": 56}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::DynamicLayerActivationCallback", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "DynamicLayerActivationCallback", "kind": "Class", "parent": null, "start_line": 117, "end_line": 165, "docstring": null, "calls": ["__init__", "len", "eval", "freeze_all_layers", "eval", "choice", "print", "get", "eval", "parameters", "switch_active_layers", "range", "parameters", "super"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::__init__", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "__init__", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 118, "end_line": 140, "docstring": null, "calls": ["__init__", "len", "get", "eval", "super"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::freeze_all_layers", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "freeze_all_layers", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 142, "end_line": 146, "docstring": null, "calls": ["eval", "parameters"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::on_step_begin", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "on_step_begin", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 148, "end_line": 151, "docstring": null, "calls": ["switch_active_layers"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.rm_tuner::switch_active_layers", "path": "v2/src/lmflow/pipeline/rm_tuner.py", "module": "v2.src.lmflow.pipeline.rm_tuner", "name": "switch_active_layers", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 153, "end_line": 165, "docstring": null, "calls": ["freeze_all_layers", "eval", "choice", "print", "range", "parameters"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.base_aligner::BaseAligner", "path": "v2/src/lmflow/pipeline/base_aligner.py", "module": "v2.src.lmflow.pipeline.base_aligner", "name": "BaseAligner", "kind": "Class", "parent": null, "start_line": 9, "end_line": 21, "docstring": "A subclass of BasePipeline which is alignable.\n    ", "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.base_aligner::__init__", "path": "v2/src/lmflow/pipeline/base_aligner.py", "module": "v2.src.lmflow.pipeline.base_aligner", "name": "__init__", "kind": "Function", "parent": "BaseAligner", "start_line": 12, "end_line": 13, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.base_aligner::_check_if_alignable", "path": "v2/src/lmflow/pipeline/base_aligner.py", "module": "v2.src.lmflow.pipeline.base_aligner", "name": "_check_if_alignable", "kind": "Function", "parent": "BaseAligner", "start_line": 15, "end_line": 18, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.base_aligner::align", "path": "v2/src/lmflow/pipeline/base_aligner.py", "module": "v2.src.lmflow.pipeline.base_aligner", "name": "align", "kind": "Function", "parent": "BaseAligner", "start_line": 20, "end_line": 21, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.base_pipeline::BasePipeline", "path": "v2/src/lmflow/pipeline/base_pipeline.py", "module": "v2.src.lmflow.pipeline.base_pipeline", "name": "BasePipeline", "kind": "Class", "parent": null, "start_line": 8, "end_line": 9, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::DPOv2Aligner", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "DPOv2Aligner", "kind": "Class", "parent": null, "start_line": 41, "end_line": 298, "docstring": null, "calls": ["DPOv2Trainer", "train", "save_model", "join", "save_pretrained", "TrainingArguments", "debug", "tqdm", "deepcopy", "Dataset", "from_dict", "ValueError", "warning", "get_type", "convert_to_paired_dataset", "select", "get_type", "convert_to_paired_dataset", "get_backend_model", "get_backend_model", "no_grad", "empty_cache", "get_type", "get_backend_dataset", "_calc_response_lengths", "_calc_reward_with_length_penalty", "sampling_paired_idx_from_rewards", "append", "NotImplementedError", "len"], "decorators": [], "num_calls": 60}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::MemorySafeDPOv2Aligner", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "MemorySafeDPOv2Aligner", "kind": "Class", "parent": null, "start_line": 301, "end_line": 349, "docstring": null, "calls": ["ReferenceModelArguments", "make_shell_args_from_dataclass", "copy", "run", "info", "files", "pop", "print", "RuntimeError", "add_dataclass_attr_prefix", "str"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::__init__", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "__init__", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 42, "end_line": 52, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::align", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "align", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 55, "end_line": 124, "docstring": null, "calls": ["DPOv2Trainer", "train", "save_model", "join", "save_pretrained", "ValueError", "warning", "get_type", "convert_to_paired_dataset", "select", "get_type", "convert_to_paired_dataset", "get_backend_model", "get_backend_model", "no_grad", "empty_cache", "get_type", "get_type", "range", "get_backend_dataset", "get_backend_dataset", "__prepare_training_args", "get_backend_model", "get_backend_model", "get_type"], "decorators": [], "num_calls": 25}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::__prepare_training_args", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "__prepare_training_args", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 127, "end_line": 154, "docstring": null, "calls": ["TrainingArguments", "debug"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::convert_to_paired_dataset", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "convert_to_paired_dataset", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 157, "end_line": 199, "docstring": "Convert a scored one to multiple (text_to_scored_textlist) to a paired dataset by rejection sampling.\n        ", "calls": ["tqdm", "deepcopy", "Dataset", "from_dict", "get_type", "get_backend_dataset", "_calc_response_lengths", "_calc_reward_with_length_penalty", "sampling_paired_idx_from_rewards", "append", "get_type"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::_calc_response_lengths", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "_calc_response_lengths", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 202, "end_line": 216, "docstring": null, "calls": ["NotImplementedError", "len"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::_calc_reward_with_length_penalty", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "_calc_reward_with_length_penalty", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 219, "end_line": 229, "docstring": "When length_penalty > 0, penalize the longer sequence by subtracting \nlength_penalty * length from the reward. Vice versa when length_penalty < 0.", "calls": ["len", "len", "zip"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::sampling_paired_idx_from_rewards", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "sampling_paired_idx_from_rewards", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 232, "end_line": 248, "docstring": "Prepare the dataset for DPO training by rejection sampling.\nWe implement different strategies to select pairs, including\nrandom: randomly select two instances\nmax_min: best v.s. worst\nmax_max: best v.s. second best\nmax_random: best v.s. random from the remaining", "calls": ["_sampling_paired_idx_from_rewards_fast", "_sampling_paired_idx_from_rewards"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::_sampling_paired_idx_from_rewards", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "_sampling_paired_idx_from_rewards", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 251, "end_line": 273, "docstring": null, "calls": ["choice", "len", "argmax", "argmin", "argsort", "argmax", "choice", "ValueError", "range", "len"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::_sampling_paired_idx_from_rewards_fast", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "_sampling_paired_idx_from_rewards_fast", "kind": "Function", "parent": "DPOv2Aligner", "start_line": 276, "end_line": 298, "docstring": null, "calls": ["argmax", "argmin", "argsort", "argmax", "ValueError"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::__init__", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "__init__", "kind": "Function", "parent": "MemorySafeDPOv2Aligner", "start_line": 302, "end_line": 313, "docstring": null, "calls": ["ReferenceModelArguments", "files", "add_dataclass_attr_prefix"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.dpov2_aligner::align", "path": "v2/src/lmflow/pipeline/dpov2_aligner.py", "module": "v2.src.lmflow.pipeline.dpov2_aligner", "name": "align", "kind": "Function", "parent": "MemorySafeDPOv2Aligner", "start_line": 315, "end_line": 349, "docstring": null, "calls": ["make_shell_args_from_dataclass", "copy", "run", "info", "pop", "print", "RuntimeError", "str"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::Finetuner", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "Finetuner", "kind": "Class", "parent": null, "start_line": 48, "end_line": 664, "docstring": "Initializes the `Finetuner` class with given arguments.\n\nParameters\n------------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\nfinetuner_args : FinetunerArguments object.\n    Contains the arguments required to perform finetuning.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.", "calls": ["send_example_telemetry", "basicConfig", "get_process_log_level", "setLevel", "set_verbosity", "set_verbosity", "enable_default_handler", "enable_explicit_format", "warning", "info", "set_seed", "get_backend_dataset", "info", "FinetuningTrainer", "isdir", "get_last_checkpoint", "len", "main_process_first", "deepcopy", "register_tokenizer", "deepcopy", "Dataset", "get_backend_dataset", "info", "load", "create_customized_optimizer", "DynamicLayerActivationCallback", "append", "train", "min"], "decorators": [], "num_calls": 139}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::__init__", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "__init__", "kind": "Function", "parent": "Finetuner", "start_line": 70, "end_line": 132, "docstring": null, "calls": ["send_example_telemetry", "basicConfig", "get_process_log_level", "setLevel", "set_verbosity", "set_verbosity", "enable_default_handler", "enable_explicit_format", "warning", "info", "set_seed", "isdir", "get_last_checkpoint", "ValueError", "StreamHandler", "bool", "len", "info", "listdir"], "decorators": [], "num_calls": 19}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::group_text", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "group_text", "kind": "Function", "parent": "Finetuner", "start_line": 135, "end_line": 236, "docstring": "Groups texts together to form blocks of maximum length `model_max_length` and returns the processed data as\na dictionary.", "calls": ["len", "main_process_first", "warning", "range", "map", "map", "warning", "warning", "len", "extend", "extend", "extend", "list", "list", "items", "chain", "keys", "chain", "keys", "list", "range", "len", "keys"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::create_customized_optimizer", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "create_customized_optimizer", "kind": "Function", "parent": "Finetuner", "start_line": 238, "end_line": 442, "docstring": null, "calls": ["is_sagemaker_mp_enabled", "split", "update", "is_sagemaker_mp_enabled", "get_decay_parameter_names", "get_optimizer_cls_and_kwargs", "optimizer_cls", "DistributedOptimizer", "split", "update", "pop", "pop", "replace", "update", "update", "update", "named_parameters", "named_parameters", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update"], "decorators": [], "num_calls": 34}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::tune", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "tune", "kind": "Function", "parent": "Finetuner", "start_line": 444, "end_line": 664, "docstring": "Perform tuning for a model\n\nParameters\n------------\nmodel : TunableModel object.\n    TunableModel to perform tuning.\n\ndataset:\n    dataset to train model.", "calls": ["get_backend_dataset", "info", "FinetuningTrainer", "deepcopy", "register_tokenizer", "deepcopy", "Dataset", "get_backend_dataset", "info", "load", "create_customized_optimizer", "DynamicLayerActivationCallback", "append", "train", "min", "log_metrics", "save_metrics", "save_state", "push_to_hub", "create_model_card", "main_process_first", "tokenize", "main_process_first", "tokenize", "isinstance", "argmax", "reshape", "reshape", "compute", "min"], "decorators": [], "num_calls": 63}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::group_texts", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "group_texts", "kind": "Function", "parent": "group_text", "start_line": 181, "end_line": 206, "docstring": null, "calls": ["len", "range", "len", "extend", "extend", "extend", "list", "list", "items", "chain", "keys", "chain", "keys", "list", "range", "len", "keys"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::CustomizedOptimTrainer", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "CustomizedOptimTrainer", "kind": "Class", "parent": "create_customized_optimizer", "start_line": 239, "end_line": 440, "docstring": null, "calls": ["is_sagemaker_mp_enabled", "split", "update", "is_sagemaker_mp_enabled", "get_decay_parameter_names", "get_optimizer_cls_and_kwargs", "optimizer_cls", "DistributedOptimizer", "split", "update", "pop", "pop", "replace", "update", "update", "update", "named_parameters", "named_parameters", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update"], "decorators": [], "num_calls": 34}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::get_optimizer_cls_and_kwargs", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "get_optimizer_cls_and_kwargs", "kind": "Function", "parent": "CustomizedOptimTrainer", "start_line": 242, "end_line": 394, "docstring": null, "calls": ["split", "update", "split", "update", "replace", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "update", "ValueError"], "decorators": ["staticmethod"], "num_calls": 24}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::create_optimizer", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "create_optimizer", "kind": "Function", "parent": "CustomizedOptimTrainer", "start_line": 396, "end_line": 440, "docstring": null, "calls": ["is_sagemaker_mp_enabled", "is_sagemaker_mp_enabled", "get_decay_parameter_names", "get_optimizer_cls_and_kwargs", "optimizer_cls", "DistributedOptimizer", "pop", "pop", "named_parameters", "named_parameters"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::preprocess_logits_for_metrics", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "preprocess_logits_for_metrics", "kind": "Function", "parent": null, "start_line": 502, "end_line": 507, "docstring": null, "calls": ["isinstance", "argmax"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::compute_metrics", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "compute_metrics", "kind": "Function", "parent": null, "start_line": 511, "end_line": 517, "docstring": null, "calls": ["reshape", "reshape", "compute"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::DynamicLayerActivationCallback", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "DynamicLayerActivationCallback", "kind": "Class", "parent": null, "start_line": 543, "end_line": 592, "docstring": null, "calls": ["__init__", "len", "eval", "freeze_all_layers", "eval", "choice", "print", "eval", "parameters", "switch_active_layers", "range", "parameters", "super"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::__init__", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "__init__", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 544, "end_line": 567, "docstring": null, "calls": ["__init__", "len", "eval", "super"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::freeze_all_layers", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "freeze_all_layers", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 569, "end_line": 573, "docstring": null, "calls": ["eval", "parameters"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::on_step_begin", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "on_step_begin", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 575, "end_line": 578, "docstring": null, "calls": ["switch_active_layers"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.finetuner::switch_active_layers", "path": "v2/src/lmflow/pipeline/finetuner.py", "module": "v2.src.lmflow.pipeline.finetuner", "name": "switch_active_layers", "kind": "Function", "parent": "DynamicLayerActivationCallback", "start_line": 580, "end_line": 592, "docstring": null, "calls": ["freeze_all_layers", "eval", "choice", "print", "range", "parameters"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::Evaluator", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "Evaluator", "kind": "Class", "parent": null, "start_line": 24, "end_line": 493, "docstring": "Initializes the `Evaluator` class with given arguments.\n\nParameters\n------------\nmodel_args : ModelArguments object.\n    Contains the arguments required to load the model.\n\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\nevaluator_args : EvaluatorArguments object.\n    Contains the arguments required to perform evaluation.", "calls": ["set_random_seed", "int", "int", "set_device", "from_pretrained", "print", "to_dict", "len", "range", "batchlize", "print", "create_dataloader", "enumerate", "create_dataloader", "enumerate", "to_dict", "size", "range", "exp", "to_dict", "from_dict", "tokenize", "get_backend_dataset", "len", "enumerate", "init", "getenv", "getenv", "Accelerator", "wait_for_everyone"], "decorators": [], "num_calls": 204}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::__init__", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "__init__", "kind": "Function", "parent": "Evaluator", "start_line": 41, "end_line": 73, "docstring": null, "calls": ["set_random_seed", "int", "int", "set_device", "from_pretrained", "print", "init", "getenv", "getenv", "Accelerator", "wait_for_everyone", "init_distributed", "print"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::create_dataloader", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "create_dataloader", "kind": "Function", "parent": "Evaluator", "start_line": 77, "end_line": 97, "docstring": null, "calls": ["to_dict", "len", "range", "batchlize", "print", "append", "len"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::_match", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "_match", "kind": "Function", "parent": "Evaluator", "start_line": 102, "end_line": 115, "docstring": null, "calls": ["lower", "lower"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::evaluate", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "evaluate", "kind": "Function", "parent": "Evaluator", "start_line": 118, "end_line": 153, "docstring": "Perform Evaluation for a model\n\nParameters\n------------\nmodel : TunableModel object.\n    TunableModel to perform inference\n\ndataset : Dataset object.\n    ", "calls": ["print", "_evaluate_acc_with_accelerator", "_evaluate_acc_with_deepspeed", "_evaluate_ppl", "print", "_evaluate_nll", "print", "NotImplementedError"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::_evaluate_acc_with_accelerator", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "_evaluate_acc_with_accelerator", "kind": "Function", "parent": "Evaluator", "start_line": 156, "end_line": 237, "docstring": null, "calls": ["create_dataloader", "enumerate", "open", "to", "decode", "decode", "tensor", "gather", "sum", "append", "print", "close", "sum", "exists", "makedirs", "len", "format", "autocast", "inference", "len", "append", "print", "print", "print", "len", "range", "tolist", "gather_object", "print", "enumerate"], "decorators": [], "num_calls": 50}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::_evaluate_acc_with_deepspeed", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "_evaluate_acc_with_deepspeed", "kind": "Function", "parent": "Evaluator", "start_line": 239, "end_line": 317, "docstring": null, "calls": ["create_dataloader", "enumerate", "open", "to", "inference", "decode", "decode", "tensor", "all_reduce", "tolist", "append", "gather_object", "print", "close", "sum", "is_initialized", "get_rank", "exists", "makedirs", "len", "format", "len", "append", "print", "print", "print", "len", "range", "print", "enumerate"], "decorators": [], "num_calls": 54}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::_evaluate_ppl", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "_evaluate_ppl", "kind": "Function", "parent": "Evaluator", "start_line": 319, "end_line": 361, "docstring": null, "calls": ["to_dict", "size", "range", "exp", "NotImplementedError", "get_tokenizer", "join", "print", "min", "to", "clone", "append", "mean", "min", "no_grad", "print", "get_max_length", "min", "get_backend_model", "stack", "get_max_length", "get_backend_model", "int", "exp", "int", "mean", "stack"], "decorators": [], "num_calls": 27}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::_evaluate_nll", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "_evaluate_nll", "kind": "Function", "parent": "Evaluator", "start_line": 364, "end_line": 493, "docstring": "Evaluates negative log likelihood of the model over a dataset.\n\nNLL = -1/N sum_{i=1}^N sum_{j=1}^|w_i| ln(p(w_{i,j}|context_window)),\n\nwhere N is the number of data samples, w_{i,j} is the j-th token in\ni-th sample. Here \"context_window\" = p(w_{i,start}, w_{i,start+1}, ...,\np_{i,j-1} with start = max(0, j - window_length + 1). \"window_length\"\nis normally the maximum length accepted by the model.\n\nReturns:\n    A float which represents the negative log likelihood.", "calls": ["to_dict", "from_dict", "tokenize", "get_backend_dataset", "len", "enumerate", "get_type", "min", "size", "range", "sum", "tensor", "tensor", "zip", "get_max_length", "min", "min", "to", "clone", "clone", "get_nll", "get_nll", "format", "get_max_length", "to", "count_nonzero", "append", "sum", "sum", "stack"], "decorators": [], "num_calls": 43}
{"symbol_id": "v2.src.lmflow.pipeline.evaluator::get_nll", "path": "v2/src/lmflow/pipeline/evaluator.py", "module": "v2.src.lmflow.pipeline.evaluator", "name": "get_nll", "kind": "Function", "parent": null, "start_line": 434, "end_line": 458, "docstring": null, "calls": ["to", "count_nonzero", "append", "all", "to", "no_grad", "get_backend_model", "zeros"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor::PreferenceDataCollatorWithPadding", "path": "v2/src/lmflow/pipeline/utils/dpov2_dataprocessor.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor", "name": "PreferenceDataCollatorWithPadding", "kind": "Class", "parent": null, "start_line": 17, "end_line": 195, "docstring": null, "calls": ["tokenizer", "tokenizer", "tokenizer", "append", "append", "append", "append", "max", "items", "keys", "collate", "len", "len", "len", "len", "items", "tokenize_batch_element", "append", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "len", "len", "len", "len", "endswith", "endswith", "endswith"], "decorators": ["dataclass"], "num_calls": 53}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor::tokenize_batch_element", "path": "v2/src/lmflow/pipeline/utils/dpov2_dataprocessor.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor", "name": "tokenize_batch_element", "kind": "Function", "parent": "PreferenceDataCollatorWithPadding", "start_line": 31, "end_line": 137, "docstring": "Tokenize a single batch element.\n\nAt this stage, we don't convert to PyTorch tensors yet; we just handle the truncation\n    in case the prompt + chosen or prompt + rejected responses is/are too long. First\n    we truncate the prompt; if we're still too long, we truncate the chosen/rejected.\n\nWe also create the labels for the chosen/rejected responses, which are of length equal to\n    the sum of the length of the prompt and the chosen/rejected response, with\n    label_pad_token_id  for the prompt tokens.", "calls": ["tokenizer", "tokenizer", "tokenizer", "append", "append", "append", "append", "max", "items", "len", "len", "len", "len", "items", "enumerate", "enumerate", "enumerate", "enumerate", "enumerate", "len", "len", "len", "len", "enumerate", "enumerate", "ValueError", "items", "items", "items", "items"], "decorators": [], "num_calls": 30}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor::collate", "path": "v2/src/lmflow/pipeline/utils/dpov2_dataprocessor.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor", "name": "collate", "kind": "Function", "parent": "PreferenceDataCollatorWithPadding", "start_line": 140, "end_line": 179, "docstring": null, "calls": ["keys", "endswith", "endswith", "endswith", "pad_sequence", "endswith", "pad_sequence", "LongTensor", "startswith", "endswith", "endswith", "endswith", "flip", "LongTensor", "LongTensor", "endswith", "startswith", "startswith", "ValueError", "ValueError"], "decorators": [], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor::__call__", "path": "v2/src/lmflow/pipeline/utils/dpov2_dataprocessor.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_dataprocessor", "name": "__call__", "kind": "Function", "parent": "PreferenceDataCollatorWithPadding", "start_line": 182, "end_line": 195, "docstring": null, "calls": ["collate", "tokenize_batch_element", "append"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_dataprocessor::RewardDataCollatorWithPadding", "path": "v2/src/lmflow/pipeline/utils/rm_dataprocessor.py", "module": "v2.src.lmflow.pipeline.utils.rm_dataprocessor", "name": "RewardDataCollatorWithPadding", "kind": "Class", "parent": null, "start_line": 14, "end_line": 50, "docstring": null, "calls": ["pad", "append", "append", "debug", "debug", "decode", "decode"], "decorators": ["dataclass"], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_dataprocessor::__call__", "path": "v2/src/lmflow/pipeline/utils/rm_dataprocessor.py", "module": "v2.src.lmflow.pipeline.utils.rm_dataprocessor", "name": "__call__", "kind": "Function", "parent": "RewardDataCollatorWithPadding", "start_line": 21, "end_line": 50, "docstring": null, "calls": ["pad", "append", "append", "debug", "debug", "decode", "decode"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_trainer::compute_metrics", "path": "v2/src/lmflow/pipeline/utils/rm_trainer.py", "module": "v2.src.lmflow.pipeline.utils.rm_trainer", "name": "compute_metrics", "kind": "Function", "parent": null, "start_line": 9, "end_line": 16, "docstring": null, "calls": ["sum", "len"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_trainer::rm_loss", "path": "v2/src/lmflow/pipeline/utils/rm_trainer.py", "module": "v2.src.lmflow.pipeline.utils.rm_trainer", "name": "rm_loss", "kind": "Function", "parent": null, "start_line": 19, "end_line": 32, "docstring": null, "calls": ["size", "arange", "model", "mean", "logsigmoid"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_trainer::RewardTrainer", "path": "v2/src/lmflow/pipeline/utils/rm_trainer.py", "module": "v2.src.lmflow.pipeline.utils.rm_trainer", "name": "RewardTrainer", "kind": "Class", "parent": null, "start_line": 35, "end_line": 37, "docstring": null, "calls": ["rm_loss"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_trainer::PeftRewardTrainer", "path": "v2/src/lmflow/pipeline/utils/rm_trainer.py", "module": "v2.src.lmflow.pipeline.utils.rm_trainer", "name": "PeftRewardTrainer", "kind": "Class", "parent": null, "start_line": 40, "end_line": 42, "docstring": null, "calls": ["rm_loss"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_trainer::compute_loss", "path": "v2/src/lmflow/pipeline/utils/rm_trainer.py", "module": "v2.src.lmflow.pipeline.utils.rm_trainer", "name": "compute_loss", "kind": "Function", "parent": "RewardTrainer", "start_line": 36, "end_line": 37, "docstring": null, "calls": ["rm_loss"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.rm_trainer::compute_loss", "path": "v2/src/lmflow/pipeline/utils/rm_trainer.py", "module": "v2.src.lmflow.pipeline.utils.rm_trainer", "name": "compute_loss", "kind": "Function", "parent": "PeftRewardTrainer", "start_line": 41, "end_line": 42, "docstring": null, "calls": ["rm_loss"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_trainer::DPOv2Trainer", "path": "v2/src/lmflow/pipeline/utils/dpov2_trainer.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_trainer", "name": "DPOv2Trainer", "kind": "Class", "parent": null, "start_line": 30, "end_line": 252, "docstring": null, "calls": ["__init__", "get_batch_metrics", "concatenated_forward", "to", "dpo_loss", "float", "mean", "mean", "mean", "mean", "mean", "mean", "mean", "mean", "PreferenceDataCollatorWithPadding", "detach", "detach", "no_grad", "mean", "super", "logsigmoid", "relu", "concatenated_forward", "tensor", "cpu", "cpu", "cpu", "cpu", "cpu", "cpu"], "decorators": [], "num_calls": 62}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_trainer::__init__", "path": "v2/src/lmflow/pipeline/utils/dpov2_trainer.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_trainer", "name": "__init__", "kind": "Function", "parent": "DPOv2Trainer", "start_line": 31, "end_line": 105, "docstring": null, "calls": ["__init__", "PreferenceDataCollatorWithPadding", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_trainer::dpo_loss", "path": "v2/src/lmflow/pipeline/utils/dpov2_trainer.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_trainer", "name": "dpo_loss", "kind": "Function", "parent": "DPOv2Trainer", "start_line": 107, "end_line": 182, "docstring": "Compute the DPO loss for a batch of policy and reference model log probabilities.\n\nArgs:\n    policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n    policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n    reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n    reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n    beta: Temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n    reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.\n\nReturns:\n    A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n    The losses tensor contains the DPO loss for each example in the batch.\n    The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.", "calls": ["detach", "detach", "logsigmoid", "relu", "logsigmoid", "sigmoid", "minimum", "exp", "sigmoid", "abs", "ones_like", "exp", "sigmoid", "minimum", "exp", "log", "log", "log", "log", "exp", "exp", "logsigmoid", "logsigmoid", "sigmoid", "ValueError", "ones_like", "exp"], "decorators": [], "num_calls": 27}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_trainer::get_batch_loss_metrics", "path": "v2/src/lmflow/pipeline/utils/dpov2_trainer.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_trainer", "name": "get_batch_loss_metrics", "kind": "Function", "parent": "DPOv2Trainer", "start_line": 184, "end_line": 190, "docstring": null, "calls": ["get_batch_metrics"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.dpov2_trainer::get_batch_metrics", "path": "v2/src/lmflow/pipeline/utils/dpov2_trainer.py", "module": "v2.src.lmflow.pipeline.utils.dpov2_trainer", "name": "get_batch_metrics", "kind": "Function", "parent": "DPOv2Trainer", "start_line": 192, "end_line": 252, "docstring": "Compute the DPO loss and other metrics for the given batch of inputs for train or test.", "calls": ["concatenated_forward", "to", "dpo_loss", "float", "mean", "mean", "mean", "mean", "mean", "mean", "mean", "mean", "no_grad", "mean", "concatenated_forward", "tensor", "cpu", "cpu", "cpu", "cpu", "cpu", "cpu", "cpu", "cpu", "disable_adapter", "concatenated_forward", "detach", "detach", "detach", "detach"], "decorators": [], "num_calls": 31}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::PeftTrainer", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "PeftTrainer", "kind": "Class", "parent": null, "start_line": 18, "end_line": 43, "docstring": null, "calls": ["_get_output_dir", "join", "makedirs", "_rotate_checkpoints", "startswith", "operator"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::PeftSavingCallback", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "PeftSavingCallback", "kind": "Class", "parent": null, "start_line": 45, "end_line": 78, "docstring": "Correctly save PEFT model and not full model ", "calls": ["join", "save_pretrained", "_save", "join", "_save", "join", "_save", "join", "save_pretrained"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::_save_checkpoint", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "_save_checkpoint", "kind": "Function", "parent": "PeftTrainer", "start_line": 19, "end_line": 43, "docstring": "Don't save base model, optimizer etc.\nbut create checkpoint folder (needed for saving adapter) ", "calls": ["_get_output_dir", "join", "makedirs", "_rotate_checkpoints", "startswith", "operator"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::_save", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "_save", "kind": "Function", "parent": "PeftSavingCallback", "start_line": 47, "end_line": 51, "docstring": null, "calls": ["join", "save_pretrained"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::on_train_end", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "on_train_end", "kind": "Function", "parent": "PeftSavingCallback", "start_line": 53, "end_line": 56, "docstring": "Save final best model adapter ", "calls": ["_save"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::on_epoch_end", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "on_epoch_end", "kind": "Function", "parent": "PeftSavingCallback", "start_line": 58, "end_line": 62, "docstring": "Save intermediate model adapters in case of interrupted training ", "calls": ["join", "_save"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.peft_trainer::on_save", "path": "v2/src/lmflow/pipeline/utils/peft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.peft_trainer", "name": "on_save", "kind": "Function", "parent": "PeftSavingCallback", "start_line": 64, "end_line": 78, "docstring": null, "calls": ["join", "_save", "join", "save_pretrained"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::RaftTrainer", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "RaftTrainer", "kind": "Class", "parent": null, "start_line": 211, "end_line": 3806, "docstring": "Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\nArgs:\n    model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n        The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n        <Tip>\n        [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n        your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\n        models.\n        </Tip>\n    args ([`TrainingArguments`], *optional*):\n        The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n        `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n    data_collator (`DataCollator`, *optional*):\n        The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n        default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n        [`DataCollatorWithPadding`] otherwise.\n    train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\n        The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n        `model.forward()` method are automatically removed.\n        Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n        distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n        `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n        manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n        sets the seed of the RNGs used.\n    eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\n         The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n         `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n         dataset prepending the dictionary key to the metric name.\n    tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n        The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n        maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n        interrupted training or reuse the fine-tuned model.\n    model_init (`Callable[[], PreTrainedModel]`, *optional*):\n        A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n        from a new instance of the model as given by this function.\n        The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n        be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n        inner layers, dropout probabilities etc).\n    compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n        The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n        a dictionary string to metric values.\n    callbacks (List of [`TrainerCallback`], *optional*):\n        A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n        detailed in [here](callback).\n        If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n    optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*): A tuple\n        containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your model\n        and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n    preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n        A function that preprocess the logits right before caching them at each evaluation step. Must take two\n        tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n        by this function will be reflected in the predictions received by `compute_metrics`.\n        Note that the labels (second parameter) will be `None` if the dataset does not have them.\nImportant attributes:\n    - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n      subclass.\n    - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n      original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n      the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n      model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n    - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n      data parallelism, this means some of the model layers are split on different GPUs).\n    - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n      to `False` if model parallel or deepspeed is used, or if the default\n      `TrainingArguments.place_model_on_device` is overridden to return `False` .\n    - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n      in `train`)", "calls": ["TrainerMemoryTracker", "start", "get_process_log_level", "set_verbosity", "getattr", "CallbackHandler", "add_callback", "is_sagemaker_mp_enabled", "TrainerState", "TrainerControl", "find_labels", "can_return_loss", "on_init_end", "stop_and_update_metrics", "add_callback", "pop_callback", "remove_callback", "to", "_set_signature_columns_if_needed", "list", "_set_signature_columns_if_needed", "RemoveColumnsCollator", "isinstance", "_get_train_sampler", "DataLoader", "isinstance", "_get_eval_sampler", "DataLoader", "isinstance", "_get_eval_sampler"], "decorators": [], "num_calls": 1151}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::__init__", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "__init__", "kind": "Function", "parent": "RaftTrainer", "start_line": 284, "end_line": 655, "docstring": null, "calls": ["TrainerMemoryTracker", "start", "get_process_log_level", "set_verbosity", "getattr", "CallbackHandler", "add_callback", "is_sagemaker_mp_enabled", "TrainerState", "TrainerControl", "find_labels", "can_return_loss", "on_init_end", "stop_and_update_metrics", "info", "TrainingArguments", "enable_full_determinism", "set_seed", "ValueError", "hasattr", "getattr", "len", "len", "get", "get", "DataCollatorWithPadding", "_move_model_to_device", "RuntimeError", "is_torch_tpu_available", "parameters"], "decorators": [], "num_calls": 91}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::add_callback", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "add_callback", "kind": "Function", "parent": "RaftTrainer", "start_line": 657, "end_line": 665, "docstring": "Add a callback to the current list of [`~transformer.TrainerCallback`].\nArgs:\n   callback (`type` or [`~transformer.TrainerCallback`]):\n       A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n       first case, will instantiate a member of that class.", "calls": ["add_callback"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::pop_callback", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "pop_callback", "kind": "Function", "parent": "RaftTrainer", "start_line": 667, "end_line": 678, "docstring": "Remove a callback from the current list of [`~transformer.TrainerCallback`] and returns it.\nIf the callback is not found, returns `None` (and no error is raised).\nArgs:\n   callback (`type` or [`~transformer.TrainerCallback`]):\n       A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n       first case, will pop the first member of that class found in the list of callbacks.\nReturns:\n    [`~transformer.TrainerCallback`]: The callback removed, if found.", "calls": ["pop_callback"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::remove_callback", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "remove_callback", "kind": "Function", "parent": "RaftTrainer", "start_line": 680, "end_line": 688, "docstring": "Remove a callback from the current list of [`~transformer.TrainerCallback`].\nArgs:\n   callback (`type` or [`~transformer.TrainerCallback`]):\n       A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n       first case, will remove the first member of that class found in the list of callbacks.", "calls": ["remove_callback"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_move_model_to_device", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_move_model_to_device", "kind": "Function", "parent": "RaftTrainer", "start_line": 690, "end_line": 694, "docstring": null, "calls": ["to", "hasattr", "tie_weights"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_set_signature_columns_if_needed", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_set_signature_columns_if_needed", "kind": "Function", "parent": "RaftTrainer", "start_line": 696, "end_line": 702, "docstring": null, "calls": ["signature", "list", "list", "keys", "set"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_remove_unused_columns", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_remove_unused_columns", "kind": "Function", "parent": "RaftTrainer", "start_line": 704, "end_line": 728, "docstring": null, "calls": ["_set_signature_columns_if_needed", "list", "len", "info", "parse", "parse", "set_format", "remove_columns", "set", "set", "join", "join"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_get_collator_with_removed_columns", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_get_collator_with_removed_columns", "kind": "Function", "parent": "RaftTrainer", "start_line": 730, "end_line": 746, "docstring": "Wrap the data collator in a callable removing unused columns.", "calls": ["_set_signature_columns_if_needed", "RemoveColumnsCollator"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_get_train_sampler", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_get_train_sampler", "kind": "Function", "parent": "RaftTrainer", "start_line": 748, "end_line": 817, "docstring": null, "calls": ["Generator", "manual_seed", "has_length", "int", "is_datasets_available", "isinstance", "LengthGroupedSampler", "DistributedLengthGroupedSampler", "RandomSampler", "item", "DistributedSamplerWithLoop", "DistributedSampler", "random_", "empty"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::get_train_dataloader", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "get_train_dataloader", "kind": "Function", "parent": "RaftTrainer", "start_line": 819, "end_line": 865, "docstring": "Returns the training [`~torch.utils.data.DataLoader`].\nWill use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\ntraining if necessary) otherwise.\nSubclass and override this method if you want to inject some custom behavior.", "calls": ["isinstance", "_get_train_sampler", "DataLoader", "ValueError", "is_datasets_available", "isinstance", "_remove_unused_columns", "_get_collator_with_removed_columns", "DataLoader", "IterableDatasetShard"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_get_eval_sampler", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_get_eval_sampler", "kind": "Function", "parent": "RaftTrainer", "start_line": 867, "end_line": 894, "docstring": null, "calls": ["is_torch_tpu_available", "SequentialSampler", "ShardSampler", "SequentialDistributedSampler", "is_sagemaker_mp_enabled", "SequentialDistributedSampler", "xrt_world_size", "get_ordinal", "SequentialDistributedSampler", "SequentialSampler", "dp_size", "dp_rank"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::get_eval_dataloader", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "get_eval_dataloader", "kind": "Function", "parent": "RaftTrainer", "start_line": 896, "end_line": 942, "docstring": "Returns the evaluation [`~torch.utils.data.DataLoader`].\nSubclass and override this method if you want to inject some custom behavior.\nArgs:\n    eval_dataset (`torch.utils.data.Dataset`, *optional*):\n        If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n        by the `model.forward()` method are automatically removed. It must implement `__len__`.", "calls": ["isinstance", "_get_eval_sampler", "DataLoader", "ValueError", "is_datasets_available", "isinstance", "_remove_unused_columns", "_get_collator_with_removed_columns", "DataLoader", "IterableDatasetShard"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::get_test_dataloader", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "get_test_dataloader", "kind": "Function", "parent": "RaftTrainer", "start_line": 944, "end_line": 988, "docstring": "Returns the test [`~torch.utils.data.DataLoader`].\nSubclass and override this method if you want to inject some custom behavior.\nArgs:\n    test_dataset (`torch.utils.data.Dataset`, *optional*):\n        The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n        `model.forward()` method are automatically removed. It must implement `__len__`.", "calls": ["isinstance", "_get_eval_sampler", "DataLoader", "is_datasets_available", "isinstance", "_remove_unused_columns", "_get_collator_with_removed_columns", "DataLoader", "IterableDatasetShard"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::create_optimizer_and_scheduler", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "create_optimizer_and_scheduler", "kind": "Function", "parent": "RaftTrainer", "start_line": 990, "end_line": 1003, "docstring": "Setup the optimizer and the learning rate scheduler.\nWe provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n`create_scheduler`) in a subclass.", "calls": ["create_optimizer", "create_scheduler"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::create_optimizer", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "create_optimizer", "kind": "Function", "parent": "RaftTrainer", "start_line": 1005, "end_line": 1058, "docstring": "Setup the optimizer.\nWe provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer's init through `optimizers`, or subclass and override this method in a subclass.", "calls": ["is_sagemaker_mp_enabled", "is_sagemaker_mp_enabled", "get_parameter_names", "get_optimizer_cls_and_kwargs", "DistributedOptimizer", "OSS", "optimizer_cls", "get_instance", "modules", "print", "isinstance", "named_parameters", "named_parameters", "sum", "print", "register_module_override", "debug", "values", "data_ptr", "numel", "parameters"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::get_optimizer_cls_and_kwargs", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "get_optimizer_cls_and_kwargs", "kind": "Function", "parent": "RaftTrainer", "start_line": 1061, "end_line": 1147, "docstring": "Returns the optimizer class and optimizer parameters based on the training arguments.\nArgs:\n    args (`transformers.training_args.TrainingArguments`):\n        The training arguments for the training session.", "calls": ["split", "update", "split", "update", "replace", "update", "update", "update", "ValueError", "update", "ValueError", "update", "ValueError", "update", "update", "ValueError", "ValueError", "strtobool", "getattr", "getattr", "getattr", "get", "get", "get", "get"], "decorators": ["staticmethod"], "num_calls": 25}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::create_scheduler", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "create_scheduler", "kind": "Function", "parent": "RaftTrainer", "start_line": 1149, "end_line": 1166, "docstring": "Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\npassed as an argument.\nArgs:\n    num_training_steps (int): The number of training steps to do.", "calls": ["get_scheduler", "get_warmup_steps"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::num_examples", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "num_examples", "kind": "Function", "parent": "RaftTrainer", "start_line": 1168, "end_line": 1180, "docstring": "Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\ndataloader.dataset does not exist or has no length, estimates as best it can", "calls": ["isinstance", "len", "len", "len"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_hp_search_setup", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_hp_search_setup", "kind": "Function", "parent": "RaftTrainer", "start_line": 1182, "end_line": 1221, "docstring": "HP search setup code", "calls": ["items", "hp_space", "getattr", "setattr", "info", "info", "info", "HfTrainerDeepSpeedConfig", "trainer_config_process", "pop", "hasattr", "warning", "type", "isinstance", "int", "items"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_report_to_hp_search", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_report_to_hp_search", "kind": "Function", "parent": "RaftTrainer", "start_line": 1223, "end_line": 1239, "docstring": null, "calls": ["compute_objective", "copy", "report", "should_prune", "on_train_end", "TrialPruned", "report", "_tune_save_checkpoint"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_tune_save_checkpoint", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_tune_save_checkpoint", "kind": "Function", "parent": "RaftTrainer", "start_line": 1241, "end_line": 1252, "docstring": null, "calls": ["checkpoint_dir", "join", "save_model", "save_to_json", "save", "save", "join", "state_dict", "join", "state_dict", "join"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::call_model_init", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "call_model_init", "kind": "Function", "parent": "RaftTrainer", "start_line": 1254, "end_line": 1266, "docstring": null, "calls": ["number_of_arguments", "model_init", "RuntimeError", "model_init", "RuntimeError"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::torch_jit_model_eval", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "torch_jit_model_eval", "kind": "Function", "parent": "RaftTrainer", "start_line": 1268, "end_line": 1304, "docstring": null, "calls": ["next", "_prepare_inputs", "warning", "iter", "eval", "freeze", "ContextManagers", "no_grad", "jit_model", "jit_model", "warning", "parse", "parse", "isinstance", "tuple", "trace", "autocast_smart_context_manager", "no_grad", "trace", "trace", "ones_like", "append", "parse"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::ipex_optimize_model", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "ipex_optimize_model", "kind": "Function", "parent": "RaftTrainer", "start_line": 1306, "end_line": 1327, "docstring": null, "calls": ["is_ipex_available", "ImportError", "eval", "optimize", "optimize", "train"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_wrap_model", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_wrap_model", "kind": "Function", "parent": "RaftTrainer", "start_line": 1329, "end_line": 1519, "docstring": null, "calls": ["is_sagemaker_mp_enabled", "compile", "ipex_optimize_model", "isinstance", "DistributedModel", "unwrap_model", "initialize", "DataParallel", "time", "torch_jit_model_eval", "round", "ShardedDDP", "to", "is_sagemaker_dp_enabled", "time", "auto_wrap", "FSDP", "DistributedDataParallel", "FullyShardedDDP", "CPUOffload", "CPUOffload", "MixedPrecision", "type", "FSDP", "partial", "step", "is_torch_neuroncore_available", "DistributedDataParallel", "partial", "ImportError"], "decorators": [], "num_calls": 48}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::train", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "train", "kind": "Function", "parent": "RaftTrainer", "start_line": 1521, "end_line": 1615, "docstring": "Main training entry point.\nArgs:\n    resume_from_checkpoint (`str` or `bool`, *optional*):\n        If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n        `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n        of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n    trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n        The trial run or the hyperparameter dictionary for hyperparameter search.\n    ignore_keys_for_eval (`List[str]`, *optional*)\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions for evaluation during the training.\n    kwargs:\n        Additional keyword arguments used to hide deprecated arguments", "calls": ["start", "_hp_search_setup", "_move_model_to_device", "pop", "warn", "len", "TypeError", "call_model_init", "isinstance", "get_last_checkpoint", "_load_from_checkpoint", "find_executable_batch_size", "inner_training_loop1", "find_executable_batch_size", "inner_training_loop2", "enable_full_determinism", "set_seed", "ValueError", "is_sagemaker_mp_enabled", "_move_model_to_device", "join", "list", "keys"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_one_train", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_one_train", "kind": "Function", "parent": "RaftTrainer", "start_line": 1618, "end_line": 1957, "docstring": null, "calls": ["TrainerState", "get_train_dataloader", "has_length", "info", "info", "info", "info", "info", "info", "info", "info", "time", "is_local_process_zero", "is_world_process_zero", "to", "zero_grad", "on_train_begin", "range", "info", "item", "speed_metrics", "store_flos", "stop_and_update_metrics", "log", "_get_output_dir", "_sorted_checkpoints", "on_train_end", "TrainOutput", "len", "max"], "decorators": [], "num_calls": 119}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_inner_training_loop", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_inner_training_loop", "kind": "Function", "parent": "RaftTrainer", "start_line": 1959, "end_line": 2055, "docstring": "0 This function serves to train one time\n1 Update the self.train_dataset before calling this function", "calls": ["get_train_dataloader", "has_length", "TrainerState", "_wrap_model", "len", "max", "num_examples", "is_sagemaker_mp_enabled", "deepspeed_init", "gradient_checkpointing_enable", "print", "create_optimizer_and_scheduler", "ceil", "ceil", "ValueError", "ValueError", "DebugUnderflowOverflow", "create_optimizer_and_scheduler", "int", "num_examples"], "decorators": [], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_get_output_dir", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_get_output_dir", "kind": "Function", "parent": "RaftTrainer", "start_line": 2065, "end_line": 2083, "docstring": null, "calls": ["join", "hp_name", "get_trial_id"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_load_from_checkpoint", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_load_from_checkpoint", "kind": "Function", "parent": "RaftTrainer", "start_line": 2085, "end_line": 2141, "docstring": null, "calls": ["info", "isfile", "isfile", "ValueError", "join", "from_json_file", "join", "is_sagemaker_mp_enabled", "load_sharded_checkpoint", "isfile", "isfile", "join", "warning", "isfile", "load", "load_state_dict", "_issue_warnings_after_load", "is_sagemaker_mp_enabled", "_issue_warnings_after_load", "join", "join", "join", "resume_from_checkpoint", "load", "load_state_dict", "join", "is_sagemaker_mp_enabled", "hasattr", "warning", "join"], "decorators": [], "num_calls": 30}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_load_best_model", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_load_best_model", "kind": "Function", "parent": "RaftTrainer", "start_line": 2143, "end_line": 2201, "docstring": null, "calls": ["info", "join", "exists", "is_sagemaker_mp_enabled", "exists", "deepspeed_init", "is_sagemaker_mp_enabled", "join", "load_sharded_checkpoint", "warning", "destroy", "isfile", "load", "load_state_dict", "is_sagemaker_mp_enabled", "_issue_warnings_after_load", "is_sagemaker_mp_enabled", "_issue_warnings_after_load", "join", "resume_from_checkpoint", "load", "load_state_dict", "is_sagemaker_mp_enabled"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_issue_warnings_after_load", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_issue_warnings_after_load", "kind": "Function", "parent": "RaftTrainer", "start_line": 2203, "end_line": 2214, "docstring": null, "calls": ["len", "len", "warning", "tie_weights", "warning", "set", "set"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_maybe_log_save_evaluate", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_maybe_log_save_evaluate", "kind": "Function", "parent": "RaftTrainer", "start_line": 2216, "end_line": 2253, "docstring": null, "calls": ["is_torch_tpu_available", "item", "round", "_get_learning_rate", "store_flos", "log", "isinstance", "_report_to_hp_search", "_save_checkpoint", "on_save", "mark_step", "items", "evaluate", "mean", "evaluate", "_nested_gather"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_load_rng_state", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_load_rng_state", "kind": "Function", "parent": "RaftTrainer", "start_line": 2255, "end_line": 2294, "docstring": null, "calls": ["load", "setstate", "set_state", "set_rng_state", "is_available", "is_torch_tpu_available", "join", "join", "set_rng_state", "isfile", "info", "isfile", "info", "set_rng_state", "set_rng_state_all", "info"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_save_checkpoint", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_save_checkpoint", "kind": "Function", "parent": "RaftTrainer", "start_line": 2296, "end_line": 2403, "docstring": null, "calls": ["_get_output_dir", "join", "save_model", "is_torch_tpu_available", "is_available", "is_torch_tpu_available", "makedirs", "store_flos", "save_checkpoint", "consolidate_state_dict", "rendezvous", "save", "is_sagemaker_mp_enabled", "save_to_json", "getstate", "get_state", "get_rng_state", "get_rng_state", "save", "save", "_push_from_checkpoint", "_rotate_checkpoints", "state_dict", "join", "catch_warnings", "save", "reissue_pt_warnings", "local_state_dict", "barrier", "startswith"], "decorators": [], "num_calls": 60}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_load_optimizer_and_scheduler", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_load_optimizer_and_scheduler", "kind": "Function", "parent": "RaftTrainer", "start_line": 2405, "end_line": 2460, "docstring": "If optimizer and scheduler states exist, load them.", "calls": ["is_sagemaker_mp_enabled", "glob", "isfile", "isfile", "is_torch_tpu_available", "join", "join", "load", "reissue_pt_warnings", "send_cpu_data_to_device", "send_cpu_data_to_device", "load_state_dict", "load_state_dict", "is_sagemaker_mp_enabled", "reissue_pt_warnings", "join", "join", "catch_warnings", "load", "is_sagemaker_mp_enabled", "isfile", "register_post_step_hook", "load_state_dict", "catch_warnings", "load_state_dict", "isfile", "load_state_dict", "join", "join", "load"], "decorators": [], "num_calls": 45}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::hyperparameter_search", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "hyperparameter_search", "kind": "Function", "parent": "RaftTrainer", "start_line": 2462, "end_line": 2560, "docstring": "Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\nby `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\nthe sum of all metrics otherwise.\n<Tip warning={true}>\nTo use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\nreinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\nsubclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\noptimizer/scheduler.\n</Tip>\nArgs:\n    hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n        A function that defines the hyperparameter search space. Will default to\n        [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n        [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n    compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n        A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n        method. Will default to [`~trainer_utils.default_compute_objective`].\n    n_trials (`int`, *optional*, defaults to 100):\n        The number of trial runs to test.\n    direction (`str`, *optional*, defaults to `\"minimize\"`):\n        Whether to optimize greater or lower objects. Can be `\"minimize\"` or `\"maximize\"`, you should pick\n        `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or several metrics.\n    backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n        The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n        on which one is installed. If all are installed, will default to optuna.\n    hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n        A function that defines the trial/run name. Will default to None.\n    kwargs (`Dict[str, Any]`, *optional*):\n        Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\n        information see:\n        - the documentation of\n          [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n        - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\n        - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\nReturns:\n    [`trainer_utils.BestRun`]: All the information about the best run. Experiment summary can be found in\n    `run_summary` attribute for Ray backend.", "calls": ["HPSearchBackend", "backend_run", "default_hp_search_backend", "RuntimeError", "RuntimeError", "RuntimeError", "RuntimeError", "RuntimeError", "RuntimeError"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::log", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "log", "kind": "Function", "parent": "RaftTrainer", "start_line": 2562, "end_line": 2575, "docstring": "Log `logs` on the various objects watching training.\nSubclass and override this method to inject custom behavior.\nArgs:\n    logs (`Dict[str, float]`):\n        The values to log.", "calls": ["append", "on_log", "round"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_prepare_input", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_prepare_input", "kind": "Function", "parent": "RaftTrainer", "start_line": 2577, "end_line": 2593, "docstring": "Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.", "calls": ["isinstance", "isinstance", "type", "isinstance", "_prepare_input", "type", "to", "items", "_prepare_input", "update", "is_floating_point", "is_complex", "dtype"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_prepare_inputs", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_prepare_inputs", "kind": "Function", "parent": "RaftTrainer", "start_line": 2595, "end_line": 2609, "docstring": "Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\nhandling potential state.", "calls": ["_prepare_input", "len", "ValueError", "join"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::compute_loss_context_manager", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "compute_loss_context_manager", "kind": "Function", "parent": "RaftTrainer", "start_line": 2611, "end_line": 2615, "docstring": "A helper wrapper to group together context managers.", "calls": ["autocast_smart_context_manager"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::autocast_smart_context_manager", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "autocast_smart_context_manager", "kind": "Function", "parent": "RaftTrainer", "start_line": 2617, "end_line": 2634, "docstring": "A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\narguments, depending on the situation.", "calls": ["autocast", "nullcontext", "suppress", "autocast", "autocast"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::training_step", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "training_step", "kind": "Function", "parent": "RaftTrainer", "start_line": 2636, "end_line": 2678, "docstring": "Perform a training step on a batch of inputs.\nSubclass and override to inject custom behavior.\nArgs:\n    model (`nn.Module`):\n        The model to train.\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\n        The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n        argument `labels`. Check your model's documentation for all accepted arguments.\nReturn:\n    `torch.Tensor`: The tensor with training loss on this batch.", "calls": ["train", "_prepare_inputs", "is_sagemaker_mp_enabled", "detach", "smp_forward_backward", "to", "compute_loss_context_manager", "compute_loss", "mean", "backward", "detach", "scale", "scale_loss", "backward", "backward", "backward", "reduce_mean"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::compute_loss", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "compute_loss", "kind": "Function", "parent": "RaftTrainer", "start_line": 2680, "end_line": 2709, "docstring": "How the loss is computed by Trainer. By default, all models return the loss in the first element.\nSubclass and override for custom behavior.", "calls": ["model", "pop", "_get_name", "values", "label_smoother", "label_smoother", "isinstance", "ValueError", "isinstance", "unwrap_model", "join", "join", "keys", "keys"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::is_local_process_zero", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "is_local_process_zero", "kind": "Function", "parent": "RaftTrainer", "start_line": 2711, "end_line": 2716, "docstring": "Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\nmachines) main process.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::is_world_process_zero", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "is_world_process_zero", "kind": "Function", "parent": "RaftTrainer", "start_line": 2718, "end_line": 2728, "docstring": "Whether or not this process is the global main process (when training in a distributed fashion on several\nmachines, this is only going to be `True` for one process).", "calls": ["is_sagemaker_mp_enabled", "rank"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::save_model", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "save_model", "kind": "Function", "parent": "RaftTrainer", "start_line": 2730, "end_line": 2791, "docstring": "Will save the model, so you can reload it using `from_pretrained()`.\nWill only save from the main process.", "calls": ["is_torch_tpu_available", "_save_tpu", "is_sagemaker_mp_enabled", "push_to_hub", "makedirs", "state_dict", "_save", "touch", "state_dict", "_save", "is_deepspeed_zero3_enabled", "Path", "_save", "_save", "join", "join", "isfile", "save_16bit_model", "warning", "save_checkpoint", "remove"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_save_tpu", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_save_tpu", "kind": "Function", "parent": "RaftTrainer", "start_line": 2793, "end_line": 2819, "docstring": null, "calls": ["info", "is_master_ordinal", "rendezvous", "makedirs", "save", "isinstance", "isinstance", "save_pretrained", "save_pretrained", "join", "unwrap_model", "save_pretrained", "info", "state_dict", "save", "join", "unwrap_model", "state_dict"], "decorators": [], "num_calls": 18}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_save", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_save", "kind": "Function", "parent": "RaftTrainer", "start_line": 2821, "end_line": 2844, "docstring": null, "calls": ["makedirs", "info", "save", "isinstance", "isinstance", "save_pretrained", "save_pretrained", "join", "unwrap_model", "save_pretrained", "info", "save", "state_dict", "state_dict", "join", "unwrap_model"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::store_flos", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "store_flos", "kind": "Function", "parent": "RaftTrainer", "start_line": 2846, "end_line": 2855, "docstring": null, "calls": ["item", "sum", "distributed_broadcast_scalars"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_sorted_checkpoints", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_sorted_checkpoints", "kind": "Function", "parent": "RaftTrainer", "start_line": 2857, "end_line": 2879, "docstring": null, "calls": ["sorted", "str", "index", "range", "glob", "isdir", "append", "match", "str", "append", "Path", "len", "Path", "getmtime", "groups", "int", "groups"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_rotate_checkpoints", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_rotate_checkpoints", "kind": "Function", "parent": "RaftTrainer", "start_line": 2881, "end_line": 2904, "docstring": null, "calls": ["_sorted_checkpoints", "max", "len", "info", "rmtree", "len"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::evaluate", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "evaluate", "kind": "Function", "parent": "RaftTrainer", "start_line": 2906, "end_line": 2971, "docstring": "Run evaluation and returns metrics.\nThe calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init `compute_metrics` argument).\nYou can also subclass and override this method to inject custom behavior.\nArgs:\n    eval_dataset (`Dataset`, *optional*):\n        Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n        not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n        method.\n    ignore_keys (`Lst[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n    metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n        An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n        \"eval_bleu\" if the prefix is \"eval\" (default)\nReturns:\n    A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n    dictionary also contains the epoch number which comes from the training state.", "calls": ["start", "get_eval_dataloader", "time", "eval_loop", "update", "log", "on_evaluate", "stop_and_update_metrics", "speed_metrics", "master_print", "metrics_report", "ceil"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::predict", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "predict", "kind": "Function", "parent": "RaftTrainer", "start_line": 2973, "end_line": 3026, "docstring": "Run prediction and returns predictions and potential metrics.\nDepending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in `evaluate()`.\nArgs:\n    test_dataset (`Dataset`):\n        Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n        `model.forward()` method are automatically removed. Has to implement the method `__len__`\n    ignore_keys (`Lst[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n    metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n        An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n        \"test_bleu\" if the prefix is \"test\" (default)\n<Tip>\nIf your predictions or labels have different sequence length (for instance because you're doing dynamic padding\nin a token classification task) the predictions will be padded (on the right) to allow for concatenation into\none array. The padding index is -100.\n</Tip>\nReturns: *NamedTuple* A namedtuple with the following keys:\n    - predictions (`np.ndarray`): The predictions on `test_dataset`.\n    - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n    - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n      labels).", "calls": ["start", "get_test_dataloader", "time", "eval_loop", "update", "on_predict", "stop_and_update_metrics", "PredictionOutput", "speed_metrics", "ceil"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::evaluation_loop", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "evaluation_loop", "kind": "Function", "parent": "RaftTrainer", "start_line": 3028, "end_line": 3236, "docstring": "Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\nWorks both with or without labels.", "calls": ["_wrap_model", "info", "has_length", "info", "eval", "getattr", "is_torch_tpu_available", "enumerate", "has_length", "denumpify_detensorize", "hasattr", "list", "EvalLoopOutput", "deepspeed_init", "info", "info", "per_device_loader", "find_batch_size", "prediction_step", "is_torch_tpu_available", "on_prediction_step", "hasattr", "delattr", "nested_numpify", "nested_numpify", "nested_numpify", "nested_numpify", "len", "nested_truncate", "nested_truncate"], "decorators": [], "num_calls": 75}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_nested_gather", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_nested_gather", "kind": "Function", "parent": "RaftTrainer", "start_line": 3238, "end_line": 3253, "docstring": "Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\nconcatenating them to `gathered`", "calls": ["is_torch_tpu_available", "nested_xla_mesh_reduce", "is_sagemaker_mp_enabled", "smp_gather", "distributed_concat"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_pad_across_processes", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_pad_across_processes", "kind": "Function", "parent": "RaftTrainer", "start_line": 3256, "end_line": 3288, "docstring": "Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\nthey can safely be gathered.", "calls": ["isinstance", "cpu", "max", "list", "isinstance", "len", "tensor", "new_zeros", "type", "_nested_gather", "tuple", "_pad_across_processes", "type", "isinstance", "TypeError", "_pad_across_processes", "items", "type"], "decorators": [], "num_calls": 18}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::prediction_step", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "prediction_step", "kind": "Function", "parent": "RaftTrainer", "start_line": 3290, "end_line": 3389, "docstring": "Perform an evaluation step on `model` using `inputs`.\nSubclass and override to inject custom behavior.\nArgs:\n    model (`nn.Module`):\n        The model to evaluate.\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\n        The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n        argument `labels`. Check your model's documentation for all accepted arguments.\n    prediction_loss_only (`bool`):\n        Whether or not to return the loss only.\n    ignore_keys (`Lst[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\nReturn:\n    Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n    logits and labels (each being optional).", "calls": ["get", "_prepare_inputs", "nested_detach", "all", "hasattr", "nested_detach", "no_grad", "is_sagemaker_mp_enabled", "len", "len", "getattr", "tuple", "len", "smp_forward_only", "len", "isinstance", "cpu", "smp_nested_concat", "isinstance", "smp_nested_concat", "detach", "isinstance", "isinstance", "get", "get", "tuple", "tuple", "compute_loss_context_manager", "compute_loss", "tuple"], "decorators": [], "num_calls": 40}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::floating_point_ops", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "floating_point_ops", "kind": "Function", "parent": "RaftTrainer", "start_line": 3391, "end_line": 3405, "docstring": "For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\noperations for every backward + forward pass. If using another model, either implement such a method in the\nmodel or subclass and override this method.\nArgs:\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\nReturns:\n    `int`: The number of floating-point operations.", "calls": ["hasattr", "floating_point_ops"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::init_git_repo", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "init_git_repo", "kind": "Function", "parent": "RaftTrainer", "start_line": 3407, "end_line": 3451, "docstring": "Initializes a git repo in `self.args.hub_model_id`.\nArgs:\n    at_init (`bool`, *optional*, defaults to `False`):\n        Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is\n        `True` and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped\n        out.", "calls": ["create_repo", "git_pull", "get", "is_world_process_zero", "get_full_repo_name", "Repository", "_add_sm_patterns_to_gitignore", "absolute", "exists", "open", "writelines", "rmtree", "Repository", "join", "join", "Path"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::create_model_card", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "create_model_card", "kind": "Function", "parent": "RaftTrainer", "start_line": 3453, "end_line": 3506, "docstring": "Creates a draft of a model card using the information available to the `Trainer`.\nArgs:\n    language (`str`, *optional*):\n        The language of the model (if applicable)\n    license (`str`, *optional*):\n        The license of the model. Will default to the license of the pretrained model used, if the original\n        model given to the `Trainer` comes from a repo on the Hub.\n    tags (`str` or `List[str]`, *optional*):\n        Some tags to be included in the metadata of the model card.\n    model_name (`str`, *optional*):\n        The name of the model.\n    finetuned_from (`str`, *optional*):\n        The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n        of the original model given to the `Trainer` (if it comes from the Hub).\n    tasks (`str` or `List[str]`, *optional*):\n        One or several task identifiers, to be included in the metadata of the model card.\n    dataset_tags (`str` or `List[str]`, *optional*):\n        One or several dataset tags, to be included in the metadata of the model card.\n    dataset (`str` or `List[str]`, *optional*):\n        One or several dataset identifiers, to be included in the metadata of the model card.\n    dataset_args (`str` or `List[str]`, *optional*):\n       One or several dataset arguments, to be included in the metadata of the model card.", "calls": ["from_trainer", "to_model_card", "is_world_process_zero", "open", "write", "join"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_push_from_checkpoint", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_push_from_checkpoint", "kind": "Function", "parent": "RaftTrainer", "start_line": 3508, "end_line": 3548, "docstring": null, "calls": ["save", "isfile", "save_pretrained", "join", "push_to_hub", "is_world_process_zero", "join", "copy", "join", "isdir", "move", "move", "join", "join", "rmtree", "int"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::push_to_hub", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "push_to_hub", "kind": "Function", "parent": "RaftTrainer", "start_line": 3550, "end_line": 3602, "docstring": "Upload *self.model* and *self.tokenizer* to the  model hub on the repo *self.args.hub_model_id*.\nParameters:\n    commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n        Message to commit while pushing.\n    blocking (`bool`, *optional*, defaults to `True`):\n        Whether the function should return only when the `git push` has finished.\n    kwargs:\n        Additional keyword arguments passed along to [`~Trainer.create_model_card`].\nReturns:\n    The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of\n    the commit and an object to track the progress of the commit if `blocking=True`", "calls": ["pop", "save_model", "push_to_hub", "hasattr", "init_git_repo", "is_world_process_zero", "kill", "create_model_card", "push_to_hub", "Path", "split", "error"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::prediction_loop", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "prediction_loop", "kind": "Function", "parent": "RaftTrainer", "start_line": 3608, "end_line": 3751, "docstring": "Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\nWorks both with or without labels.", "calls": ["_wrap_model", "num_examples", "info", "info", "info", "max", "DistributedTensorGatherer", "eval", "is_torch_tpu_available", "enumerate", "add_arrays", "finalize", "denumpify_detensorize", "list", "EvalLoopOutput", "has_length", "ValueError", "deepspeed_init", "DistributedTensorGatherer", "DistributedTensorGatherer", "DistributedTensorGatherer", "per_device_loader", "prediction_step", "on_prediction_step", "hasattr", "delattr", "_gather_and_numpify", "add_arrays", "add_arrays", "add_arrays"], "decorators": [], "num_calls": 64}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_gather_and_numpify", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_gather_and_numpify", "kind": "Function", "parent": "RaftTrainer", "start_line": 3753, "end_line": 3767, "docstring": "Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\nconcatenating them to `gathered`", "calls": ["is_torch_tpu_available", "nested_numpify", "nested_xla_mesh_reduce", "is_sagemaker_mp_enabled", "smp_gather", "distributed_concat"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::_add_sm_patterns_to_gitignore", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "_add_sm_patterns_to_gitignore", "kind": "Function", "parent": "RaftTrainer", "start_line": 3769, "end_line": 3806, "docstring": "Add SageMaker Checkpointing patterns to .gitignore file.", "calls": ["exists", "git_add", "sleep", "is_world_process_zero", "join", "is_repo_clean", "git_commit", "git_push", "open", "read", "endswith", "open", "debug", "write", "join", "join"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::patched_optimizer_step", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "patched_optimizer_step", "kind": "Function", "parent": null, "start_line": 1486, "end_line": 1490, "docstring": null, "calls": ["step", "mark_step"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::auto_wrapper_callable", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "auto_wrapper_callable", "kind": "Function", "parent": null, "start_line": 1473, "end_line": 1474, "docstring": null, "calls": ["FSDP", "checkpoint_module"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::opt_load_hook", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "opt_load_hook", "kind": "Function", "parent": null, "start_line": 2438, "end_line": 2439, "docstring": null, "calls": ["load_state_dict", "load", "join"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.pipeline.utils.raft_trainer::opt_load_hook", "path": "v2/src/lmflow/pipeline/utils/raft_trainer.py", "module": "v2.src.lmflow.pipeline.utils.raft_trainer", "name": "opt_load_hook", "kind": "Function", "parent": null, "start_line": 2443, "end_line": 2449, "docstring": null, "calls": ["load_state_dict", "load_state_dict", "load", "load", "join", "join"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.pipeline.utils.memory_safe_dpov2_align::main", "path": "v2/src/lmflow/pipeline/utils/memory_safe_dpov2_align.py", "module": "v2.src.lmflow.pipeline.utils.memory_safe_dpov2_align", "name": "main", "kind": "Function", "parent": null, "start_line": 34, "end_line": 66, "docstring": null, "calls": ["HfArgumentParser", "parse_args_into_dataclasses", "remove_dataclass_attr_prefix", "ModelArguments", "HFDecoderModel", "HFDecoderModel", "Dataset", "deepcopy", "DPOv2Aligner", "align", "sample"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.pipeline.utils.memory_safe_vllm_inference::main", "path": "v2/src/lmflow/pipeline/utils/memory_safe_vllm_inference.py", "module": "v2.src.lmflow.pipeline.utils.memory_safe_vllm_inference", "name": "main", "kind": "Function", "parent": null, "start_line": 32, "end_line": 64, "docstring": null, "calls": ["get_pipeline_args_class", "HfArgumentParser", "Dataset", "get_model", "VLLMInferencer", "inference", "print", "endswith", "parse_json_file", "parse_args_into_dataclasses", "len", "abspath"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::CustomMultiModalDataset", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "CustomMultiModalDataset", "kind": "Class", "parent": null, "start_line": 20, "end_line": 96, "docstring": "Dataset for Multi Modal data", "calls": ["__init__", "load", "print", "len", "getattr", "isinstance", "isinstance", "open", "len", "convert", "preprocess_multimodal_llava", "deepcopy", "preprocess_llama_from_llava_plain", "preprocess_llama_from_llava_v1", "dict", "zeros", "super", "expand2square", "deepcopy", "open", "tuple", "join", "preprocess", "preprocess", "new", "paste", "new", "paste", "int"], "decorators": [], "num_calls": 29}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::preprocess_multimodal_llava", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "preprocess_multimodal_llava", "kind": "Function", "parent": null, "start_line": 100, "end_line": 121, "docstring": null, "calls": ["replace", "strip", "strip", "replace", "replace"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::tokenizer_image_token", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "tokenizer_image_token", "kind": "Function", "parent": null, "start_line": 124, "end_line": 146, "docstring": null, "calls": ["insert_separator", "append", "extend", "ValueError", "tokenizer", "split", "len", "len", "tensor", "zip", "len"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::preprocess_llama_from_llava_plain", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "preprocess_llama_from_llava_plain", "kind": "Function", "parent": null, "start_line": 149, "end_line": 177, "docstring": "This function just add the image in the front of text.\nAnd don't add any prompt.\nArgs:\n    sources: The input data with text and image.\n    tokenizer: The tokenizer to process text.\n    has_image: Whether the input data has image.\nReturns:\n    The input_ids and labels for the model.", "calls": ["deepcopy", "zip", "dict", "append", "tokenizer_image_token", "len", "len", "tokenizer_image_token"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::preprocess_llama_from_llava_v1", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "preprocess_llama_from_llava_v1", "kind": "Function", "parent": null, "start_line": 179, "end_line": 267, "docstring": "This function add the prompt and then put the image after the prompt.\nSo it needs additional code to generate the target label.\nArgs:\n    sources: The input data with text and image.\n    tokenizer: The tokenizer to process text.\n    has_image: Whether the input data has image.\nReturns:\n    The input_ids and labels for the model.", "calls": ["copy", "enumerate", "clone", "zip", "dict", "enumerate", "append", "stack", "int", "split", "enumerate", "append_message", "get_prompt", "tokenizer", "sum", "split", "tokenizer_image_token", "len", "len", "len", "print", "ne", "tokenizer_image_token", "len", "len", "tokenizer_image_token", "tokenizer", "tokenizer"], "decorators": [], "num_calls": 28}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::DataCollatorForSupervisedDataset", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "DataCollatorForSupervisedDataset", "kind": "Class", "parent": null, "start_line": 271, "end_line": 300, "docstring": "Collate examples for supervised fine-tuning.", "calls": ["tuple", "pad_sequence", "pad_sequence", "dict", "all", "ne", "stack"], "decorators": ["dataclass"], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::__init__", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "__init__", "kind": "Function", "parent": "CustomMultiModalDataset", "start_line": 23, "end_line": 30, "docstring": null, "calls": ["__init__", "load", "print", "open", "super"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::__len__", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "__len__", "kind": "Function", "parent": "CustomMultiModalDataset", "start_line": 32, "end_line": 33, "docstring": null, "calls": ["len"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::register_tokenizer", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "register_tokenizer", "kind": "Function", "parent": "CustomMultiModalDataset", "start_line": 35, "end_line": 38, "docstring": null, "calls": ["getattr"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::__getitem__", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "__getitem__", "kind": "Function", "parent": "CustomMultiModalDataset", "start_line": 40, "end_line": 96, "docstring": null, "calls": ["isinstance", "isinstance", "len", "convert", "preprocess_multimodal_llava", "deepcopy", "preprocess_llama_from_llava_plain", "preprocess_llama_from_llava_v1", "dict", "zeros", "expand2square", "deepcopy", "open", "tuple", "join", "preprocess", "preprocess", "new", "paste", "new", "paste", "int"], "decorators": [], "num_calls": 22}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::insert_separator", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "insert_separator", "kind": "Function", "parent": "tokenizer_image_token", "start_line": 130, "end_line": 131, "docstring": null, "calls": ["zip", "len"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::__call__", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "__call__", "kind": "Function", "parent": "DataCollatorForSupervisedDataset", "start_line": 276, "end_line": 300, "docstring": null, "calls": ["tuple", "pad_sequence", "pad_sequence", "dict", "all", "ne", "stack"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.datasets.multi_modal_dataset::expand2square", "path": "v2/src/lmflow/datasets/multi_modal_dataset.py", "module": "v2.src.lmflow.datasets.multi_modal_dataset", "name": "expand2square", "kind": "Function", "parent": null, "start_line": 51, "end_line": 62, "docstring": null, "calls": ["new", "paste", "new", "paste"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.datasets.dataset::Dataset", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "Dataset", "kind": "Class", "parent": null, "start_line": 60, "end_line": 597, "docstring": "Initializes the Dataset object with the given parameters.\n\nParameters\n------------\ndata_args : DatasetArguments object.\n    Contains the arguments required to load the dataset.\n\nbackend : str,  default=\"huggingface\"\n    A string representing the dataset backend. Defaults to \"huggingface\".\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.", "calls": ["len", "tqdm", "DatasetArguments", "Dataset", "from_dict", "info", "_check_hf_json_format", "info", "load_dataset", "_check_instance_format", "issubset", "ValueError", "get_dataset_type_fast", "check_dataset_instances_key_fast", "enumerate", "_check_instance_format", "get_type", "to_dict", "keys", "map", "NotImplementedError", "error", "select", "create_from_dict", "NotImplementedError", "train_test_split", "create_from_dict", "create_from_dict", "NotImplementedError", "remove_indices"], "decorators": [], "num_calls": 101}
{"symbol_id": "v2.src.lmflow.datasets.dataset::__init__", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "__init__", "kind": "Function", "parent": "Dataset", "start_line": 78, "end_line": 121, "docstring": null, "calls": ["info", "_check_hf_json_format", "info", "load_dataset", "_check_instance_format", "as_posix", "glob", "CustomMultiModalDataset", "NotImplementedError", "absolute", "is_multimodal_available", "ValueError", "Path"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.datasets.dataset::__len__", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "__len__", "kind": "Function", "parent": "Dataset", "start_line": 124, "end_line": 125, "docstring": null, "calls": ["len"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.datasets.dataset::_check_instance_format", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "_check_instance_format", "kind": "Function", "parent": "Dataset", "start_line": 128, "end_line": 139, "docstring": "Checks if data (instances) have required fields. \nRaises messages with hints if not matched.", "calls": ["issubset", "ValueError", "set", "set", "list"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.datasets.dataset::_check_hf_json_format", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "_check_hf_json_format", "kind": "Function", "parent": "Dataset", "start_line": 142, "end_line": 167, "docstring": null, "calls": ["tqdm", "get_dataset_type_fast", "check_dataset_instances_key_fast", "ValueError", "ValueError", "ValueError"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.datasets.dataset::from_dict", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "from_dict", "kind": "Function", "parent": "Dataset", "start_line": 170, "end_line": 263, "docstring": "Create a Dataset object from a dictionary.\n\nReturn a Dataset given a dict with format:\n    {\n        \"type\": TYPE,\n        \"instances\": [\n            {\n                \"key_1\": VALUE_1.1,\n                \"key_2\": VALUE_1.2,\n                ...\n            },\n            {\n                \"key_1\": VALUE_2.1,\n                \"key_2\": VALUE_2.2,\n                ...\n            },\n            ...\n        ]\n    }\n\nParameters\n-----------\n\ndict_obj : dict.\n    A dictionary containing the dataset information.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n---------\n\nself : Dataset object.", "calls": ["enumerate", "_check_instance_format", "ValueError", "ValueError", "ValueError", "keys", "from_dict", "NotImplementedError", "issubset", "ValueError", "len", "keys", "ValueError", "set", "set", "list"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.datasets.dataset::create_from_dict", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "create_from_dict", "kind": "Function", "parent": "Dataset", "start_line": 267, "end_line": 276, "docstring": "Returns\n--------\n\nReturns a Dataset object given a dict.", "calls": ["DatasetArguments", "Dataset", "from_dict"], "decorators": ["classmethod"], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.datasets.dataset::to_dict", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "to_dict", "kind": "Function", "parent": "Dataset", "start_line": 279, "end_line": 331, "docstring": "Returns\n---------\n\nReturn a dict represents the dataset:\n    {\n        \"type\": TYPE,\n        \"instances\": [\n            {\n                \"key_1\": VALUE_1.1,\n                \"key_2\": VALUE_1.2,\n                ...\n            },\n            {\n                \"key_1\": VALUE_2.1,\n                \"key_2\": VALUE_2.2,\n                ...\n            },\n            ...\n        ]\n    }\n\nA python dict object represents the content of this dataset.", "calls": ["get_type", "to_dict", "keys", "len", "NotImplementedError", "range", "keys"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.datasets.dataset::to_list", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "to_list", "kind": "Function", "parent": "Dataset", "start_line": 334, "end_line": 347, "docstring": "Returns a list of instances.", "calls": ["__getitem__", "deepcopy", "NotImplementedError", "range", "len"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.datasets.dataset::map", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "map", "kind": "Function", "parent": "Dataset", "start_line": 350, "end_line": 376, "docstring": "Parameters\n------------\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n---------\n\nself : Dataset object.", "calls": ["map", "NotImplementedError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.datasets.dataset::get_backend", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "get_backend", "kind": "Function", "parent": "Dataset", "start_line": 379, "end_line": 386, "docstring": "Returns\n---------\n\nself.backend", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.datasets.dataset::get_backend_dataset", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "get_backend_dataset", "kind": "Function", "parent": "Dataset", "start_line": 389, "end_line": 396, "docstring": "Returns\n---------\n\nself.backend_dataset", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.datasets.dataset::get_fingerprint", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "get_fingerprint", "kind": "Function", "parent": "Dataset", "start_line": 399, "end_line": 406, "docstring": "Returns\n---------\n\nFingerprint of the backend_dataset which controls the cache", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.datasets.dataset::get_data_args", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "get_data_args", "kind": "Function", "parent": "Dataset", "start_line": 409, "end_line": 416, "docstring": "Returns\n---------\n\nself.data_args", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.datasets.dataset::get_type", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "get_type", "kind": "Function", "parent": "Dataset", "start_line": 419, "end_line": 426, "docstring": "Returns\n---------\n\nself.type", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.datasets.dataset::save", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "save", "kind": "Function", "parent": "Dataset", "start_line": 429, "end_line": 448, "docstring": "Save the dataset to a json file.\n\nParameters\n------------\nfile_path : str.\n    The path to the file where the dataset will be saved.", "calls": ["error", "open", "dump", "Path", "to_dict"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.datasets.dataset::sample", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "sample", "kind": "Function", "parent": "Dataset", "start_line": 451, "end_line": 482, "docstring": "Sample n instances from the dataset.\n\nParameters\n------------\nn : int.\n    The number of instances to sample from the dataset.\n\nReturns\n---------\n\nsample_dataset : Dataset object.\n    A new dataset object containing the sampled instances.", "calls": ["select", "create_from_dict", "NotImplementedError", "range", "shuffle", "get_type", "range"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.datasets.dataset::train_test_split", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "train_test_split", "kind": "Function", "parent": "Dataset", "start_line": 485, "end_line": 531, "docstring": "Split the dataset into training and testing sets.\n\nParameters\n------------\ntest_size : float, default=0.2.\n    The proportion of the dataset that will be used for testing.\n\nReturns\n---------\n\ntrain_dataset : Dataset object.\n    A new dataset object containing the training instances.\n\ntest_dataset : Dataset object.\n    A new dataset object containing the testing instances.", "calls": ["train_test_split", "create_from_dict", "create_from_dict", "NotImplementedError", "get_type", "get_type", "range", "range", "len", "len"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.datasets.dataset::drop_instances", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "drop_instances", "kind": "Function", "parent": "Dataset", "start_line": 534, "end_line": 548, "docstring": "Drop instances from the dataset.\n\nParameters\n------------\nindices : list.\n    A list of indices of the instances to drop from the dataset.", "calls": ["remove_indices", "NotImplementedError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.datasets.dataset::sanity_check", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "sanity_check", "kind": "Function", "parent": "Dataset", "start_line": 551, "end_line": 563, "docstring": "Perform a sanity check on the dataset.", "calls": ["hf_dataset_sanity_check", "NotImplementedError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.datasets.dataset::hf_dataset_sanity_check", "path": "v2/src/lmflow/datasets/dataset.py", "module": "v2.src.lmflow.datasets.dataset", "name": "hf_dataset_sanity_check", "kind": "Function", "parent": "Dataset", "start_line": 566, "end_line": 597, "docstring": "Perform a sanity check on the HuggingFace dataset.", "calls": ["ValueError", "len", "filter", "filter", "filter", "warning", "len", "len", "len", "warning", "ValueError", "len", "len", "all", "len", "len", "len"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.lamb::Lamb", "path": "v2/src/lmflow/optim/lamb.py", "module": "v2.src.lmflow.optim.lamb", "name": "Lamb", "kind": "Class", "parent": null, "start_line": 8, "end_line": 132, "docstring": "Implements Lamb algorithm.\n\nIt has been proposed in `Large Batch Optimization for Deep Learning:\nTraining BERT in 76 minutes`\nhttps://arxiv.org/abs/1904.00962\n\nNote:\n    Reference code: https://github.com/cybertronai/pytorch-lamb", "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "closure", "format", "format", "format", "format", "format", "format", "super", "add_", "addcmul_", "clamp", "norm", "add_", "RuntimeError", "len", "zeros_like", "zeros_like", "sqrt", "add", "add_", "mul_", "mul_"], "decorators": [], "num_calls": 32}
{"symbol_id": "v2.src.lmflow.optim.lamb::__init__", "path": "v2/src/lmflow/optim/lamb.py", "module": "v2.src.lmflow.optim.lamb", "name": "__init__", "kind": "Function", "parent": "Lamb", "start_line": 19, "end_line": 54, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.optim.lamb::step", "path": "v2/src/lmflow/optim/lamb.py", "module": "v2.src.lmflow.optim.lamb", "name": "step", "kind": "Function", "parent": "Lamb", "start_line": 56, "end_line": 132, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["closure", "add_", "addcmul_", "clamp", "norm", "add_", "RuntimeError", "len", "zeros_like", "zeros_like", "sqrt", "add", "add_", "mul_", "mul_", "norm", "sqrt"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.adan::Adan", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "Adan", "kind": "Class", "parent": null, "start_line": 10, "end_line": 166, "docstring": "Implements a pytorch variant of Adan.\n\nAdan was proposed in\nAdan : Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models.\nhttps://arxiv.org/abs/2208.06677", "calls": ["no_grad", "no_grad", "dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "setdefault", "zeros", "tensor", "clamp", "dict", "zip", "format", "format", "format", "format", "format", "format", "super", "super", "sqrt", "append", "append", "append", "append"], "decorators": [], "num_calls": 45}
{"symbol_id": "v2.src.lmflow.optim.adan::_single_tensor_adan", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "_single_tensor_adan", "kind": "Function", "parent": null, "start_line": 169, "end_line": 217, "docstring": null, "calls": ["enumerate", "mul_", "append", "add_", "add_", "addcmul_", "add_", "div_", "clone", "mul_", "add_", "add_", "div_", "mul_", "mul_", "mul_", "add_", "sqrt"], "decorators": [], "num_calls": 18}
{"symbol_id": "v2.src.lmflow.optim.adan::_multi_tensor_adan", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "_multi_tensor_adan", "kind": "Function", "parent": null, "start_line": 220, "end_line": 280, "docstring": null, "calls": ["_foreach_sub", "_foreach_add", "_foreach_mul_", "_foreach_add_", "_foreach_mul_", "_foreach_add_", "_foreach_mul_", "_foreach_addcmul_", "_foreach_sqrt", "_foreach_div_", "_foreach_add_", "_foreach_div", "_foreach_add_", "_foreach_div_", "_foreach_mul_", "clone", "_foreach_mul", "_foreach_mul_", "_foreach_add_", "_foreach_div_", "item"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.optim.adan::__init__", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "__init__", "kind": "Function", "parent": "Adan", "start_line": 19, "end_line": 51, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.optim.adan::__setstate__", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "__setstate__", "kind": "Function", "parent": "Adan", "start_line": 53, "end_line": 56, "docstring": null, "calls": ["__setstate__", "setdefault", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adan::restart_opt", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "restart_opt", "kind": "Function", "parent": "Adan", "start_line": 59, "end_line": 72, "docstring": null, "calls": ["no_grad", "zeros_like", "zeros_like", "zeros_like"], "decorators": [null], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.optim.adan::step", "path": "v2/src/lmflow/optim/adan.py", "module": "v2.src.lmflow.optim.adan", "name": "step", "kind": "Function", "parent": "Adan", "start_line": 75, "end_line": 166, "docstring": "Performs a single optimization step.", "calls": ["no_grad", "zeros", "tensor", "clamp", "dict", "zip", "sqrt", "append", "append", "append", "append", "append", "append", "_multi_tensor_adan", "_single_tensor_adan", "len", "zeros_like", "zeros_like", "zeros_like", "sqrt", "add_", "sum", "pow"], "decorators": [null], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.optim.lars::LARS", "path": "v2/src/lmflow/optim/lars.py", "module": "v2.src.lmflow.optim.lars", "name": "LARS", "kind": "Class", "parent": null, "start_line": 7, "end_line": 143, "docstring": "Extends SGD in PyTorch with LARS scaling from the paper\n`Large batch training of Convolutional Networks`__.\n.. note::\n    The application of momentum in the SGD part is modified according to\n    the PyTorch standards. LARS scaling fits into the equation in the\n    following fashion.\n\n    .. math::\n        \\begin{aligned}\n            g_{t+1} & = \\text{lars_lr} * (\\beta * p_{t} + g_{t+1}), \\\\\n            v_{t+1} & = \\\\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\\\end{aligned}\n\n    where :math:`p`, :math:`g`, :math:`v`, :math:`\\\\mu` and :math:`\\beta`\n    denote the parameters, gradient, velocity, momentum, and weight decay\n    respectively.  The :math:`lars_lr` is defined by Eq. 6 in the paper.\n    The Nesterov version is analogously modified.\n\n.. warning::\n    Parameters with weight decay set to 0 will automatically be excluded\n    from layer-wise LR scaling. This is to ensure consistency with papers\n    like SimCLR and BYOL.\n\n\n__ https://arxiv.org/pdf/1708.03888.pdf\n\nNote:\n    Reference code: https://github.com/PyTorchLightning/lightning-bolts/", "calls": ["no_grad", "dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "setdefault", "format", "format", "format", "format", "format", "format", "super", "super", "enable_grad", "closure", "norm", "norm", "add_", "add", "detach", "add_", "add", "clone"], "decorators": [], "num_calls": 31}
{"symbol_id": "v2.src.lmflow.optim.lars::__init__", "path": "v2/src/lmflow/optim/lars.py", "module": "v2.src.lmflow.optim.lars", "name": "__init__", "kind": "Function", "parent": "LARS", "start_line": 39, "end_line": 81, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.optim.lars::__setstate__", "path": "v2/src/lmflow/optim/lars.py", "module": "v2.src.lmflow.optim.lars", "name": "__setstate__", "kind": "Function", "parent": "LARS", "start_line": 83, "end_line": 87, "docstring": null, "calls": ["__setstate__", "setdefault", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.lars::step", "path": "v2/src/lmflow/optim/lars.py", "module": "v2.src.lmflow.optim.lars", "name": "step", "kind": "Function", "parent": "LARS", "start_line": 90, "end_line": 143, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["no_grad", "enable_grad", "closure", "norm", "norm", "add_", "add", "detach", "add_", "add", "clone", "mul_"], "decorators": [null], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.optim.adamax::Adamax", "path": "v2/src/lmflow/optim/adamax.py", "module": "v2.src.lmflow.optim.adamax", "name": "Adamax", "kind": "Class", "parent": null, "start_line": 7, "end_line": 68, "docstring": null, "calls": ["dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "closure", "format", "format", "format", "format", "format", "super", "super", "add_", "cat", "max", "addcdiv_", "RuntimeError", "len", "zeros_like", "zeros_like", "add", "mul_", "unsqueeze", "unsqueeze_", "long", "mul_"], "decorators": [], "num_calls": 32}
{"symbol_id": "v2.src.lmflow.optim.adamax::__init__", "path": "v2/src/lmflow/optim/adamax.py", "module": "v2.src.lmflow.optim.adamax", "name": "__init__", "kind": "Function", "parent": "Adamax", "start_line": 8, "end_line": 20, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.adamax::__setstate__", "path": "v2/src/lmflow/optim/adamax.py", "module": "v2.src.lmflow.optim.adamax", "name": "__setstate__", "kind": "Function", "parent": "Adamax", "start_line": 22, "end_line": 23, "docstring": null, "calls": ["__setstate__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.optim.adamax::step", "path": "v2/src/lmflow/optim/adamax.py", "module": "v2.src.lmflow.optim.adamax", "name": "step", "kind": "Function", "parent": "Adamax", "start_line": 25, "end_line": 68, "docstring": null, "calls": ["closure", "add_", "cat", "max", "addcdiv_", "RuntimeError", "len", "zeros_like", "zeros_like", "add", "mul_", "unsqueeze", "unsqueeze_", "long", "mul_", "abs", "new"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.sgd_schedule_free::SGDScheduleFree", "path": "v2/src/lmflow/optim/sgd_schedule_free.py", "module": "v2.src.lmflow.optim.sgd_schedule_free", "name": "SGDScheduleFree", "kind": "Class", "parent": null, "start_line": 12, "end_line": 158, "docstring": "Schedule-Free SGD\nAs the name suggests, no scheduler is needed with this optimizer. \nTo add warmup, rather than using a learning rate schedule you can just\nset the warmup_steps parameter.\n\nThis optimizer requires that .train() and .eval() be called before the\nbeginning of training and evaluation respectively. The optimizer should\nalso be placed in eval mode when saving checkpoints.", "calls": ["hasattr", "dict", "__init__", "ValueError", "ValueError", "ValueError", "closure", "max", "format", "format", "format", "super", "Exception", "zip", "_foreach_lerp_", "_foreach_add_", "_foreach_sub_", "clone", "len", "_foreach_add_", "lerp_", "add_", "sub_", "lerp_", "lerp_", "add_"], "decorators": [], "num_calls": 26}
{"symbol_id": "v2.src.lmflow.optim.sgd_schedule_free::__init__", "path": "v2/src/lmflow/optim/sgd_schedule_free.py", "module": "v2.src.lmflow.optim.sgd_schedule_free", "name": "__init__", "kind": "Function", "parent": "SGDScheduleFree", "start_line": 23, "end_line": 51, "docstring": null, "calls": ["hasattr", "dict", "__init__", "ValueError", "ValueError", "ValueError", "format", "format", "format", "super"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.optim.sgd_schedule_free::eval", "path": "v2/src/lmflow/optim/sgd_schedule_free.py", "module": "v2.src.lmflow.optim.sgd_schedule_free", "name": "eval", "kind": "Function", "parent": "SGDScheduleFree", "start_line": 53, "end_line": 63, "docstring": null, "calls": ["lerp_"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.optim.sgd_schedule_free::train", "path": "v2/src/lmflow/optim/sgd_schedule_free.py", "module": "v2.src.lmflow.optim.sgd_schedule_free", "name": "train", "kind": "Function", "parent": "SGDScheduleFree", "start_line": 65, "end_line": 75, "docstring": null, "calls": ["lerp_"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.optim.sgd_schedule_free::step", "path": "v2/src/lmflow/optim/sgd_schedule_free.py", "module": "v2.src.lmflow.optim.sgd_schedule_free", "name": "step", "kind": "Function", "parent": "SGDScheduleFree", "start_line": 77, "end_line": 158, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.", "calls": ["closure", "max", "Exception", "zip", "_foreach_lerp_", "_foreach_add_", "_foreach_sub_", "clone", "len", "_foreach_add_", "lerp_", "add_", "sub_", "add_"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.optim.adamp::AdamP", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "AdamP", "kind": "Class", "parent": null, "start_line": 9, "end_line": 173, "docstring": "Implements AdamP algorithm.\n\nIt has been proposed in `Slowing Down the Weight Norm Increase in\nMomentum-based Optimizers`\nhttps://arxiv.org/abs/2006.08217\n\nNote:\n    Reference code: https://github.com/clovaai/AdamP", "calls": ["dict", "__init__", "view", "view", "view_func", "view_func", "add_", "add_", "sum", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "size", "_cosine_similarity", "closure", "format", "format", "format", "format", "format", "format", "format", "super", "norm", "norm", "abs"], "decorators": [], "num_calls": 56}
{"symbol_id": "v2.src.lmflow.optim.adamp::__init__", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "__init__", "kind": "Function", "parent": "AdamP", "start_line": 20, "end_line": 61, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.adamp::_channel_view", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "_channel_view", "kind": "Function", "parent": "AdamP", "start_line": 64, "end_line": 65, "docstring": null, "calls": ["view", "size"], "decorators": ["staticmethod"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.optim.adamp::_layer_view", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "_layer_view", "kind": "Function", "parent": "AdamP", "start_line": 68, "end_line": 69, "docstring": null, "calls": ["view"], "decorators": ["staticmethod"], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.optim.adamp::_cosine_similarity", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "_cosine_similarity", "kind": "Function", "parent": "AdamP", "start_line": 72, "end_line": 80, "docstring": null, "calls": ["view_func", "view_func", "add_", "add_", "sum", "norm", "norm", "abs"], "decorators": ["staticmethod"], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.optim.adamp::_projection", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "_projection", "kind": "Function", "parent": "AdamP", "start_line": 82, "end_line": 99, "docstring": null, "calls": ["_cosine_similarity", "max", "len", "sqrt", "add_", "view", "size", "view", "sum", "view_func", "norm", "view_func", "view_func"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.adamp::step", "path": "v2/src/lmflow/optim/adamp.py", "module": "v2.src.lmflow.optim.adamp", "name": "step", "kind": "Function", "parent": "AdamP", "start_line": 101, "end_line": 173, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["closure", "add_", "addcmul_", "add_", "add_", "len", "zeros_like", "zeros_like", "len", "_projection", "mul_", "mul_", "mul_", "sqrt", "sqrt"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.optim.adabelief::AdaBelief", "path": "v2/src/lmflow/optim/adabelief.py", "module": "v2.src.lmflow.optim.adabelief", "name": "AdaBelief", "kind": "Class", "parent": null, "start_line": 8, "end_line": 184, "docstring": "Implements AdaBelief algorithm. Modified from Adam in PyTorch\nreference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients, NeurIPS 2020", "calls": ["dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance", "print", "print", "print", "setdefault", "closure", "format", "format", "format", "format", "len", "super", "print", "super", "zeros_like", "zeros_like", "add_", "addcmul_", "zeros_like", "float", "float", "RuntimeError"], "decorators": [], "num_calls": 59}
{"symbol_id": "v2.src.lmflow.optim.adabelief::__init__", "path": "v2/src/lmflow/optim/adabelief.py", "module": "v2.src.lmflow.optim.adabelief", "name": "__init__", "kind": "Function", "parent": "AdaBelief", "start_line": 13, "end_line": 47, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance", "print", "print", "print", "format", "format", "format", "format", "len", "super", "print", "range", "range"], "decorators": [], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.optim.adabelief::__setstate__", "path": "v2/src/lmflow/optim/adabelief.py", "module": "v2.src.lmflow.optim.adabelief", "name": "__setstate__", "kind": "Function", "parent": "AdaBelief", "start_line": 49, "end_line": 52, "docstring": null, "calls": ["__setstate__", "setdefault", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adabelief::reset", "path": "v2/src/lmflow/optim/adabelief.py", "module": "v2.src.lmflow.optim.adabelief", "name": "reset", "kind": "Function", "parent": "AdaBelief", "start_line": 54, "end_line": 69, "docstring": null, "calls": ["zeros_like", "zeros_like", "zeros_like"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adabelief::step", "path": "v2/src/lmflow/optim/adabelief.py", "module": "v2.src.lmflow.optim.adabelief", "name": "step", "kind": "Function", "parent": "AdaBelief", "start_line": 71, "end_line": 184, "docstring": "Performs a single optimization step.\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.", "calls": ["closure", "add_", "addcmul_", "float", "float", "RuntimeError", "len", "zeros_like", "zeros_like", "max", "add_", "add_", "addcdiv_", "half", "half", "zeros_like", "mul_", "mul_", "add_", "mul_", "mul_", "add_", "int", "add_", "addcdiv_", "add_", "sqrt", "sqrt", "sqrt", "sqrt"], "decorators": [], "num_calls": 33}
{"symbol_id": "v2.src.lmflow.optim.adagrad::AdaGrad", "path": "v2/src/lmflow/optim/adagrad.py", "module": "v2.src.lmflow.optim.adagrad", "name": "AdaGrad", "kind": "Class", "parent": null, "start_line": 7, "end_line": 35, "docstring": null, "calls": ["dict", "__init__", "closure", "super", "addcmul_", "add_", "addcdiv_", "add", "len", "zeros_like", "sqrt"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.optim.adagrad::__init__", "path": "v2/src/lmflow/optim/adagrad.py", "module": "v2.src.lmflow.optim.adagrad", "name": "__init__", "kind": "Function", "parent": "AdaGrad", "start_line": 8, "end_line": 10, "docstring": null, "calls": ["dict", "__init__", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adagrad::step", "path": "v2/src/lmflow/optim/adagrad.py", "module": "v2.src.lmflow.optim.adagrad", "name": "step", "kind": "Function", "parent": "AdaGrad", "start_line": 12, "end_line": 35, "docstring": null, "calls": ["closure", "addcmul_", "add_", "addcdiv_", "add", "len", "zeros_like", "sqrt"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.optim.novograd::NovoGrad", "path": "v2/src/lmflow/optim/novograd.py", "module": "v2.src.lmflow.optim.novograd", "name": "NovoGrad", "kind": "Class", "parent": null, "start_line": 7, "end_line": 84, "docstring": null, "calls": ["dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "setdefault", "closure", "format", "format", "format", "format", "format", "super", "super", "sum", "div_", "add_", "add_", "RuntimeError", "len", "zeros_like", "to", "pow", "copy_", "add_", "max", "add_"], "decorators": [], "num_calls": 40}
{"symbol_id": "v2.src.lmflow.optim.novograd::__init__", "path": "v2/src/lmflow/optim/novograd.py", "module": "v2.src.lmflow.optim.novograd", "name": "__init__", "kind": "Function", "parent": "NovoGrad", "start_line": 8, "end_line": 20, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.novograd::__setstate__", "path": "v2/src/lmflow/optim/novograd.py", "module": "v2.src.lmflow.optim.novograd", "name": "__setstate__", "kind": "Function", "parent": "NovoGrad", "start_line": 22, "end_line": 25, "docstring": null, "calls": ["__setstate__", "setdefault", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.novograd::step", "path": "v2/src/lmflow/optim/novograd.py", "module": "v2.src.lmflow.optim.novograd", "name": "step", "kind": "Function", "parent": "NovoGrad", "start_line": 27, "end_line": 84, "docstring": null, "calls": ["closure", "sum", "div_", "add_", "add_", "RuntimeError", "len", "zeros_like", "to", "pow", "copy_", "add_", "max", "add_", "add_", "add_", "mul_", "to", "mul_", "zeros", "mul_", "sqrt", "sqrt", "zeros"], "decorators": [], "num_calls": 24}
{"symbol_id": "v2.src.lmflow.optim.yogi::Yogi", "path": "v2/src/lmflow/optim/yogi.py", "module": "v2.src.lmflow.optim.yogi", "name": "Yogi", "kind": "Class", "parent": null, "start_line": 9, "end_line": 126, "docstring": "Implements Yogi Optimizer Algorithm.\nIt has been proposed in `Adaptive methods for Nonconvex Optimization`.\n\nhttps://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization  # noqa\n\nNote:\n    Reference code: https://github.com/4rtemi5/Yogi-Optimizer_Keras", "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "closure", "format", "format", "format", "format", "format", "super", "add_", "mul", "addcmul_", "add_", "addcdiv_", "RuntimeError", "len", "constant_", "constant_", "add", "sign", "empty_like", "empty_like", "mul_", "sqrt", "sqrt"], "decorators": [], "num_calls": 30}
{"symbol_id": "v2.src.lmflow.optim.yogi::__init__", "path": "v2/src/lmflow/optim/yogi.py", "module": "v2.src.lmflow.optim.yogi", "name": "__init__", "kind": "Function", "parent": "Yogi", "start_line": 19, "end_line": 52, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.yogi::step", "path": "v2/src/lmflow/optim/yogi.py", "module": "v2.src.lmflow.optim.yogi", "name": "step", "kind": "Function", "parent": "Yogi", "start_line": 54, "end_line": 126, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["closure", "add_", "mul", "addcmul_", "add_", "addcdiv_", "RuntimeError", "len", "constant_", "constant_", "add", "sign", "empty_like", "empty_like", "mul_", "sqrt", "sqrt"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.radam::RAdam", "path": "v2/src/lmflow/optim/radam.py", "module": "v2.src.lmflow.optim.radam", "name": "RAdam", "kind": "Class", "parent": null, "start_line": 9, "end_line": 173, "docstring": "Implements RAdam optimization algorithm.\n\nNote:\n    Deprecated, please use version provided by PyTorch_.\n\nIt has been proposed in `On the Variance of the Adaptive Learning\nRate and Beyond`.\nhttps://arxiv.org/abs/1908.03265\n\nNote:\n    Reference code: https://github.com/LiyuanLucasLiu/RAdam", "calls": ["warn", "dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance", "closure", "format", "format", "format", "format", "format", "len", "super", "super", "float", "float", "addcmul_", "add_", "copy_", "RuntimeError", "len", "zeros_like", "zeros_like", "type_as"], "decorators": [], "num_calls": 42}
{"symbol_id": "v2.src.lmflow.optim.radam::__init__", "path": "v2/src/lmflow/optim/radam.py", "module": "v2.src.lmflow.optim.radam", "name": "__init__", "kind": "Function", "parent": "RAdam", "start_line": 23, "end_line": 73, "docstring": null, "calls": ["warn", "dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "isinstance", "isinstance", "format", "format", "format", "format", "format", "len", "super", "range", "range"], "decorators": [], "num_calls": 19}
{"symbol_id": "v2.src.lmflow.optim.radam::__setstate__", "path": "v2/src/lmflow/optim/radam.py", "module": "v2.src.lmflow.optim.radam", "name": "__setstate__", "kind": "Function", "parent": "RAdam", "start_line": 75, "end_line": 76, "docstring": null, "calls": ["__setstate__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.optim.radam::step", "path": "v2/src/lmflow/optim/radam.py", "module": "v2.src.lmflow.optim.radam", "name": "step", "kind": "Function", "parent": "RAdam", "start_line": 78, "end_line": 173, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["closure", "float", "float", "addcmul_", "add_", "copy_", "RuntimeError", "len", "zeros_like", "zeros_like", "type_as", "type_as", "int", "add_", "add_", "addcdiv_", "add_", "mul_", "mul_", "sqrt", "sqrt"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.optim.adam::Adam", "path": "v2/src/lmflow/optim/adam.py", "module": "v2.src.lmflow.optim.adam", "name": "Adam", "kind": "Class", "parent": null, "start_line": 7, "end_line": 46, "docstring": null, "calls": ["dict", "__init__", "closure", "super", "add_", "addcmul_", "add_", "addcdiv_", "len", "zeros_like", "zeros_like", "mul_", "mul_", "sqrt"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.optim.adam::__init__", "path": "v2/src/lmflow/optim/adam.py", "module": "v2.src.lmflow.optim.adam", "name": "__init__", "kind": "Function", "parent": "Adam", "start_line": 8, "end_line": 10, "docstring": null, "calls": ["dict", "__init__", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adam::step", "path": "v2/src/lmflow/optim/adam.py", "module": "v2.src.lmflow.optim.adam", "name": "step", "kind": "Function", "parent": "Adam", "start_line": 12, "end_line": 46, "docstring": null, "calls": ["closure", "add_", "addcmul_", "add_", "addcdiv_", "len", "zeros_like", "zeros_like", "mul_", "mul_", "sqrt"], "decorators": [], "num_calls": 11}
{"symbol_id": "v2.src.lmflow.optim.adamw_schedule_free::AdamWScheduleFree", "path": "v2/src/lmflow/optim/adamw_schedule_free.py", "module": "v2.src.lmflow.optim.adamw_schedule_free", "name": "AdamWScheduleFree", "kind": "Class", "parent": null, "start_line": 13, "end_line": 179, "docstring": "Schedule-Free AdamW\nAs the name suggests, no scheduler is needed with this optimizer. \nTo add warmup, rather than using a learning rate schedule you can just\nset the warmup_steps parameter.\n\nThis optimizer requires that .train() and .eval() be called before the\nbeginning of training and evaluation respectively. The optimizer should\nalso be placed in eval mode when saving checkpoints.", "calls": ["hasattr", "dict", "__init__", "closure", "max", "super", "sqrt", "Exception", "zip", "_foreach_mul_", "_foreach_addcmul_", "_foreach_sqrt", "_foreach_add_", "_foreach_div_", "_foreach_lerp_", "_foreach_add_", "_foreach_sub_", "clone", "zeros_like", "len", "_foreach_add_", "addcmul_", "add_", "div_", "lerp_", "add_", "sub_", "lerp_", "lerp_", "add_"], "decorators": [], "num_calls": 32}
{"symbol_id": "v2.src.lmflow.optim.adamw_schedule_free::__init__", "path": "v2/src/lmflow/optim/adamw_schedule_free.py", "module": "v2.src.lmflow.optim.adamw_schedule_free", "name": "__init__", "kind": "Function", "parent": "AdamWScheduleFree", "start_line": 24, "end_line": 48, "docstring": null, "calls": ["hasattr", "dict", "__init__", "super"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.optim.adamw_schedule_free::eval", "path": "v2/src/lmflow/optim/adamw_schedule_free.py", "module": "v2.src.lmflow.optim.adamw_schedule_free", "name": "eval", "kind": "Function", "parent": "AdamWScheduleFree", "start_line": 50, "end_line": 60, "docstring": null, "calls": ["lerp_"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.optim.adamw_schedule_free::train", "path": "v2/src/lmflow/optim/adamw_schedule_free.py", "module": "v2.src.lmflow.optim.adamw_schedule_free", "name": "train", "kind": "Function", "parent": "AdamWScheduleFree", "start_line": 62, "end_line": 72, "docstring": null, "calls": ["lerp_"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.optim.adamw_schedule_free::step", "path": "v2/src/lmflow/optim/adamw_schedule_free.py", "module": "v2.src.lmflow.optim.adamw_schedule_free", "name": "step", "kind": "Function", "parent": "AdamWScheduleFree", "start_line": 74, "end_line": 179, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.", "calls": ["closure", "max", "sqrt", "Exception", "zip", "_foreach_mul_", "_foreach_addcmul_", "_foreach_sqrt", "_foreach_add_", "_foreach_div_", "_foreach_lerp_", "_foreach_add_", "_foreach_sub_", "clone", "zeros_like", "len", "_foreach_add_", "addcmul_", "add_", "div_", "lerp_", "add_", "sub_", "add_", "mul_", "sqrt"], "decorators": [], "num_calls": 26}
{"symbol_id": "v2.src.lmflow.optim.sophia::SophiaG", "path": "v2/src/lmflow/optim/sophia.py", "module": "v2.src.lmflow.optim.sophia", "name": "SophiaG", "kind": "Class", "parent": null, "start_line": 7, "end_line": 117, "docstring": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.\nCode from: https://github.com/Liuhong99/Sophia/", "calls": ["no_grad", "no_grad", "dict", "__init__", "__setstate__", "list", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "setdefault", "setdefault", "values", "is_tensor", "zip", "format", "format", "format", "format", "format", "super", "super", "len", "tensor", "addcmul_", "enable_grad", "closure", "append", "append"], "decorators": [], "num_calls": 60}
{"symbol_id": "v2.src.lmflow.optim.sophia::__init__", "path": "v2/src/lmflow/optim/sophia.py", "module": "v2.src.lmflow.optim.sophia", "name": "__init__", "kind": "Function", "parent": "SophiaG", "start_line": 13, "end_line": 29, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.sophia::__setstate__", "path": "v2/src/lmflow/optim/sophia.py", "module": "v2.src.lmflow.optim.sophia", "name": "__setstate__", "kind": "Function", "parent": "SophiaG", "start_line": 31, "end_line": 40, "docstring": null, "calls": ["__setstate__", "list", "setdefault", "setdefault", "values", "is_tensor", "super", "len", "tensor", "float"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.optim.sophia::update_hessian", "path": "v2/src/lmflow/optim/sophia.py", "module": "v2.src.lmflow.optim.sophia", "name": "update_hessian", "kind": "Function", "parent": "SophiaG", "start_line": 43, "end_line": 60, "docstring": null, "calls": ["no_grad", "addcmul_", "len", "zeros_like", "zeros_like", "keys", "zeros_like", "zeros", "tensor", "mul_"], "decorators": [null], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.optim.sophia::step", "path": "v2/src/lmflow/optim/sophia.py", "module": "v2.src.lmflow.optim.sophia", "name": "step", "kind": "Function", "parent": "SophiaG", "start_line": 63, "end_line": 117, "docstring": null, "calls": ["no_grad", "zip", "enable_grad", "closure", "append", "append", "append", "append", "append", "add_", "add_", "div", "add_", "addcdiv_", "RuntimeError", "len", "zeros_like", "zeros_like", "keys", "zeros_like", "add", "zeros", "tensor", "ones", "mul_", "sqrt", "item"], "decorators": [null], "num_calls": 27}
{"symbol_id": "v2.src.lmflow.optim.adabound::AdaBound", "path": "v2/src/lmflow/optim/adabound.py", "module": "v2.src.lmflow.optim.adabound", "name": "AdaBound", "kind": "Class", "parent": null, "start_line": 9, "end_line": 158, "docstring": "Implements AdaBound algorithm.\n\nIt has been proposed in `Adaptive Gradient Methods with Dynamic Bound of\nLearning Rate\nhttps://arxiv.org/abs/1902.09843\nNote:\n    Reference code: https://github.com/Luolc/AdaBound", "calls": ["dict", "__init__", "__setstate__", "zip", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "setdefault", "closure", "format", "format", "format", "format", "format", "format", "format", "super", "super", "add_", "addcmul_", "full_like", "mul_", "add_", "RuntimeError", "len", "zeros_like"], "decorators": [], "num_calls": 43}
{"symbol_id": "v2.src.lmflow.optim.adabound::__init__", "path": "v2/src/lmflow/optim/adabound.py", "module": "v2.src.lmflow.optim.adabound", "name": "__init__", "kind": "Function", "parent": "AdaBound", "start_line": 19, "end_line": 62, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.adabound::__setstate__", "path": "v2/src/lmflow/optim/adabound.py", "module": "v2.src.lmflow.optim.adabound", "name": "__setstate__", "kind": "Function", "parent": "AdaBound", "start_line": 64, "end_line": 67, "docstring": null, "calls": ["__setstate__", "setdefault", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adabound::step", "path": "v2/src/lmflow/optim/adabound.py", "module": "v2.src.lmflow.optim.adabound", "name": "step", "kind": "Function", "parent": "AdaBound", "start_line": 69, "end_line": 158, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["zip", "closure", "add_", "addcmul_", "full_like", "mul_", "add_", "RuntimeError", "len", "zeros_like", "zeros_like", "add", "max", "add_", "add_", "zeros_like", "mul_", "mul_", "sqrt", "clamp_", "sqrt", "sqrt", "div_"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.optim.nadam::NAdam", "path": "v2/src/lmflow/optim/nadam.py", "module": "v2.src.lmflow.optim.nadam", "name": "NAdam", "kind": "Class", "parent": null, "start_line": 7, "end_line": 73, "docstring": null, "calls": ["dict", "__init__", "__setstate__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "closure", "format", "format", "format", "format", "format", "format", "super", "super", "addcmul_", "add_", "add_", "addcdiv_", "RuntimeError", "len", "zeros_like", "zeros_like", "add", "mul_", "sqrt", "mul_"], "decorators": [], "num_calls": 31}
{"symbol_id": "v2.src.lmflow.optim.nadam::__init__", "path": "v2/src/lmflow/optim/nadam.py", "module": "v2.src.lmflow.optim.nadam", "name": "__init__", "kind": "Function", "parent": "NAdam", "start_line": 8, "end_line": 22, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.optim.nadam::__setstate__", "path": "v2/src/lmflow/optim/nadam.py", "module": "v2.src.lmflow.optim.nadam", "name": "__setstate__", "kind": "Function", "parent": "NAdam", "start_line": 24, "end_line": 25, "docstring": null, "calls": ["__setstate__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.optim.nadam::step", "path": "v2/src/lmflow/optim/nadam.py", "module": "v2.src.lmflow.optim.nadam", "name": "step", "kind": "Function", "parent": "NAdam", "start_line": 27, "end_line": 73, "docstring": null, "calls": ["closure", "addcmul_", "add_", "add_", "addcdiv_", "RuntimeError", "len", "zeros_like", "zeros_like", "add", "mul_", "sqrt", "mul_", "sqrt"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.optim.muon::zeropower_via_newtonschulz5", "path": "v2/src/lmflow/optim/muon.py", "module": "v2.src.lmflow.optim.muon", "name": "zeropower_via_newtonschulz5", "kind": "Function", "parent": null, "start_line": 10, "end_line": 36, "docstring": "Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\nquintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\nof minimizing steps, it turns out to be empirically effective to keep increasing the slope at\nzero even beyond the point where the iteration no longer converges all the way to one everywhere\non the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\nwhere S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\nperformance at all relative to UV^T, where USV^T = G is the SVD.", "calls": ["bfloat16", "range", "size", "size", "size", "size", "norm"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.optim.muon::Muon", "path": "v2/src/lmflow/optim/muon.py", "module": "v2.src.lmflow.optim.muon", "name": "Muon", "kind": "Class", "parent": null, "start_line": 37, "end_line": 98, "docstring": "Adam optimizer with orthogonalization step.", "calls": ["no_grad", "dict", "__init__", "closure", "super", "add_", "addcmul_", "add_", "add_", "len", "zeros_like", "zeros_like", "zeropower_via_newtonschulz5", "add_", "mul_", "mul_", "sqrt", "sqrt"], "decorators": [], "num_calls": 18}
{"symbol_id": "v2.src.lmflow.optim.muon::__init__", "path": "v2/src/lmflow/optim/muon.py", "module": "v2.src.lmflow.optim.muon", "name": "__init__", "kind": "Function", "parent": "Muon", "start_line": 41, "end_line": 43, "docstring": null, "calls": ["dict", "__init__", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.muon::step", "path": "v2/src/lmflow/optim/muon.py", "module": "v2.src.lmflow.optim.muon", "name": "step", "kind": "Function", "parent": "Muon", "start_line": 46, "end_line": 98, "docstring": "Performs a single optimization step.\n\nArgs:\n    closure (callable, optional): A closure that reevaluates the model\n        and returns the loss.", "calls": ["no_grad", "closure", "add_", "addcmul_", "add_", "add_", "len", "zeros_like", "zeros_like", "zeropower_via_newtonschulz5", "add_", "mul_", "mul_", "sqrt", "sqrt"], "decorators": [null], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.optim.sgdp::SGDP", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "SGDP", "kind": "Class", "parent": null, "start_line": 9, "end_line": 161, "docstring": "Implements SGDP algorithm.\n\nIt has been proposed in `Slowing Down the Weight Norm Increase in\nMomentum-based Optimizers`.\nhttps://arxiv.org/abs/2006.08217\n\nNote:\n    Reference code: https://github.com/clovaai/AdamP", "calls": ["dict", "__init__", "view", "view", "view_func", "view_func", "add_", "add_", "sum", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "size", "_cosine_similarity", "closure", "format", "format", "format", "format", "format", "format", "format", "super", "norm", "norm", "abs"], "decorators": [], "num_calls": 50}
{"symbol_id": "v2.src.lmflow.optim.sgdp::__init__", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "__init__", "kind": "Function", "parent": "SGDP", "start_line": 20, "end_line": 59, "docstring": null, "calls": ["dict", "__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "format", "format", "format", "format", "format", "format", "format", "super"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.optim.sgdp::_channel_view", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "_channel_view", "kind": "Function", "parent": "SGDP", "start_line": 62, "end_line": 63, "docstring": null, "calls": ["view", "size"], "decorators": ["staticmethod"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.optim.sgdp::_layer_view", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "_layer_view", "kind": "Function", "parent": "SGDP", "start_line": 66, "end_line": 67, "docstring": null, "calls": ["view"], "decorators": ["staticmethod"], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.optim.sgdp::_cosine_similarity", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "_cosine_similarity", "kind": "Function", "parent": "SGDP", "start_line": 70, "end_line": 78, "docstring": null, "calls": ["view_func", "view_func", "add_", "add_", "sum", "norm", "norm", "abs"], "decorators": ["staticmethod"], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.optim.sgdp::_projection", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "_projection", "kind": "Function", "parent": "SGDP", "start_line": 80, "end_line": 97, "docstring": null, "calls": ["_cosine_similarity", "max", "len", "sqrt", "add_", "view", "size", "view", "sum", "view_func", "norm", "view_func", "view_func"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.sgdp::step", "path": "v2/src/lmflow/optim/sgdp.py", "module": "v2.src.lmflow.optim.sgdp", "name": "step", "kind": "Function", "parent": "SGDP", "start_line": 99, "end_line": 161, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure: A closure that reevaluates the model and returns the loss.", "calls": ["closure", "add_", "add_", "len", "zeros_like", "len", "_projection", "mul_", "mul_"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.optim.dummy::Dummy", "path": "v2/src/lmflow/optim/dummy.py", "module": "v2.src.lmflow.optim.dummy", "name": "Dummy", "kind": "Class", "parent": null, "start_line": 13, "end_line": 80, "docstring": "An dummy optimizer that does nothing.\n\nParameters:\n    params (:obj:`Iterable[nn.parameter.Parameter]`):\n        Iterable of parameters to optimize or dictionaries defining parameter groups.\n    lr (:obj:`float`, `optional`, defaults to 0):\n        The learning rate to use.", "calls": ["no_grad", "__init__", "ValueError", "ValueError", "ValueError", "closure", "super", "add_", "RuntimeError", "len", "zeros_like", "zeros_like", "add_"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.optim.dummy::__init__", "path": "v2/src/lmflow/optim/dummy.py", "module": "v2.src.lmflow.optim.dummy", "name": "__init__", "kind": "Function", "parent": "Dummy", "start_line": 24, "end_line": 38, "docstring": null, "calls": ["__init__", "ValueError", "ValueError", "ValueError", "super"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.optim.dummy::step", "path": "v2/src/lmflow/optim/dummy.py", "module": "v2.src.lmflow.optim.dummy", "name": "step", "kind": "Function", "parent": "Dummy", "start_line": 42, "end_line": 80, "docstring": "Performs a single optimization step.\n\nArguments:\n    closure (:obj:`Callable`, `optional`): A closure that reevaluates the model and returns the loss.", "calls": ["no_grad", "closure", "add_", "RuntimeError", "len", "zeros_like", "zeros_like", "add_"], "decorators": [null], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.optim.adadelta::Adadelta", "path": "v2/src/lmflow/optim/adadelta.py", "module": "v2.src.lmflow.optim.adadelta", "name": "Adadelta", "kind": "Class", "parent": null, "start_line": 7, "end_line": 43, "docstring": null, "calls": ["dict", "__init__", "closure", "super", "addcmul_", "sqrt_", "mul_", "add_", "addcmul_", "len", "zeros_like", "zeros_like", "mul_", "add", "div_", "mul_", "sqrt_", "add"], "decorators": [], "num_calls": 18}
{"symbol_id": "v2.src.lmflow.optim.adadelta::__init__", "path": "v2/src/lmflow/optim/adadelta.py", "module": "v2.src.lmflow.optim.adadelta", "name": "__init__", "kind": "Function", "parent": "Adadelta", "start_line": 8, "end_line": 10, "docstring": null, "calls": ["dict", "__init__", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.optim.adadelta::step", "path": "v2/src/lmflow/optim/adadelta.py", "module": "v2.src.lmflow.optim.adadelta", "name": "step", "kind": "Function", "parent": "Adadelta", "start_line": 12, "end_line": 43, "docstring": null, "calls": ["closure", "addcmul_", "sqrt_", "mul_", "add_", "addcmul_", "len", "zeros_like", "zeros_like", "mul_", "add", "div_", "mul_", "sqrt_", "add"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::SeparatorStyle", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "SeparatorStyle", "kind": "Class", "parent": null, "start_line": 6, "end_line": 12, "docstring": "Different separator style.", "calls": ["auto", "auto", "auto", "auto", "auto"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::Conversation", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "Conversation", "kind": "Class", "parent": null, "start_line": 16, "end_line": 219, "docstring": "A class that keeps all conversation history.", "calls": ["append", "enumerate", "enumerate", "Conversation", "copy", "copy", "strip", "len", "len", "type", "insert", "insert", "enumerate", "get_images", "replace", "type", "int", "int", "resize", "type", "int", "int", "resize", "BytesIO", "save", "decode", "append", "strip", "append", "type"], "decorators": ["dataclass"], "num_calls": 66}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::get_prompt", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "get_prompt", "kind": "Function", "parent": "Conversation", "start_line": 29, "end_line": 104, "docstring": null, "calls": ["copy", "copy", "strip", "len", "type", "insert", "insert", "enumerate", "replace", "type", "enumerate", "lstrip", "type", "enumerate", "ValueError", "type", "type", "wrap_inst", "wrap_sys", "type"], "decorators": [], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::append_message", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "append_message", "kind": "Function", "parent": "Conversation", "start_line": 106, "end_line": 107, "docstring": null, "calls": ["append"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::get_images", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "get_images", "kind": "Function", "parent": "Conversation", "start_line": 109, "end_line": 156, "docstring": null, "calls": ["enumerate", "type", "int", "int", "resize", "expand2square", "max", "min", "min", "append", "BytesIO", "save", "decode", "append", "resize", "ValueError", "b64encode", "new", "paste", "new", "paste", "getvalue"], "decorators": [], "num_calls": 22}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::to_gradio_chatbot", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "to_gradio_chatbot", "kind": "Function", "parent": "Conversation", "start_line": 158, "end_line": 189, "docstring": null, "calls": ["enumerate", "type", "int", "int", "resize", "BytesIO", "save", "decode", "append", "strip", "append", "max", "min", "min", "len", "append", "b64encode", "replace", "getvalue"], "decorators": [], "num_calls": 19}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::copy", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "copy", "kind": "Function", "parent": "Conversation", "start_line": 191, "end_line": 200, "docstring": null, "calls": ["Conversation"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::dict", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "dict", "kind": "Function", "parent": "Conversation", "start_line": 202, "end_line": 219, "docstring": null, "calls": ["len", "get_images", "type"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.utils.llava_conversation_lib::expand2square", "path": "v2/src/lmflow/utils/llava_conversation_lib.py", "module": "v2.src.lmflow.utils.llava_conversation_lib", "name": "expand2square", "kind": "Function", "parent": null, "start_line": 119, "end_line": 130, "docstring": null, "calls": ["new", "paste", "new", "paste"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.multimodal::update_custom_config", "path": "v2/src/lmflow/utils/multimodal.py", "module": "v2.src.lmflow.utils.multimodal", "name": "update_custom_config", "kind": "Function", "parent": null, "start_line": 7, "end_line": 21, "docstring": null, "calls": ["from_pretrained", "getattr"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.multimodal::load_llava_pretrain_model", "path": "v2/src/lmflow/utils/multimodal.py", "module": "v2.src.lmflow.utils.multimodal", "name": "load_llava_pretrain_model", "kind": "Function", "parent": null, "start_line": 24, "end_line": 36, "docstring": null, "calls": ["glob", "tqdm", "load", "adapt_llava_model_to_lmflow_type", "keys", "keys", "load_state_dict", "state_dict", "print"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.utils.multimodal::adapt_llava_model_to_lmflow_type", "path": "v2/src/lmflow/utils/multimodal.py", "module": "v2.src.lmflow.utils.multimodal", "name": "adapt_llava_model_to_lmflow_type", "kind": "Function", "parent": null, "start_line": 38, "end_line": 50, "docstring": null, "calls": ["items", "replace", "replace", "replace", "replace", "replace"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.utils.model::check_homogeneity", "path": "v2/src/lmflow/utils/model.py", "module": "v2.src.lmflow.utils.model", "name": "check_homogeneity", "kind": "Function", "parent": null, "start_line": 15, "end_line": 25, "docstring": null, "calls": ["all", "len", "from_pretrained", "append", "len", "isinstance", "set"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.utils.data_utils::set_random_seed", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "set_random_seed", "kind": "Function", "parent": null, "start_line": 15, "end_line": 29, "docstring": "Set the random seed for `random`, `numpy`, `torch`, `torch.cuda`.\n\nParameters\n------------\nseed : int\n    The default seed.\n    ", "calls": ["seed", "seed", "manual_seed", "is_available", "manual_seed_all"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.data_utils::load_data", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "load_data", "kind": "Function", "parent": null, "start_line": 31, "end_line": 62, "docstring": "Load data with file name.\n\nParameters\n------------\nfile_name : str.\n    The dataset file name.\n\nReturns\n------------\ninputs : list.\n    The input texts of the dataset.\noutputs : list.\n    The output texts file datasets.    \nlen : int.\n    The length of the dataset.", "calls": ["print", "print", "open", "load", "len", "append", "append", "len"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.utils.data_utils::batchlize", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "batchlize", "kind": "Function", "parent": null, "start_line": 64, "end_line": 94, "docstring": "Convert examples to a dataloader.\n\nParameters\n------------\nexamples : list.\n    Data list.\nbatch_size : int.\n\nrandom_shuffle : bool\n    If true, the dataloader shuffle the training data.\n\nReturns\n------------\ndataloader:\n    Dataloader with batch generator.", "calls": ["len", "shuffle", "append", "append"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.data_utils::preview_file", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "preview_file", "kind": "Function", "parent": null, "start_line": 97, "end_line": 124, "docstring": "Returns the first and last specified number of characters from a file\nwithout loading the entire file into memory, working with any file type.\n\nArgs:\n    file_path (str): Path to the file to be previewed\n    chars (int, optional): Number of characters to show from start and end. Defaults to 100.\n\nReturns:\n    tuple: (first_chars, last_chars) - The first and last characters from the file", "calls": ["getsize", "open", "read", "max", "seek", "seek", "read"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.utils.data_utils::get_dataset_type_fast", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "get_dataset_type_fast", "kind": "Function", "parent": null, "start_line": 127, "end_line": 140, "docstring": "Get the type values from the first and last n lines of a large json dataset.\n    ", "calls": ["compile", "extend", "preview_file", "group", "search"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.data_utils::check_dataset_instances_key_fast", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "check_dataset_instances_key_fast", "kind": "Function", "parent": null, "start_line": 143, "end_line": 152, "docstring": "Check if the dataset instances key matches the instance_key.\n    ", "calls": ["compile", "extend", "preview_file", "search"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.data_utils::answer_extraction", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "answer_extraction", "kind": "Function", "parent": null, "start_line": 155, "end_line": 274, "docstring": "Use this funtion to extract answers from generated text\n\nParameters\n------------\nargs : \n    Arguments.\nresponse : str\n    plain string response.\n\n\nReturns\n------------\nanswer:\n    Decoded answer (such as A, B, C, D, E for mutiple-choice QA).", "calls": ["replace", "len", "findall", "findall", "lower", "sub", "split", "str", "sub", "round", "search", "float", "group", "strip", "lower", "search", "search", "lower", "group", "lower", "search", "search", "group", "lower", "search", "NotImplementedError", "split", "group", "lower", "lower"], "decorators": [], "num_calls": 38}
{"symbol_id": "v2.src.lmflow.utils.data_utils::process_image_flag", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "process_image_flag", "kind": "Function", "parent": null, "start_line": 277, "end_line": 286, "docstring": null, "calls": ["split", "list", "join", "len", "cumsum", "len"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.utils.data_utils::VLLMInferenceResultWithInput", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "VLLMInferenceResultWithInput", "kind": "Class", "parent": null, "start_line": 289, "end_line": 291, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.data_utils::RewardModelInferenceResultWithInput", "path": "v2/src/lmflow/utils/data_utils.py", "module": "v2.src.lmflow.utils.data_utils", "name": "RewardModelInferenceResultWithInput", "kind": "Class", "parent": null, "start_line": 294, "end_line": 296, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.common::make_shell_args_from_dataclass", "path": "v2/src/lmflow/utils/common.py", "module": "v2.src.lmflow.utils.common", "name": "make_shell_args_from_dataclass", "kind": "Function", "parent": null, "start_line": 14, "end_line": 74, "docstring": "Return a string or a list of strings that can be used as shell arguments.\n\nParameters\n----------\ndataclass_objects : List\n    A list of dataclass objects.\nformat : str, optional\n    Return format, can be \"shell\" or \"subprocess\", by default \"subprocess\".\nskip_default : bool, optional\n    Whether to skip attributes with default values, by default True. \n\nReturns\n-------\nUnion[str, List[str]]", "calls": ["isinstance", "items", "join", "items", "ValueError", "isinstance", "extend", "str", "isinstance", "items", "join", "warning", "str"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.utils.common::create_copied_dataclass", "path": "v2/src/lmflow/utils/common.py", "module": "v2.src.lmflow.utils.common", "name": "create_copied_dataclass", "kind": "Function", "parent": null, "start_line": 77, "end_line": 136, "docstring": "Create a copied dataclass with new field names and default values.\n\nParameters\n----------\noriginal_dataclass : dataclass\nfield_prefix : str\n    The prefix to add to the **field** names of the copied dataclass.\nclass_prefix : str\n    The prefix to add to the **class** name of the copied dataclass.\nnew_default : Dict, optional\n    The new default values for the copied dataclass. When None, the \n    default values of the original dataclass are used.\n\nReturns\n-------\ndataclass", "calls": ["fields", "make_dataclass", "append", "get_python_version", "Field", "Field", "get", "get"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.utils.common::remove_dataclass_attr_prefix", "path": "v2/src/lmflow/utils/common.py", "module": "v2.src.lmflow.utils.common", "name": "remove_dataclass_attr_prefix", "kind": "Function", "parent": null, "start_line": 139, "end_line": 159, "docstring": "Remove the prefix from the attribute names of a dataclass instance.\n\nParameters\n----------\ndata_instance : dataclass\nprefix : str\n    The prefix to remove from the attribute names of the dataclass instance.\n\nReturns\n-------\nDict", "calls": ["fields", "getattr", "len"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.utils.common::add_dataclass_attr_prefix", "path": "v2/src/lmflow/utils/common.py", "module": "v2.src.lmflow.utils.common", "name": "add_dataclass_attr_prefix", "kind": "Function", "parent": null, "start_line": 162, "end_line": 182, "docstring": "Add the prefix to the attribute names of a dataclass instance.\n\nParameters\n----------\ndata_instance : dataclass\nprefix : str\n    The prefix to add to the attribute names of the dataclass instance.\n\nReturns\n-------\nDict", "calls": ["fields", "getattr"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.common::print_banner", "path": "v2/src/lmflow/utils/common.py", "module": "v2.src.lmflow.utils.common", "name": "print_banner", "kind": "Function", "parent": null, "start_line": 185, "end_line": 191, "docstring": null, "calls": ["info", "info", "info", "len"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.versioning::get_python_version", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "get_python_version", "kind": "Function", "parent": null, "start_line": 13, "end_line": 14, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.versioning::_is_package_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "_is_package_available", "kind": "Function", "parent": null, "start_line": 17, "end_line": 30, "docstring": null, "calls": ["isinstance", "import_module", "type", "warning"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.versioning::_is_packages_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "_is_packages_available", "kind": "Function", "parent": null, "start_line": 33, "end_line": 39, "docstring": null, "calls": ["isinstance", "all", "isinstance", "all", "ValueError", "_is_package_available", "_is_package_available", "type"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_package_version_at_least", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_package_version_at_least", "kind": "Function", "parent": null, "start_line": 42, "end_line": 50, "docstring": null, "calls": ["get_distribution", "parse_version", "parse_version"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_gradio_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_gradio_available", "kind": "Function", "parent": null, "start_line": 53, "end_line": 54, "docstring": null, "calls": ["_is_package_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_ray_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_ray_available", "kind": "Function", "parent": null, "start_line": 57, "end_line": 58, "docstring": null, "calls": ["_is_package_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_vllm_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_vllm_available", "kind": "Function", "parent": null, "start_line": 61, "end_line": 62, "docstring": null, "calls": ["_is_package_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_flash_attn_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_flash_attn_available", "kind": "Function", "parent": null, "start_line": 65, "end_line": 66, "docstring": null, "calls": ["_is_package_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_flask_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_flask_available", "kind": "Function", "parent": null, "start_line": 69, "end_line": 70, "docstring": null, "calls": ["_is_packages_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_trl_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_trl_available", "kind": "Function", "parent": null, "start_line": 73, "end_line": 74, "docstring": null, "calls": ["_is_package_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.versioning::is_multimodal_available", "path": "v2/src/lmflow/utils/versioning.py", "module": "v2.src.lmflow.utils.versioning", "name": "is_multimodal_available", "kind": "Function", "parent": null, "start_line": 77, "end_line": 78, "docstring": null, "calls": ["_is_packages_available"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.llama::Llama2ConversationTemplate", "path": "v2/src/lmflow/utils/conversation_template/llama.py", "module": "v2.src.lmflow.utils.conversation_template.llama", "name": "Llama2ConversationTemplate", "kind": "Class", "parent": null, "start_line": 15, "end_line": 49, "docstring": null, "calls": ["join", "range", "warning", "format", "len", "format", "format", "_encode_template", "_encode_template", "append"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.llama::Llama2ConversationTemplateForTool", "path": "v2/src/lmflow/utils/conversation_template/llama.py", "module": "v2.src.lmflow.utils.conversation_template.llama", "name": "Llama2ConversationTemplateForTool", "kind": "Class", "parent": null, "start_line": 51, "end_line": 100, "docstring": null, "calls": ["join", "range", "format", "len", "append", "format", "_encode_template", "append", "tuple", "format", "_encode_template", "append", "format", "_encode_template", "append", "format", "_encode_template", "append", "append", "tuple"], "decorators": [], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.llama::_encode", "path": "v2/src/lmflow/utils/conversation_template/llama.py", "module": "v2.src.lmflow.utils.conversation_template.llama", "name": "_encode", "kind": "Function", "parent": "Llama2ConversationTemplate", "start_line": 16, "end_line": 49, "docstring": null, "calls": ["join", "range", "warning", "format", "len", "format", "format", "_encode_template", "_encode_template", "append"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.llama::_encode", "path": "v2/src/lmflow/utils/conversation_template/llama.py", "module": "v2.src.lmflow.utils.conversation_template.llama", "name": "_encode", "kind": "Function", "parent": "Llama2ConversationTemplateForTool", "start_line": 52, "end_line": 100, "docstring": null, "calls": ["join", "range", "format", "len", "append", "format", "_encode_template", "append", "tuple", "format", "_encode_template", "append", "format", "_encode_template", "append", "format", "_encode_template", "append", "append", "tuple"], "decorators": [], "num_calls": 20}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.zephyr::ZephyrConversationTemplate", "path": "v2/src/lmflow/utils/conversation_template/zephyr.py", "module": "v2.src.lmflow.utils.conversation_template.zephyr", "name": "ZephyrConversationTemplate", "kind": "Class", "parent": null, "start_line": 15, "end_line": 50, "docstring": null, "calls": ["_encode_template", "range", "format", "len", "format", "format", "_encode_template", "_encode_template", "append", "replace"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.zephyr::_encode", "path": "v2/src/lmflow/utils/conversation_template/zephyr.py", "module": "v2.src.lmflow.utils.conversation_template.zephyr", "name": "_encode", "kind": "Function", "parent": "ZephyrConversationTemplate", "start_line": 16, "end_line": 50, "docstring": null, "calls": ["_encode_template", "range", "format", "len", "format", "format", "_encode_template", "_encode_template", "append", "replace"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.gemma::GemmaConversationTemplate", "path": "v2/src/lmflow/utils/conversation_template/gemma.py", "module": "v2.src.lmflow.utils.conversation_template.gemma", "name": "GemmaConversationTemplate", "kind": "Class", "parent": null, "start_line": 14, "end_line": 24, "docstring": null, "calls": ["get", "encode_conversation", "warning", "super"], "decorators": ["dataclass"], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.gemma::encode_conversation", "path": "v2/src/lmflow/utils/conversation_template/gemma.py", "module": "v2.src.lmflow.utils.conversation_template.gemma", "name": "encode_conversation", "kind": "Function", "parent": "GemmaConversationTemplate", "start_line": 15, "end_line": 24, "docstring": null, "calls": ["get", "encode_conversation", "warning", "super"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::TemplateComponent", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "TemplateComponent", "kind": "Class", "parent": null, "start_line": 18, "end_line": 75, "docstring": "The minimal unit of a template, which can be a token, a string, or a list of tools.\n\nParameters\n----------\ntype : Literal['token', 'token_id', 'string', 'tools']\n    - Type of the component.  \n    \n    - When the component is a token or a string, the content should be `string`. \n    The difference between the two is that token will be converted to token ids \n    by the tokenizer.convert_tokens_to_ids() method, while string will be directly \n    encoded by the tokenizer.encode() method. Specially, since the bos token and eos\n    token are frequently used across different templates, we provide the convenience\n    to use `'bos_token'` and `'eos_token'` to represent the actual bos and eos tokens when\n    `type` of the `TemplateComponent` is `token`. For example:  \n    \n    ```python\n    TemplateComponent(type='token', content='bos_token')\n    ```\n    \n    After encoding, the content will be replaced by the actual token id of the bos token.\n    Please do remember that if you set the `type` to `string`, the tokenizer will try to \n    encode the string 'bos_token' instead of providing the actual bos token.\n    \n    - When the component is token_id, the content should be `int` or `List[int]`, and \n    will be directly appended to the encoded token ids.\n    \n    - Tools are not supported yet.\n    \ncontent : Union[str, int, List[str], List[int]]\n    Content of the component.", "calls": ["replace", "replace", "isinstance", "isinstance", "type", "ValueError", "type", "isinstance", "all", "isinstance"], "decorators": ["dataclass"], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::Formatter", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "Formatter", "kind": "Class", "parent": null, "start_line": 79, "end_line": 92, "docstring": null, "calls": ["field", "search"], "decorators": ["dataclass"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::EmptyFormatter", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "EmptyFormatter", "kind": "Class", "parent": null, "start_line": 96, "end_line": 110, "docstring": null, "calls": ["has_placeholder", "ValueError"], "decorators": ["dataclass"], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::StringFormatter", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "StringFormatter", "kind": "Class", "parent": null, "start_line": 114, "end_line": 147, "docstring": null, "calls": ["debug", "has_placeholder", "ValueError", "items", "append", "replace", "append", "len", "warning", "TemplateComponent"], "decorators": ["dataclass"], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::ConversationTemplate", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "ConversationTemplate", "kind": "Class", "parent": null, "start_line": 151, "end_line": 402, "docstring": null, "calls": ["isinstance", "_encode", "post_process_pairs", "_encode_template", "range", "isinstance", "warning", "replace", "format", "len", "format", "format", "_encode_template", "_encode_template", "append", "remove_last_separator", "add_special_starter", "add_special_stopper", "encode", "len", "len", "ValueError", "encode", "encode", "isinstance", "NotImplementedError", "NotImplementedError", "format", "_ensure_id_list", "ValueError"], "decorators": ["dataclass"], "num_calls": 52}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::ConversationTemplateForTool", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "ConversationTemplateForTool", "kind": "Class", "parent": null, "start_line": 405, "end_line": 548, "docstring": null, "calls": ["isinstance", "_handle_tools", "_encode", "post_process_pairs", "_encode_template", "range", "replace", "format", "len", "append", "join", "format", "format", "_encode_template", "append", "tuple", "ValueError", "format", "_encode_template", "append", "len", "warning", "encode", "format", "_encode_template", "append", "_ensure_id_list", "NotImplementedError", "format", "_encode_template"], "decorators": ["dataclass"], "num_calls": 35}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::__post_init__", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "__post_init__", "kind": "Function", "parent": "TemplateComponent", "start_line": 55, "end_line": 69, "docstring": null, "calls": ["isinstance", "isinstance", "type", "ValueError", "type", "isinstance", "all", "isinstance"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::__repr__", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "__repr__", "kind": "Function", "parent": "TemplateComponent", "start_line": 71, "end_line": 72, "docstring": null, "calls": ["replace"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::__str__", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "__str__", "kind": "Function", "parent": "TemplateComponent", "start_line": 74, "end_line": 75, "docstring": null, "calls": ["replace"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::format", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "format", "kind": "Function", "parent": "Formatter", "start_line": 83, "end_line": 83, "docstring": null, "calls": [], "decorators": ["abstractmethod"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::has_placeholder", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "has_placeholder", "kind": "Function", "parent": "Formatter", "start_line": 85, "end_line": 92, "docstring": null, "calls": ["search"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::__post_init__", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "__post_init__", "kind": "Function", "parent": "EmptyFormatter", "start_line": 97, "end_line": 99, "docstring": null, "calls": ["has_placeholder", "ValueError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::format", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "format", "kind": "Function", "parent": "EmptyFormatter", "start_line": 101, "end_line": 110, "docstring": "Empty formatter for when no formatting is needed.\nThis is useful when user has already applied formatting to the dataset.\n\nReturns\n-------\nlist\n    Original template.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::__post_init__", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "__post_init__", "kind": "Function", "parent": "StringFormatter", "start_line": 115, "end_line": 117, "docstring": null, "calls": ["has_placeholder", "ValueError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::format", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "format", "kind": "Function", "parent": "StringFormatter", "start_line": 119, "end_line": 147, "docstring": "Format the string components with the provided keyword arguments. \nMostly used for formatting system prompt, user and assistant messages.\n\nParameters\n----------\n**kwargs : dict\n    Keyword arguments containing values to replace in the template components.\n\nReturns\n-------\nlist\n    Formatted template.", "calls": ["debug", "items", "append", "replace", "append", "len", "warning", "TemplateComponent"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::__post_init__", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "__post_init__", "kind": "Function", "parent": "ConversationTemplate", "start_line": 166, "end_line": 173, "docstring": null, "calls": ["NotImplementedError", "NotImplementedError"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::encode_conversation", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "encode_conversation", "kind": "Function", "parent": "ConversationTemplate", "start_line": 175, "end_line": 220, "docstring": "Messages here should be guaranteed to be in pairs, with the first message being the user message and the second message being the system message.\nData example: \n```json\n{\n    \"conversation_id\": 2,\n    \"system\": \"sysinfo1\",\n    \"tools\": [\"tool_1_desc\"],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"hi\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Hello!\"\n        }\n    ]\n}\n```", "calls": ["isinstance", "_encode", "post_process_pairs", "warning", "replace", "ValueError"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::_encode", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "_encode", "kind": "Function", "parent": "ConversationTemplate", "start_line": 222, "end_line": 256, "docstring": null, "calls": ["_encode_template", "range", "format", "len", "format", "format", "_encode_template", "_encode_template", "append", "format"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::_encode_template", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "_encode_template", "kind": "Function", "parent": "ConversationTemplate", "start_line": 258, "end_line": 297, "docstring": "Encode template components into token ids.\n\nParameters\n----------\ntemplate : List[TemplateComponent]\n    Formatted template components.\ntokenizer : PreTrainedTokenizer\n    Tokenizer to convert tokens into token ids.\n\nReturns\n-------\nList[int]\n    Encoded token ids.", "calls": ["len", "warning", "encode", "_ensure_id_list", "NotImplementedError", "_ensure_id_list", "convert_tokens_to_ids"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::post_process_pairs", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "post_process_pairs", "kind": "Function", "parent": "ConversationTemplate", "start_line": 299, "end_line": 321, "docstring": null, "calls": ["remove_last_separator", "add_special_starter", "add_special_stopper"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::remove_last_separator", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "remove_last_separator", "kind": "Function", "parent": "ConversationTemplate", "start_line": 323, "end_line": 346, "docstring": null, "calls": ["encode", "len", "len", "ValueError", "_ensure_id_list", "convert_tokens_to_ids", "_ensure_id_list", "ValueError", "len", "len"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::add_special_starter", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "add_special_starter", "kind": "Function", "parent": "ConversationTemplate", "start_line": 348, "end_line": 369, "docstring": null, "calls": ["encode", "_ensure_id_list", "ValueError", "_ensure_id_list", "convert_tokens_to_ids"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::add_special_stopper", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "add_special_stopper", "kind": "Function", "parent": "ConversationTemplate", "start_line": 371, "end_line": 392, "docstring": null, "calls": ["encode", "_ensure_id_list", "ValueError", "_ensure_id_list", "convert_tokens_to_ids"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::_ensure_id_list", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "_ensure_id_list", "kind": "Function", "parent": "ConversationTemplate", "start_line": 394, "end_line": 402, "docstring": "Make sure the object is a list of integers. Useful for handling token ids.\n        ", "calls": ["isinstance", "isinstance", "ValueError", "type"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::encode_conversation", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "encode_conversation", "kind": "Function", "parent": "ConversationTemplateForTool", "start_line": 406, "end_line": 451, "docstring": "Messages here should be guaranteed to be in pairs, with the first message being the user message and the second message being the system message.\nData example: \n```json\n{\n    \"conversation_id\": 2,\n    \"system\": \"sysinfo1\",\n    \"tools\": [\"tool_1_desc\"],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"hi\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Hello!\"\n        }\n    ]\n}\n```", "calls": ["isinstance", "_handle_tools", "_encode", "post_process_pairs", "replace", "ValueError"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::_encode", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "_encode", "kind": "Function", "parent": "ConversationTemplateForTool", "start_line": 453, "end_line": 503, "docstring": null, "calls": ["_encode_template", "range", "format", "len", "append", "format", "format", "_encode_template", "append", "tuple", "format", "_encode_template", "append", "format", "_encode_template", "append", "format", "_encode_template", "append", "append", "tuple"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::_encode_template", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "_encode_template", "kind": "Function", "parent": "ConversationTemplateForTool", "start_line": 505, "end_line": 544, "docstring": "Encode template components into token ids.\n\nParameters\n----------\ntemplate : List[TemplateComponent]\n    Formatted template components.\ntokenizer : PreTrainedTokenizer\n    Tokenizer to convert tokens into token ids.\n\nReturns\n-------\nList[int]\n    Encoded token ids.", "calls": ["len", "warning", "encode", "_ensure_id_list", "NotImplementedError", "_ensure_id_list", "convert_tokens_to_ids"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.base::_handle_tools", "path": "v2/src/lmflow/utils/conversation_template/base.py", "module": "v2.src.lmflow.utils.conversation_template.base", "name": "_handle_tools", "kind": "Function", "parent": "ConversationTemplateForTool", "start_line": 546, "end_line": 548, "docstring": null, "calls": ["join"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.hymba::HymbaConversationTemplate", "path": "v2/src/lmflow/utils/conversation_template/hymba.py", "module": "v2.src.lmflow.utils.conversation_template.hymba", "name": "HymbaConversationTemplate", "kind": "Class", "parent": null, "start_line": 39, "end_line": 45, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.conversation_template.hymba::_handle_tools", "path": "v2/src/lmflow/utils/conversation_template/hymba.py", "module": "v2.src.lmflow.utils.conversation_template.hymba", "name": "_handle_tools", "kind": "Function", "parent": "HymbaConversationTemplate", "start_line": 40, "end_line": 45, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch::CondenseRotaryEmbedding", "path": "v2/src/lmflow/utils/position_interpolation/llama_rope_scaled_monkey_patch.py", "module": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch", "name": "CondenseRotaryEmbedding", "kind": "Class", "parent": null, "start_line": 7, "end_line": 46, "docstring": null, "calls": ["__init__", "register_buffer", "einsum", "cat", "get_default_dtype", "register_buffer", "register_buffer", "arange", "to", "to", "einsum", "to", "register_buffer", "register_buffer", "to", "to", "super", "arange", "to", "to", "to", "cat", "cos", "sin", "float", "cos", "sin", "arange"], "decorators": [], "num_calls": 28}
{"symbol_id": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch::replace_llama_with_condense", "path": "v2/src/lmflow/utils/position_interpolation/llama_rope_scaled_monkey_patch.py", "module": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch", "name": "replace_llama_with_condense", "kind": "Function", "parent": null, "start_line": 48, "end_line": 49, "docstring": null, "calls": ["partial"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch::__init__", "path": "v2/src/lmflow/utils/position_interpolation/llama_rope_scaled_monkey_patch.py", "module": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch", "name": "__init__", "kind": "Function", "parent": "CondenseRotaryEmbedding", "start_line": 8, "end_line": 28, "docstring": null, "calls": ["__init__", "register_buffer", "einsum", "cat", "get_default_dtype", "register_buffer", "register_buffer", "arange", "to", "to", "super", "to", "cos", "sin", "float", "arange"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch::forward", "path": "v2/src/lmflow/utils/position_interpolation/llama_rope_scaled_monkey_patch.py", "module": "v2.src.lmflow.utils.position_interpolation.llama_rope_scaled_monkey_patch", "name": "forward", "kind": "Function", "parent": "CondenseRotaryEmbedding", "start_line": 30, "end_line": 46, "docstring": null, "calls": ["einsum", "to", "register_buffer", "register_buffer", "to", "to", "arange", "to", "to", "cat", "cos", "sin"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.gpt2_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/gpt2_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.gpt2_flash_attention", "name": "forward", "kind": "Function", "parent": null, "start_line": 20, "end_line": 120, "docstring": null, "calls": ["size", "_split_heads", "_split_heads", "_split_heads", "stack", "transpose", "rearrange", "_merge_heads", "c_proj", "resid_dropout", "q_attn", "split", "split", "cat", "cat", "rearrange", "arange", "flash_attn_unpadded_qkvpacked_func", "rearrange", "clone", "squeeze", "rearrange", "unpad_input", "rearrange", "flash_attn_unpadded_qkvpacked_func", "rearrange", "hasattr", "ValueError", "pad_input", "c_attn"], "decorators": [], "num_calls": 33}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.gpt2_flash_attention::_prepare_decoder_attention_mask", "path": "v2/src/lmflow/utils/flash_attention/gpt2_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.gpt2_flash_attention", "name": "_prepare_decoder_attention_mask", "kind": "Function", "parent": null, "start_line": 125, "end_line": 129, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.gpt2_flash_attention::replace_gpt2_attn_with_flash_attn", "path": "v2/src/lmflow/utils/flash_attention/gpt2_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.gpt2_flash_attention", "name": "replace_gpt2_attn_with_flash_attn", "kind": "Function", "parent": null, "start_line": 132, "end_line": 136, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.gpt_neo_flash_attention::_attn", "path": "v2/src/lmflow/utils/flash_attention/gpt_neo_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.gpt_neo_flash_attention", "name": "_attn", "kind": "Function", "parent": null, "start_line": 15, "end_line": 57, "docstring": null, "calls": ["to", "to", "stack", "transpose", "where", "sqrt", "rearrange", "rearrange", "arange", "flash_attn_unpadded_qkvpacked_func", "rearrange", "rearrange", "unpad_input", "rearrange", "flash_attn_unpadded_qkvpacked_func", "rearrange", "tensor", "pad_input", "rearrange"], "decorators": [], "num_calls": 19}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.gpt_neo_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/gpt_neo_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.gpt_neo_flash_attention", "name": "forward", "kind": "Function", "parent": null, "start_line": 59, "end_line": 96, "docstring": null, "calls": ["q_proj", "k_proj", "v_proj", "_split_heads", "_split_heads", "_split_heads", "_attn", "view", "out_proj", "resid_dropout", "cat", "cat", "size"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.gpt_neo_flash_attention::replace_gpt_neo_attn_with_flash_attn", "path": "v2/src/lmflow/utils/flash_attention/gpt_neo_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.gpt_neo_flash_attention", "name": "replace_gpt_neo_attn_with_flash_attn", "kind": "Function", "parent": null, "start_line": 98, "end_line": 100, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.bloom_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/bloom_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.bloom_flash_attention", "name": "forward", "kind": "Function", "parent": null, "start_line": 14, "end_line": 87, "docstring": null, "calls": ["query_key_value", "_split_heads", "rearrange", "bool", "masked_fill", "concat", "flash_attn_qkvpacked_func", "rearrange", "_merge_heads", "dropout_add", "cat", "cat", "zeros_like", "range", "dense", "unsqueeze", "unsqueeze", "unsqueeze", "linear", "int", "int", "int", "int"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.bloom_flash_attention::_prepare_attn_mask", "path": "v2/src/lmflow/utils/flash_attention/bloom_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.bloom_flash_attention", "name": "_prepare_attn_mask", "kind": "Function", "parent": null, "start_line": 92, "end_line": 96, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.bloom_flash_attention::replace_bloom_attn_with_flash_attn", "path": "v2/src/lmflow/utils/flash_attention/bloom_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.bloom_flash_attention", "name": "replace_bloom_attn_with_flash_attn", "kind": "Function", "parent": null, "start_line": 98, "end_line": 102, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_fwd_kernel", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_fwd_kernel", "kind": "Function", "parent": null, "start_line": 67, "end_line": 230, "docstring": null, "calls": ["heuristics", "program_id", "program_id", "arange", "arange", "zeros", "range", "exp", "store", "load", "program_id", "store", "arange", "arange", "zeros", "float", "zeros", "float", "minimum", "multiple_of", "zeros", "dot", "sum", "exp", "store", "load", "to", "dot", "arange", "load"], "decorators": [null, "jit"], "num_calls": 65}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_bwd_preprocess_do_o_dot", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_bwd_preprocess_do_o_dot", "kind": "Function", "parent": null, "start_line": 234, "end_line": 255, "docstring": null, "calls": ["program_id", "program_id", "arange", "to", "to", "sum", "store", "arange", "load", "load"], "decorators": ["jit"], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_bwd_store_dk_dv", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_bwd_store_dk_dv", "kind": "Function", "parent": null, "start_line": 259, "end_line": 278, "docstring": null, "calls": ["store", "store", "store", "store", "store", "store", "store", "store"], "decorators": ["jit"], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_bwd_kernel_one_col_block", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_bwd_kernel_one_col_block", "kind": "Function", "parent": null, "start_line": 282, "end_line": 479, "docstring": null, "calls": ["arange", "arange", "zeros", "zeros", "cdiv", "range", "_bwd_store_dk_dv", "arange", "arange", "_bwd_store_dk_dv", "multiple_of", "dot", "load", "dot", "dot", "load", "to", "dot", "load", "load", "load", "load", "load", "load", "load", "load", "load", "where", "where", "debug_barrier"], "decorators": ["jit"], "num_calls": 64}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::init_to_zero", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "init_to_zero", "kind": "Function", "parent": null, "start_line": 482, "end_line": 483, "docstring": null, "calls": ["zero_"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_bwd_kernel", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_bwd_kernel", "kind": "Function", "parent": null, "start_line": 507, "end_line": 581, "docstring": null, "calls": ["autotune", "heuristics", "program_id", "cdiv", "range", "program_id", "_bwd_kernel_one_col_block", "_bwd_kernel_one_col_block", "Config", "Config", "init_to_zero", "init_to_zero"], "decorators": [null, null, "jit"], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_flash_attn_forward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_flash_attn_forward", "kind": "Function", "parent": null, "start_line": 584, "end_line": 641, "docstring": null, "calls": ["empty", "empty", "empty_like", "max", "expand", "ceil", "next_power_of_2", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "sqrt", "dim", "stride", "contiguous", "stride", "stride", "stride", "cdiv", "RuntimeError"], "decorators": [], "num_calls": 28}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::_flash_attn_backward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "_flash_attn_backward", "kind": "Function", "parent": null, "start_line": 644, "end_line": 717, "docstring": null, "calls": ["empty_like", "empty_like", "max", "copy_", "stride", "contiguous", "ceil", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "next_power_of_2", "stride", "stride", "stride", "stride", "stride", "stride", "expand", "stride", "stride", "stride", "stride", "stride", "stride", "stride", "stride"], "decorators": [], "num_calls": 52}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::FlashAttnQKVPackedFunc", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "FlashAttnQKVPackedFunc", "kind": "Class", "parent": null, "start_line": 720, "end_line": 752, "docstring": null, "calls": ["_flash_attn_forward", "save_for_backward", "stride", "contiguous", "inference_mode", "empty_like", "_flash_attn_backward"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::FlashAttnKVPackedFunc", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "FlashAttnKVPackedFunc", "kind": "Class", "parent": null, "start_line": 758, "end_line": 791, "docstring": null, "calls": ["_flash_attn_forward", "save_for_backward", "len", "inference_mode", "empty_like", "empty_like", "_flash_attn_backward", "contiguous", "stride"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::FlashAttnFunc", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "FlashAttnFunc", "kind": "Class", "parent": null, "start_line": 797, "end_line": 829, "docstring": null, "calls": ["_flash_attn_forward", "save_for_backward", "inference_mode", "empty_like", "empty_like", "empty_like", "_flash_attn_backward", "contiguous", "stride"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "forward", "kind": "Function", "parent": "FlashAttnQKVPackedFunc", "start_line": 723, "end_line": 739, "docstring": "qkv: (batch, seqlen, 3, nheads, headdim)\nbias: optional, shape broadcastible to (batch, nheads, seqlen, seqlen).\n    For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen).\n    ALiBi mask for non-causal would have shape (1, nheads, seqlen, seqlen)", "calls": ["_flash_attn_forward", "save_for_backward", "stride", "contiguous"], "decorators": ["staticmethod"], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::backward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "backward", "kind": "Function", "parent": "FlashAttnQKVPackedFunc", "start_line": 742, "end_line": 752, "docstring": null, "calls": ["inference_mode", "empty_like", "_flash_attn_backward"], "decorators": ["staticmethod"], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "forward", "kind": "Function", "parent": "FlashAttnKVPackedFunc", "start_line": 761, "end_line": 776, "docstring": "q: (batch, seqlen_q, nheads, headdim)\nkv: (batch, seqlen_k, 2, nheads, headdim)\nbias: optional, shape broadcastible to (batch, nheads, seqlen_q, seqlen_k).\n    For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen_k).\n    ALiBi mask for non-causal would have shape (1, nheads, seqlen_q, seqlen_k)", "calls": ["_flash_attn_forward", "save_for_backward", "contiguous", "stride"], "decorators": ["staticmethod"], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::backward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "backward", "kind": "Function", "parent": "FlashAttnKVPackedFunc", "start_line": 779, "end_line": 791, "docstring": null, "calls": ["len", "inference_mode", "empty_like", "empty_like", "_flash_attn_backward"], "decorators": ["staticmethod"], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "forward", "kind": "Function", "parent": "FlashAttnFunc", "start_line": 800, "end_line": 815, "docstring": "q: (batch_size, seqlen_q, nheads, headdim)\nk, v: (batch_size, seqlen_k, nheads, headdim)\nbias: optional, shape broadcastible to (batch, nheads, seqlen_q, seqlen_k).\n    For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen_k).\n    ALiBi mask for non-causal would have shape (1, nheads, seqlen_q, seqlen_k)", "calls": ["_flash_attn_forward", "save_for_backward", "contiguous", "stride"], "decorators": ["staticmethod"], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.triton_flash_attention::backward", "path": "v2/src/lmflow/utils/flash_attention/triton_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.triton_flash_attention", "name": "backward", "kind": "Function", "parent": "FlashAttnFunc", "start_line": 818, "end_line": 829, "docstring": null, "calls": ["inference_mode", "empty_like", "empty_like", "empty_like", "_flash_attn_backward"], "decorators": ["staticmethod"], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.llama_flash_attention::forward", "path": "v2/src/lmflow/utils/flash_attention/llama_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.llama_flash_attention", "name": "forward", "kind": "Function", "parent": null, "start_line": 21, "end_line": 88, "docstring": null, "calls": ["size", "transpose", "transpose", "transpose", "rotary_emb", "apply_rotary_pos_emb", "flash_attn_func", "reshape", "o_proj", "ValueError", "q_proj", "k_proj", "v_proj", "cat", "cat", "rearrange", "hasattr", "to", "to", "to", "size", "ValueError", "NotImplementedError", "view", "view", "view", "size"], "decorators": [], "num_calls": 27}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.llama_flash_attention::_prepare_decoder_attention_mask", "path": "v2/src/lmflow/utils/flash_attention/llama_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.llama_flash_attention", "name": "_prepare_decoder_attention_mask", "kind": "Function", "parent": null, "start_line": 93, "end_line": 119, "docstring": null, "calls": ["_make_causal_mask", "to", "_expand_mask"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.utils.flash_attention.llama_flash_attention::replace_llama_attn_with_flash_attn", "path": "v2/src/lmflow/utils/flash_attention/llama_flash_attention.py", "module": "v2.src.lmflow.utils.flash_attention.llama_flash_attention", "name": "replace_llama_attn_with_flash_attn", "kind": "Function", "parent": null, "start_line": 122, "end_line": 124, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::Timer", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "Timer", "kind": "Class", "parent": null, "start_line": 5, "end_line": 29, "docstring": null, "calls": ["time", "_to_readable", "pprint", "items", "time", "strftime", "round", "strftime", "localtime", "localtime"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::__init__", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "__init__", "kind": "Function", "parent": "Timer", "start_line": 6, "end_line": 9, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::start", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "start", "kind": "Function", "parent": "Timer", "start_line": 11, "end_line": 12, "docstring": null, "calls": ["time"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::end", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "end", "kind": "Function", "parent": "Timer", "start_line": 14, "end_line": 16, "docstring": null, "calls": ["time"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::get_runtime", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "get_runtime", "kind": "Function", "parent": "Timer", "start_line": 18, "end_line": 19, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::show", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "show", "kind": "Function", "parent": "Timer", "start_line": 21, "end_line": 23, "docstring": null, "calls": ["_to_readable", "pprint"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.utils.debug.profiler::_to_readable", "path": "v2/src/lmflow/utils/debug/profiler.py", "module": "v2.src.lmflow.utils.debug.profiler", "name": "_to_readable", "kind": "Function", "parent": "Timer", "start_line": 25, "end_line": 29, "docstring": null, "calls": ["items", "strftime", "round", "strftime", "localtime", "localtime"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::HFEncoderDecoderModel", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "HFEncoderDecoderModel", "kind": "Class", "parent": null, "start_line": 63, "end_line": 522, "docstring": "Initializes a HFEncoderDecoderModel instance.\n\nParameters\n------------\n\nmodel_args :\n    Model arguments such as model name, path, revision, etc.\n\ntune_strategy : str or none,  default=\"normal\".\n    A string representing the dataset backend. Defaults to \"huggingface\".\n\nds_config :\n    Deepspeed configuations.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.", "calls": ["NotImplementedError", "isinstance", "isinstance", "save_pretrained", "NotImplementedError", "update", "isinstance", "tensor", "dim", "batch_decode", "decode", "update", "no_grad", "merge_and_unload", "warning", "save_pretrained", "save_pretrained", "HfTrainerDeepSpeedConfig", "tokenizer", "tokenizer", "isinstance", "deepcopy", "pop", "update", "get_tokenizer", "NotImplementedError", "trainer_config_process", "warning", "from_pretrained", "from_pretrained"], "decorators": [], "num_calls": 76}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::__init__", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "__init__", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 86, "end_line": 295, "docstring": "Initializes a HFDecoderModel instance.\n:param model_args: dictionary with model arguments such as model name, path, revision, etc.\n:param tune_strategy: tuning strategy: normal, none, lora or adapter\n:param ds_config: deepspeed configuration for distributed training", "calls": ["NotImplementedError", "HfTrainerDeepSpeedConfig", "NotImplementedError", "trainer_config_process", "warning", "from_pretrained", "from_pretrained", "NotImplementedError", "from_pretrained", "from_pretrained", "language_model_from_pretrained", "load", "load_state_dict", "from_pretrained", "load_llava_pretrain_model", "init_distributed", "eval", "from_pretrained", "from_pretrained", "dict", "dict", "from_pretrained", "from_pretrained", "update_custom_config", "CustomAutoVision2SeqModel", "save_pretrained", "from_pretrained", "initialize", "from_pretrained", "warning"], "decorators": [], "num_calls": 36}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::tokenize", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "tokenize", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 297, "end_line": 317, "docstring": "Tokenize the full dataset.\n\nParameters\n------------\ndataset :\n    Text dataset.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n------------\ntokenized_datasets :\n    The tokenized dataset.", "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::encode", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "encode", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 319, "end_line": 363, "docstring": "Perform encoding process of the tokenizer.\n\nParameters\n------------\ninputs : str or list.\n    The text sequence.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n------------\noutputs :\n    The tokenized inputs.", "calls": ["isinstance", "update", "isinstance", "tokenizer", "tokenizer", "isinstance", "getattr", "tokenizer", "encode", "NotImplementedError", "getattr", "pop", "tokenizer", "print", "preprocess", "type"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::decode", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "decode", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 366, "end_line": 392, "docstring": "Perform decoding process of the tokenizer.\n\nParameters\n------------\ninputs : list.\n    The token sequence.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n------------\noutputs :\n    The text decoded from the token inputs.", "calls": ["isinstance", "tensor", "dim", "batch_decode", "decode"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::inference", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "inference", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 395, "end_line": 462, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ninputs :\n    The sequence used as a prompt for the generation or as model inputs to the model.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n------------\noutputs :\n    The generated sequence output", "calls": ["update", "no_grad", "deepcopy", "pop", "update", "getattr", "generate", "generate", "generate", "NotImplementedError"], "decorators": [], "num_calls": 10}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::merge_lora_weights", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "merge_lora_weights", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 465, "end_line": 469, "docstring": null, "calls": ["merge_and_unload", "warning", "get_backend_model"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::save", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "save", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 472, "end_line": 496, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ndir :\n    The directory to save model and tokenizer\n\nsave_full_model : Optional.\n    Whether to save full model.\n\nkwargs : Optional.\n    Keyword arguments.\n\nReturns\n------------\noutputs :\n    The generated sequence output", "calls": ["save_pretrained", "save_pretrained", "save_pretrained", "get_tokenizer", "get_backend_model"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::get_max_length", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "get_max_length", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 499, "end_line": 508, "docstring": "Return max acceptable input length in terms of tokens.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::get_tokenizer", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "get_tokenizer", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 511, "end_line": 515, "docstring": "Return the tokenizer of the model.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.hf_encoder_decoder_model::get_backend_model", "path": "v2/src/lmflow/models/hf_encoder_decoder_model.py", "module": "v2.src.lmflow.models.hf_encoder_decoder_model", "name": "get_backend_model", "kind": "Function", "parent": "HFEncoderDecoderModel", "start_line": 518, "end_line": 522, "docstring": "Return the backend model.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.text_regression_model::TextRegressionModel", "path": "v2/src/lmflow/models/text_regression_model.py", "module": "v2.src.lmflow.models.text_regression_model", "name": "TextRegressionModel", "kind": "Class", "parent": null, "start_line": 11, "end_line": 57, "docstring": "Initializes a TextRegressionModel instance.\n\nParameters\n------------\n\nmodel_args : \n    Model arguments such as model name, path, revision, etc.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    ", "calls": ["inference_func"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.text_regression_model::__init__", "path": "v2/src/lmflow/models/text_regression_model.py", "module": "v2.src.lmflow.models.text_regression_model", "name": "__init__", "kind": "Function", "parent": "TextRegressionModel", "start_line": 28, "end_line": 38, "docstring": "Initializes a TextRegressionModel instance.\n:param model_args: dictionary with model arguments such as model name, path, revision, etc.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.text_regression_model::register_inference_function", "path": "v2/src/lmflow/models/text_regression_model.py", "module": "v2.src.lmflow.models.text_regression_model", "name": "register_inference_function", "kind": "Function", "parent": "TextRegressionModel", "start_line": 41, "end_line": 45, "docstring": "Registers a regression function.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.text_regression_model::inference", "path": "v2/src/lmflow/models/text_regression_model.py", "module": "v2.src.lmflow.models.text_regression_model", "name": "inference", "kind": "Function", "parent": "TextRegressionModel", "start_line": 48, "end_line": 57, "docstring": "Gets regression results of a given dataset.\n\n:inputs: Dataset object, only accept type \"text_only\".", "calls": ["inference_func"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.auto_model::AutoModel", "path": "v2/src/lmflow/models/auto_model.py", "module": "v2.src.lmflow.models.auto_model", "name": "AutoModel", "kind": "Class", "parent": null, "start_line": 10, "end_line": 25, "docstring": null, "calls": ["HFDecoderModel", "HFTextRegressionModel", "HFEncoderDecoderModel", "NotImplementedError"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.models.auto_model::get_model", "path": "v2/src/lmflow/models/auto_model.py", "module": "v2.src.lmflow.models.auto_model", "name": "get_model", "kind": "Function", "parent": "AutoModel", "start_line": 13, "end_line": 25, "docstring": null, "calls": ["HFDecoderModel", "HFTextRegressionModel", "HFEncoderDecoderModel", "NotImplementedError"], "decorators": ["classmethod"], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::HFDecoderModel", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "HFDecoderModel", "kind": "Class", "parent": null, "start_line": 67, "end_line": 682, "docstring": "Initializes a HFDecoderModel instance.\n\nParameters\n------------\n\nmodel_args : \n    Model arguments such as model name, path, revision, etc.\n\ntune_strategy : str or none,  default=\"normal\".\n    A string representing the dataset backend. Defaults to \"huggingface\".\n\nds_config :   \n    Deepspeed configuations.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    ", "calls": ["__init__", "get_type", "get_backend_dataset", "list", "get_data_args", "print", "print", "print", "map", "isinstance", "isinstance", "generate", "NotImplementedError", "save_pretrained", "get_backend", "NotImplementedError", "hexdigest", "warning", "tokenizer", "isinstance", "tensor", "dim", "batch_decode", "decode", "activate_model_for_inference", "__vllm_inference", "__inference", "deactivate_model_for_inference", "no_grad", "append"], "decorators": [], "num_calls": 102}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::__init__", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "__init__", "kind": "Function", "parent": "HFDecoderModel", "start_line": 90, "end_line": 115, "docstring": "Initializes a HFDecoderModel instance.\n:param model_args: dictionary with model arguments such as model name, path, revision, etc.\n:param tune_strategy: tuning strategy: normal, none, lora or adapter\n:param ds_config: deepspeed configuration for distributed training", "calls": ["__init__"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::tokenize", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "tokenize", "kind": "Function", "parent": "HFDecoderModel", "start_line": 118, "end_line": 255, "docstring": "Tokenize the full dataset.\n\nParameters\n------------\ndataset : lmflow.datasets.Dataset.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\ntokenized_datasets :\n    The tokenized dataset, without any leading or trailing special\n    tokens (normally they are Begin-Of-Sentence or End-Of-Sentence\n    tokens).", "calls": ["get_type", "get_backend_dataset", "list", "get_data_args", "print", "print", "print", "map", "get_backend", "NotImplementedError", "hexdigest", "warning", "warning", "NotImplementedError", "md5", "warning", "encode", "keys", "NotImplementedError", "get_fingerprint", "str", "str"], "decorators": [], "num_calls": 22}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::encode", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "encode", "kind": "Function", "parent": "HFDecoderModel", "start_line": 258, "end_line": 286, "docstring": "Perform encoding process of the tokenizer.\n\nParameters\n------------\ninputs : str or list.\n    The text sequence.\n    \nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    if string input,return the tokenized inputs.\n    \"Hello,world!\"-> [101, 7592, 1010, 2088, 102]\n    if batch input,return {input_ids,attention_mask,token_type_ids}\n    [\"Hello,world!\",\"Hello!\"]-> {'input_ids': tensor([[  101,  7592,  1010,  2088,   102],...),'attention_mask': tensor([[1, 1, 1, 1, 1],[0,0,1,1,1]])}", "calls": ["isinstance", "tokenizer", "isinstance", "encode", "NotImplementedError", "type"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::decode", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "decode", "kind": "Function", "parent": "HFDecoderModel", "start_line": 289, "end_line": 319, "docstring": "Perform decoding process of the tokenizer.\n\nParameters\n------------\ninputs : list or tensor.\n    The token sequence.\n    \nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The text decoded from the token inputs.\n    if batch input,return the list of text\n    [[101, 7592, 1010, 2088, 102],[101, 7592, 1010, 2088, 102]]-> [\"Hello,world!\",\"Hello,world!\"\n    if single input,return the text\n    [101, 7592, 1010, 2088, 102]-> \"Hello,world!\"", "calls": ["isinstance", "tensor", "dim", "batch_decode", "decode"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::inference", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "inference", "kind": "Function", "parent": "HFDecoderModel", "start_line": 322, "end_line": 366, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ninputs :\n    The sequence used as a prompt for the generation or as model inputs to the model.\n    When using vllm inference, this should be a string or a list of strings.\n    When using normal inference, this should be a tensor.\nrelease_gpu : bool, optional\n    Whether to release the GPU resource after inference, by default False.\nuse_vllm : bool, optional\n    Whether to use VLLM for inference, by default False.\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The generated sequence output ", "calls": ["activate_model_for_inference", "__vllm_inference", "__inference", "deactivate_model_for_inference", "is_vllm_available", "ImportError"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::__inference", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "__inference", "kind": "Function", "parent": "HFDecoderModel", "start_line": 369, "end_line": 418, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ninputs :\n    The **tokenized** sequence used as a prompt for the generation or as model inputs to the model.\n    \nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The generated sequence output ", "calls": ["no_grad", "generate", "generate", "generate", "NotImplementedError"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::__vllm_inference", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "__vllm_inference", "kind": "Function", "parent": "HFDecoderModel", "start_line": 421, "end_line": 462, "docstring": "Perform VLLM inference process of the model.\n\nParameters\n----------\ninputs : Union[str, List[str]]\n    Prompt(s), string or a list of strings.\nsampling_params : Optional[SamplingParams], optional\n    vllm SamplingParams object, by default None.\n\nReturns\n-------\nList[VLLMInferenceResultWithInput]\n    Return a list of VLLMInferenceResultWithInput, where each\n    element contains the input prompt and the corresponding output.\n    \n    When `sampling_params.detokenize = True`, the output would be a list of strings,\n    contains sampling_params.n samples for the corresponding prompt.\n    \n    When `sampling_params.detokenize = False`, return a list of list of ints \n    (token ids, no decoding after generation).", "calls": ["generate", "append"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::prepare_inputs_for_inference", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "prepare_inputs_for_inference", "kind": "Function", "parent": "HFDecoderModel", "start_line": 465, "end_line": 507, "docstring": "Prepare inputs for inference.\n\nParameters\n------------\ndataset : lmflow.datasets.Dataset.\n    The dataset used for inference.\n    \nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The prepared inputs for inference.", "calls": ["__prepare_inputs_for_vllm_inference", "__prepare_inputs_for_inference", "ImportError", "is_ray_available"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::__prepare_inputs_for_vllm_inference", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "__prepare_inputs_for_vllm_inference", "kind": "Function", "parent": "HFDecoderModel", "start_line": 510, "end_line": 590, "docstring": null, "calls": ["get_type", "from_items", "map", "get_type", "warning", "get_backend_dataset", "get_backend_dataset", "map", "get_type", "NotImplementedError", "len", "get_backend_dataset", "get_backend_dataset", "map", "warning", "apply_chat_template", "get_backend_dataset", "apply_chat_template", "warning", "get_type", "apply_chat_template", "get_backend_dataset", "len"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::__prepare_inputs_for_inference", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "__prepare_inputs_for_inference", "kind": "Function", "parent": "HFDecoderModel", "start_line": 593, "end_line": 598, "docstring": null, "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::merge_lora_weights", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "merge_lora_weights", "kind": "Function", "parent": "HFDecoderModel", "start_line": 601, "end_line": 610, "docstring": null, "calls": ["merge_and_unload", "warning", "get_peft_without_qlora", "merge_and_unload", "warning", "get_backend_model", "get_backend_model"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::get_peft_without_qlora", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "get_peft_without_qlora", "kind": "Function", "parent": "HFDecoderModel", "start_line": 612, "end_line": 650, "docstring": null, "calls": ["TemporaryDirectory", "print", "save_pretrained", "from_pretrained", "from_pretrained", "from_pretrained", "getattr", "get", "int", "get_backend_model", "get", "bool"], "decorators": [], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::save", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "save", "kind": "Function", "parent": "HFDecoderModel", "start_line": 652, "end_line": 682, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ndir :\n    The directory to save model and tokenizer\n    \nsave_full_model : Optional.\n    Whether to save full model.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The generated sequence output ", "calls": ["save_pretrained", "save_pretrained", "warning", "save_pretrained", "get_tokenizer", "getattr", "to", "get_backend_model"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.models.hf_decoder_model::preprocess_conversation", "path": "v2/src/lmflow/models/hf_decoder_model.py", "module": "v2.src.lmflow.models.hf_decoder_model", "name": "preprocess_conversation", "kind": "Function", "parent": null, "start_line": 551, "end_line": 567, "docstring": null, "calls": ["warning", "apply_chat_template", "len"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.encoder_decoder_model::EncoderDecoderModel", "path": "v2/src/lmflow/models/encoder_decoder_model.py", "module": "v2.src.lmflow.models.encoder_decoder_model", "name": "EncoderDecoderModel", "kind": "Class", "parent": null, "start_line": 19, "end_line": 22, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.encoder_decoder_model::__init__", "path": "v2/src/lmflow/models/encoder_decoder_model.py", "module": "v2.src.lmflow.models.encoder_decoder_model", "name": "__init__", "kind": "Function", "parent": "EncoderDecoderModel", "start_line": 21, "end_line": 22, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.base_model::BaseModel", "path": "v2/src/lmflow/models/base_model.py", "module": "v2.src.lmflow.models.base_model", "name": "BaseModel", "kind": "Class", "parent": null, "start_line": 9, "end_line": 12, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.base_model::__init__", "path": "v2/src/lmflow/models/base_model.py", "module": "v2.src.lmflow.models.base_model", "name": "__init__", "kind": "Function", "parent": "BaseModel", "start_line": 11, "end_line": 12, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::HFTextRegressionModel", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "HFTextRegressionModel", "kind": "Class", "parent": null, "start_line": 55, "end_line": 487, "docstring": "Initializes a HFTextRegressionModel instance.\n\nParameters\n------------\n\nmodel_args : \n    Model arguments such as model name, path, revision, etc.\n\ntune_strategy : str or none,  default=\"normal\".\n    A string representing the dataset backend. Defaults to \"huggingface\".\n\nds_config :   \n    Deepspeed configuations.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    ", "calls": ["__init__", "get_type", "get_backend_dataset", "list", "get_data_args", "map", "get", "NotImplementedError", "tokenize", "enumerate", "deepcopy", "Dataset", "from_dict", "deepcopy", "Dataset", "from_dict", "save_pretrained", "save_pretrained", "get_backend", "NotImplementedError", "update", "hexdigest", "warning", "activate_model_for_inference", "__vllm_inference", "__inference", "deactivate_model_for_inference", "no_grad", "NotImplementedError", "sanity_check"], "decorators": [], "num_calls": 61}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::__init__", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "__init__", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 78, "end_line": 109, "docstring": "Initializes a HFTextRegressionModel instance.\n:param model_args: dictionary with model arguments such as model name, path, revision, etc.\n:param tune_strategy: tuning strategy: normal, none, lora or adapter\n:param ds_config: deepspeed configuration for distributed training", "calls": ["__init__"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::tokenize", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "tokenize", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 112, "end_line": 259, "docstring": "Tokenize the full dataset.\n\nParameters\n------------\ndataset : lmflow.datasets.Dataset.\n\nargs : Optional.\n    Positional arguments.\n\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\ntokenized_datasets :\n    The tokenized dataset, without any leading or trailing special\n    tokens (normally they are Begin-Of-Sentence or End-Of-Sentence\n    tokens).", "calls": ["get_type", "get_backend_dataset", "list", "get_data_args", "map", "get_backend", "NotImplementedError", "update", "hexdigest", "update", "warning", "md5", "warning", "update", "NotImplementedError", "encode", "keys", "NotImplementedError", "get_fingerprint", "str", "str"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::inference", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "inference", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 262, "end_line": 310, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ninputs :\n    The sequence used as a prompt for the generation or as model inputs to the model.\n    When using vllm inference, this should be a string or a list of strings.\n    When using normal inference, this should be a tensor.\nrelease_gpu : bool, optional\n    Whether to release the GPU resource after inference, by default False.\nuse_vllm : bool, optional\n    Whether to use VLLM for inference, by default False.\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The generated sequence output ", "calls": ["warning", "activate_model_for_inference", "__vllm_inference", "__inference", "deactivate_model_for_inference"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::__inference", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "__inference", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 313, "end_line": 360, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ninputs :\n    The **tokenized** sequence used as a prompt for the generation or as model inputs to the model.\nkwargs : Optional.\n    Keyword arguments.    \n\nReturns\n------------\noutputs :\n    The generated sequence output ", "calls": ["get", "no_grad", "backend_model", "module", "backend_model", "NotImplementedError"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::__vllm_inference", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "__vllm_inference", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 363, "end_line": 383, "docstring": "Perform VLLM inference process of the model.\n\nParameters\n----------\ninputs : Union[str, List[str]]\n    Prompt(s), string or a list of strings.\nsampling_params : Optional[SamplingParams], optional\n    vllm SamplingParams object, by default None.\n\nReturns\n-------", "calls": ["NotImplementedError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::prepare_inputs_for_inference", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "prepare_inputs_for_inference", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 386, "end_line": 423, "docstring": null, "calls": ["tokenize", "NotImplementedError", "sanity_check", "get_backend_dataset", "from_items", "is_ray_available", "ValueError"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::postprocess_inference_outputs", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "postprocess_inference_outputs", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 427, "end_line": 456, "docstring": null, "calls": ["enumerate", "deepcopy", "Dataset", "from_dict", "get_type", "enumerate", "NotImplementedError", "enumerate", "get_backend_dataset", "append", "len", "warning", "get_type"], "decorators": ["staticmethod"], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::postprocess_distributed_inference_outputs", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "postprocess_distributed_inference_outputs", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 460, "end_line": 471, "docstring": null, "calls": ["deepcopy", "Dataset", "from_dict"], "decorators": ["staticmethod"], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.hf_text_regression_model::save", "path": "v2/src/lmflow/models/hf_text_regression_model.py", "module": "v2.src.lmflow.models.hf_text_regression_model", "name": "save", "kind": "Function", "parent": "HFTextRegressionModel", "start_line": 474, "end_line": 487, "docstring": "Perform generation process of the model.\n\nParameters\n------------\ndir :\n    The directory to save model and tokenizer\n\nkwargs : Optional.\n    Keyword arguments.    ", "calls": ["save_pretrained", "save_pretrained", "get_tokenizer", "get_backend_model"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::HFModelMixin", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "HFModelMixin", "kind": "Class", "parent": null, "start_line": 59, "end_line": 579, "docstring": null, "calls": ["__prepare_tokenizer", "__prepare_dtype", "__prepare_model_config", "__prepare_quant_config", "__prepare_peft_config", "__model_module_inject", "debug", "info", "__prepare_model_post_process", "info", "hasattr", "info", "deepcopy", "__prepare_model_post_process", "LLM", "__prepare_model_for_training", "ValueError", "from_pretrained", "bool", "update", "from_pretrained", "LoraConfig", "LoraConfig", "from_pretrained", "from_config", "sum", "info", "enable_input_require_grads", "print_trainable_parameters", "GatheredParameters"], "decorators": [], "num_calls": 93}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__init__", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__init__", "kind": "Function", "parent": "HFModelMixin", "start_line": 60, "end_line": 114, "docstring": "Initializes a HFModel instance.\n\nParameters\n----------\nmodel_args : \n    Dictionary with model arguments such as model name, path, revision, etc.\ndo_train : bool\n    To prepare the model for training or inference.\nds_config : optional\n    Deepspeed configuration for distributed training, by default None\ndevice : str, optional\n    By default \"gpu\"\nuse_accelerator : bool, optional\n    By default False", "calls": ["__prepare_tokenizer", "__prepare_dtype", "__prepare_model_config", "__prepare_quant_config", "__prepare_peft_config", "__model_module_inject", "__prepare_model_for_training"], "decorators": [], "num_calls": 7}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_tokenizer", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_tokenizer", "kind": "Function", "parent": "HFModelMixin", "start_line": 117, "end_line": 158, "docstring": null, "calls": ["ValueError", "from_pretrained", "warning", "from_pretrained", "get_vocab", "add_special_tokens"], "decorators": [], "num_calls": 6}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_dtype", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_dtype", "kind": "Function", "parent": "HFModelMixin", "start_line": 161, "end_line": 184, "docstring": null, "calls": ["debug", "getattr", "warning", "getattr"], "decorators": [], "num_calls": 4}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_model_config", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_model_config", "kind": "Function", "parent": "HFModelMixin", "start_line": 187, "end_line": 229, "docstring": "Prepare model configuration for hf auto register,\nParameters\n----------\nmodel_args : ModelArguments\n    LMFlow model arguments.\nhf_auto_model_additional_args : Optional[Dict], optional\n    Special configurations such as `num_labels` in `AutoModelForSequenceClassification` \n    (commonly used in reward modeling) will not preset in __prepare_model_config, \n    so it should be passed in hf_auto_model_additional_args.\nReturns\n-------\nconfig : ModelConfig\n    hf model config.", "calls": ["bool", "update", "from_pretrained", "from_pretrained", "warning", "info", "update_from_string", "info"], "decorators": [], "num_calls": 8}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_quant_config", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_quant_config", "kind": "Function", "parent": "HFModelMixin", "start_line": 232, "end_line": 254, "docstring": null, "calls": ["BitsAndBytesConfig", "BitsAndBytesConfig"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_peft_config", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_peft_config", "kind": "Function", "parent": "HFModelMixin", "start_line": 257, "end_line": 308, "docstring": null, "calls": ["LoraConfig", "LoraConfig", "hasattr", "hasattr", "get", "to_dict", "warning", "to_dict", "warning"], "decorators": [], "num_calls": 9}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__model_module_inject", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__model_module_inject", "kind": "Function", "parent": "HFModelMixin", "start_line": 311, "end_line": 327, "docstring": "Override some model modules with custom implementations.\n\nCurrent implementations:\n- Position interpolation (model_args.do_rope_scaling): \n    replace llama embeddings with condense embeddings.", "calls": ["replace_llama_with_condense"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_model_for_training", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_model_for_training", "kind": "Function", "parent": "HFModelMixin", "start_line": 330, "end_line": 395, "docstring": null, "calls": ["info", "__prepare_model_post_process", "from_pretrained", "from_config", "sum", "info", "enable_input_require_grads", "print_trainable_parameters", "GatheredParameters", "len", "resize_token_embeddings", "gradient_checkpointing_enable", "prepare_model_for_kbit_training", "values", "from_pretrained", "get_peft_model", "get_input_embeddings", "len", "named_buffers", "get_input_embeddings", "dict", "data_ptr", "numel", "parameters"], "decorators": [], "num_calls": 24}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_model_for_inference", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_model_for_inference", "kind": "Function", "parent": "HFModelMixin", "start_line": 398, "end_line": 462, "docstring": null, "calls": ["info", "hasattr", "info", "deepcopy", "__prepare_model_post_process", "update", "HfDeepSpeedConfig", "from_pretrained", "from_pretrained", "init_distributed", "eval", "device", "to", "warning", "from_pretrained", "initialize"], "decorators": [], "num_calls": 16}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_model_for_vllm_inference", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_model_for_vllm_inference", "kind": "Function", "parent": "HFModelMixin", "start_line": 465, "end_line": 481, "docstring": null, "calls": ["LLM", "is_vllm_available", "ImportError"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::__prepare_model_post_process", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "__prepare_model_post_process", "kind": "Function", "parent": "HFModelMixin", "start_line": 484, "end_line": 504, "docstring": null, "calls": ["hasattr", "warning", "warning"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::activate_model_for_inference", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "activate_model_for_inference", "kind": "Function", "parent": "HFModelMixin", "start_line": 507, "end_line": 530, "docstring": null, "calls": ["warning", "__prepare_model_for_vllm_inference", "__prepare_model_for_inference", "get", "get"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::deactivate_model_for_inference", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "deactivate_model_for_inference", "kind": "Function", "parent": "HFModelMixin", "start_line": 533, "end_line": 558, "docstring": "Deactivate the model and release the resources.\n\nNOTE: Currently, VLLM doesn't have an official way to do this, and the\nimplementation below cannot release all gpu resources by our observation.\nThus this method is just a placeholder for future implementation. See: \n[Github issue](https://github.com/vllm-project/vllm/issues/1908)", "calls": ["warning", "destroy_model_parallel", "collect", "empty_cache", "to"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::get_max_length", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "get_max_length", "kind": "Function", "parent": "HFModelMixin", "start_line": 561, "end_line": 565, "docstring": "Return max acceptable input length in terms of tokens.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::get_tokenizer", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "get_tokenizer", "kind": "Function", "parent": "HFModelMixin", "start_line": 568, "end_line": 572, "docstring": "Return the tokenizer of the model.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.hf_model_mixin::get_backend_model", "path": "v2/src/lmflow/models/hf_model_mixin.py", "module": "v2.src.lmflow.models.hf_model_mixin", "name": "get_backend_model", "kind": "Function", "parent": "HFModelMixin", "start_line": 575, "end_line": 579, "docstring": "Return the backend model.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.regression_model::RegressionModel", "path": "v2/src/lmflow/models/regression_model.py", "module": "v2.src.lmflow.models.regression_model", "name": "RegressionModel", "kind": "Class", "parent": null, "start_line": 8, "end_line": 11, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.regression_model::__init__", "path": "v2/src/lmflow/models/regression_model.py", "module": "v2.src.lmflow.models.regression_model", "name": "__init__", "kind": "Function", "parent": "RegressionModel", "start_line": 10, "end_line": 11, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::CustomAutoVision2SeqModel", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "CustomAutoVision2SeqModel", "kind": "Class", "parent": null, "start_line": 36, "end_line": 495, "docstring": null, "calls": ["no_grad", "__init__", "getattr", "getattr", "dict", "from_pretrained", "from_pretrained", "from_pretrained", "save", "load", "register_prompt_cache", "to", "language_model", "CausalLMOutputWithPast", "ones", "to", "enumerate", "append", "cat", "append", "cat", "to", "to", "to", "to", "generate", "build_vision_tower", "from_pretrained", "getattr", "Linear"], "decorators": [], "num_calls": 86}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::__init__", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "__init__", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 37, "end_line": 126, "docstring": "TODO update the docs\nArgs:\n    config:\n    # the below varaible are used to overwrite the model in config\n    image_encoder_name_or_path:\n    qformer_name_or_path:\n    language_model_name_or_path:\nReturns:", "calls": ["__init__", "getattr", "getattr", "dict", "build_vision_tower", "from_pretrained", "getattr", "Linear", "Linear", "post_init", "super", "from_pretrained", "Blip2VisionModel", "Parameter", "from_pretrained", "Parameter", "Blip2QFormerModel", "dict", "from_config", "from_config", "zeros", "zeros", "dict"], "decorators": [], "num_calls": 23}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::get_backend_model", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "get_backend_model", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 128, "end_line": 129, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::vision_model_from_pretrained", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "vision_model_from_pretrained", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 131, "end_line": 134, "docstring": null, "calls": ["from_pretrained"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::qformer_from_pretrained", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "qformer_from_pretrained", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 136, "end_line": 139, "docstring": null, "calls": ["from_pretrained"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::language_model_from_pretrained", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "language_model_from_pretrained", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 141, "end_line": 165, "docstring": null, "calls": ["from_pretrained", "dict", "Linear"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::vision_feature_select", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "vision_feature_select", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 167, "end_line": 175, "docstring": null, "calls": ["ValueError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::register_prompt_cache", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "register_prompt_cache", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 177, "end_line": 190, "docstring": "Udpate the prompt id and embedding for reuse in the future\n\nArgs:\n    prompt_ids (torch.LongTensor): The id of the prompt.\n    prompt_keys_values (torch.FloatTensor): The embedding of the prompt.\n\nReturns:\n    None", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::save_prompt_cache", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "save_prompt_cache", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 192, "end_line": 208, "docstring": "Save prompt embedding and id.\n\nArgs:\n    path: The path to save the prompt embedding and id.\n\nReturns:\n    None", "calls": ["save", "dict"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::load_prompt_cache", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "load_prompt_cache", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 210, "end_line": 221, "docstring": "Load prompt embedding and id.\nArgs:\n    path: The path to load the prompt embedding and id.\n\nReturns:\n    None", "calls": ["load", "register_prompt_cache"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::get_tokenizer", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "get_tokenizer", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 223, "end_line": 224, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::forward", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "forward", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 226, "end_line": 345, "docstring": null, "calls": ["to", "language_model", "CausalLMOutputWithPast", "prepare_inputs_labels_for_multimodal", "contiguous", "contiguous", "CrossEntropyLoss", "view", "view", "to", "loss_fct", "ones", "ones", "language_projection", "processor_image_token_in_minigpt4", "vision_model", "expand", "qformer", "size"], "decorators": [], "num_calls": 19}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::processor_image_token_in_minigpt4", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "processor_image_token_in_minigpt4", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 347, "end_line": 402, "docstring": null, "calls": ["ones", "to", "enumerate", "append", "cat", "append", "cat", "to", "to", "to", "ones_like", "to", "get_input_embeddings", "len", "append", "append", "append", "append", "size", "repeat", "LongTensor"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.models.vision2seq_model::generate", "path": "v2/src/lmflow/models/vision2seq_model.py", "module": "v2.src.lmflow.models.vision2seq_model", "name": "generate", "kind": "Function", "parent": "CustomAutoVision2SeqModel", "start_line": 405, "end_line": 495, "docstring": "Overrides `generate` function to be able to use the model as a conditional generator.\n\nArgs:\n    pixel_values (`torch.FloatTensor` of shape (batch_size, num_channels, height, width)):\n        Input images to be processed.\n    input_ids (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n        The sequence used as a prompt for the generation.\n    attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n        Mask to avoid performing attention on padding token indices\n    image_token_indexes (bool, *optional*):\n        The index for inserting the image tokens.\n    one_sample_multiple_images: (bool, *optional*):\n        The flag for inference that the input batch size is 1 and contain multiple images.\n\nReturns:\n    captions (list): A list of strings of length batch_size * num_captions.", "calls": ["no_grad", "to", "to", "generate", "ones", "language_projection", "processor_image_token_in_minigpt4", "prepare_inputs_labels_for_multimodal", "vision_model", "expand", "qformer", "dim", "size"], "decorators": [null], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.models.decoder_model::DecoderModel", "path": "v2/src/lmflow/models/decoder_model.py", "module": "v2.src.lmflow.models.decoder_model", "name": "DecoderModel", "kind": "Class", "parent": null, "start_line": 19, "end_line": 22, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.decoder_model::__init__", "path": "v2/src/lmflow/models/decoder_model.py", "module": "v2.src.lmflow.models.decoder_model", "name": "__init__", "kind": "Function", "parent": "DecoderModel", "start_line": 21, "end_line": 22, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.interfaces.tunable::Tunable", "path": "v2/src/lmflow/models/interfaces/tunable.py", "module": "v2.src.lmflow.models.interfaces.tunable", "name": "Tunable", "kind": "Class", "parent": null, "start_line": 9, "end_line": 10, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::build_vision_tower", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "build_vision_tower", "kind": "Function", "parent": null, "start_line": 13, "end_line": 18, "docstring": null, "calls": ["getattr", "ValueError", "startswith", "startswith", "CLIPVisionTower"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::CLIPVisionTower", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "CLIPVisionTower", "kind": "Class", "parent": null, "start_line": 21, "end_line": 227, "docstring": null, "calls": ["no_grad", "__init__", "getattr", "from_pretrained", "from_pretrained", "requires_grad_", "self", "zeros", "enumerate", "any", "load_model", "from_pretrained", "language_projection", "type", "vision_tower", "to", "cat", "encode_images", "split", "encode_images", "cat", "append", "max", "stack", "stack", "super", "ValueError", "vision_tower", "to", "append"], "decorators": [], "num_calls": 98}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::__init__", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "__init__", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 22, "end_line": 33, "docstring": null, "calls": ["__init__", "getattr", "load_model", "from_pretrained", "super"], "decorators": [], "num_calls": 5}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::load_model", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "load_model", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 35, "end_line": 39, "docstring": null, "calls": ["from_pretrained", "from_pretrained", "requires_grad_"], "decorators": [], "num_calls": 3}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::encode_images", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "encode_images", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 41, "end_line": 47, "docstring": null, "calls": ["self", "language_projection"], "decorators": [], "num_calls": 2}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::feature_select", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "feature_select", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 49, "end_line": 57, "docstring": null, "calls": ["ValueError"], "decorators": [], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::forward", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "forward", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 60, "end_line": 71, "docstring": null, "calls": ["no_grad", "type", "vision_tower", "to", "vision_tower", "to", "append", "to", "unsqueeze", "feature_select", "feature_select", "to"], "decorators": [null], "num_calls": 12}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::dummy_feature", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "dummy_feature", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 74, "end_line": 75, "docstring": null, "calls": ["zeros"], "decorators": ["property"], "num_calls": 1}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::dtype", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "dtype", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 78, "end_line": 79, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::device", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "device", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 82, "end_line": 83, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::config", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "config", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 86, "end_line": 90, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::hidden_size", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "hidden_size", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 93, "end_line": 94, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::num_patches", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "num_patches", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 97, "end_line": 98, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "v2.src.lmflow.models.vision_encoder.clip_encoder::prepare_inputs_labels_for_multimodal", "path": "v2/src/lmflow/models/vision_encoder/clip_encoder.py", "module": "v2.src.lmflow.models.vision_encoder.clip_encoder", "name": "prepare_inputs_labels_for_multimodal", "kind": "Function", "parent": "CLIPVisionTower", "start_line": 100, "end_line": 227, "docstring": "Copy from the LLAVA code base.\nShould be polished.", "calls": ["enumerate", "any", "cat", "encode_images", "split", "encode_images", "cat", "append", "max", "stack", "stack", "ones", "type", "flatten", "sum", "embed_tokens", "append", "where", "numel", "numel", "to", "cat", "append", "cat", "append", "stack", "zip", "stack", "stack", "full"], "decorators": [], "num_calls": 74}
{"symbol_id": "v2.src.lmflow.tokenization.hf_decoder_model::blocking", "path": "v2/src/lmflow/tokenization/hf_decoder_model.py", "module": "v2.src.lmflow.tokenization.hf_decoder_model", "name": "blocking", "kind": "Function", "parent": null, "start_line": 19, "end_line": 54, "docstring": null, "calls": ["len", "range", "min", "len", "list", "extend", "extend", "extend", "keys", "ValueError", "ValueError", "range", "range", "range", "range", "range", "range"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.tokenization.hf_decoder_model::tokenize_function", "path": "v2/src/lmflow/tokenization/hf_decoder_model.py", "module": "v2.src.lmflow.tokenization.hf_decoder_model", "name": "tokenize_function", "kind": "Function", "parent": null, "start_line": 57, "end_line": 108, "docstring": "Handels text_only and text2text datasets tokenization", "calls": ["len", "CaptureLogger", "blocking", "warning", "tokenizer", "range", "range", "range", "range", "copy", "extend", "extend", "extend", "len", "range"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.tokenization.hf_decoder_model::conversation_tokenize_function", "path": "v2/src/lmflow/tokenization/hf_decoder_model.py", "module": "v2.src.lmflow.tokenization.hf_decoder_model", "name": "conversation_tokenize_function", "kind": "Function", "parent": null, "start_line": 111, "end_line": 202, "docstring": "Handels conversation datasets tokenization", "calls": ["len", "CaptureLogger", "range", "blocking", "warning", "len", "isinstance", "range", "range", "range", "get", "get", "extend", "apply_chat_template", "extend", "extend", "extend", "encode_conversation", "enumerate", "extend", "extend", "extend", "warning", "warning", "len", "len", "len", "enumerate", "len"], "decorators": [], "num_calls": 29}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::blocking_paired", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "blocking_paired", "kind": "Function", "parent": null, "start_line": 22, "end_line": 69, "docstring": null, "calls": ["len", "range", "min", "list", "len", "keys", "extend", "extend", "ValueError", "ValueError", "range", "range", "range", "range"], "decorators": [], "num_calls": 14}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::blocking", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "blocking", "kind": "Function", "parent": null, "start_line": 72, "end_line": 123, "docstring": null, "calls": ["len", "range", "min", "len", "list", "extend", "extend", "extend", "keys", "ValueError", "ValueError", "range", "range", "range", "range", "range", "range"], "decorators": [], "num_calls": 17}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::blocking_text_to_textlist", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "blocking_text_to_textlist", "kind": "Function", "parent": null, "start_line": 126, "end_line": 166, "docstring": null, "calls": ["len", "min", "range", "range", "len", "list", "len", "keys", "extend", "ValueError", "ValueError", "range", "range"], "decorators": [], "num_calls": 13}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::paired_conversation_tokenize_function", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "paired_conversation_tokenize_function", "kind": "Function", "parent": null, "start_line": 169, "end_line": 242, "docstring": null, "calls": ["len", "CaptureLogger", "range", "blocking_paired", "warning", "error", "range", "range", "get", "get", "encode_conversation", "enumerate", "extend", "extend", "error", "error", "warning", "warning", "len", "len", "len"], "decorators": [], "num_calls": 21}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::conversation_tokenize_function", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "conversation_tokenize_function", "kind": "Function", "parent": null, "start_line": 245, "end_line": 314, "docstring": "Handels conversation datasets tokenization\n    ", "calls": ["len", "CaptureLogger", "range", "blocking", "warning", "len", "encode_conversation", "enumerate", "extend", "extend", "extend", "range", "range", "range", "get", "get", "warning", "warning", "len", "len", "len", "len"], "decorators": [], "num_calls": 22}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::tokenize_function", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "tokenize_function", "kind": "Function", "parent": null, "start_line": 317, "end_line": 376, "docstring": "Handels text_only and text2text datasets tokenization\n    ", "calls": ["len", "CaptureLogger", "blocking", "warning", "tokenizer", "range", "range", "range", "range", "copy", "extend", "extend", "extend", "len", "range"], "decorators": [], "num_calls": 15}
{"symbol_id": "v2.src.lmflow.tokenization.hf_text_regression_model::text_to_textlist_tokenize_function", "path": "v2/src/lmflow/tokenization/hf_text_regression_model.py", "module": "v2.src.lmflow.tokenization.hf_text_regression_model", "name": "text_to_textlist_tokenize_function", "kind": "Function", "parent": null, "start_line": 379, "end_line": 416, "docstring": "For rm inference, and don't need attn mask and labels.\nNOTE: input_ids here refers to the tokenized input_ids of the input **and** output", "calls": ["len", "range", "tokenizer", "blocking_text_to_textlist", "range", "range", "len"], "decorators": [], "num_calls": 7}
{"symbol_id": "llada.eval_llada::set_seed", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "set_seed", "kind": "Function", "parent": null, "start_line": 40, "end_line": 46, "docstring": null, "calls": ["manual_seed", "seed", "seed"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.eval_llada::LLaDAEvalHarness", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "LLaDAEvalHarness", "kind": "Class", "parent": null, "start_line": 50, "end_line": 394, "docstring": null, "calls": ["register_model", "no_grad", "no_grad", "no_grad", "__init__", "Accelerator", "from_pretrained", "from_pretrained", "eval", "device", "from_pretrained", "int", "item", "randint", "long", "repeat", "range", "cat", "where", "to", "range", "full", "range", "all", "len", "from_list", "map", "with_format", "empty_cache", "enumerate"], "decorators": [null], "num_calls": 171}
{"symbol_id": "llada.eval_llada::__init__", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "__init__", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 51, "end_line": 134, "docstring": "Args:\n    model_path: LLaDA-8B-Base model path.\n    mask_id: The token id of [MASK] is 126336.\n    max_length: the max sequence length.\n    batch_size: mini batch size.\n    mc_num: Monte Carlo estimation iterations\n    is_check_greedy: For certain metrics like LAMBADA, the evaluation requires the model to verify whether the answer \n                     is generated through greedy sampling conditioned on the prompt (note that this differs from conditional\n                     generation). We implement this verification through the suffix_greedy_prediction() function, which \n                     returns a True/False judgment used for accuracy calculation. \n                     When is_check_greedy is set to True, the lm-evaluation-harness library automatically invokes this function. \n                     However, since none of the metrics in the LLaDA paper (https://arxiv.org/abs/2502.09992) require this functionality, \n                     we recommend setting is_check_greedy to False. This configuration causes suffix_greedy_prediction() to return False \n                     by default, significantly accelerating the evaluation process.\n    cfg_scale: Unsupervised classifier-free guidance scale.", "calls": ["__init__", "Accelerator", "from_pretrained", "from_pretrained", "eval", "device", "from_pretrained", "int", "update", "prepare", "device", "to", "super", "lower"], "decorators": [], "num_calls": 14}
{"symbol_id": "llada.eval_llada::rank", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "rank", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 136, "end_line": 137, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.eval_llada::world_size", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "world_size", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 140, "end_line": 141, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.eval_llada::_forward_process", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "_forward_process", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 143, "end_line": 163, "docstring": null, "calls": ["item", "randint", "long", "repeat", "range", "cat", "where", "unsqueeze", "repeat", "round", "min", "max", "arange", "randperm", "zeros", "sum", "linspace", "sum", "unsqueeze", "float"], "decorators": [], "num_calls": 20}
{"symbol_id": "llada.eval_llada::get_logits", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "get_logits", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 166, "end_line": 179, "docstring": null, "calls": ["no_grad", "repeat", "clone", "cat", "model", "chunk", "len", "unsqueeze"], "decorators": [null], "num_calls": 8}
{"symbol_id": "llada.eval_llada::get_loglikelihood", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "get_loglikelihood", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 182, "end_line": 200, "docstring": null, "calls": ["no_grad", "to", "range", "concatenate", "arange", "len", "_forward_process", "get_logits", "append", "len", "repeat", "cross_entropy", "sum", "item", "sum"], "decorators": [null], "num_calls": 15}
{"symbol_id": "llada.eval_llada::suffix_greedy_prediction", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "suffix_greedy_prediction", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 203, "end_line": 224, "docstring": null, "calls": ["no_grad", "full", "range", "all", "arange", "len", "to", "to", "len", "argmax", "softmax", "squeeze", "sort", "clone", "get_logits", "to", "len", "len", "len", "gather", "len", "unsqueeze"], "decorators": [null], "num_calls": 22}
{"symbol_id": "llada.eval_llada::_encode_pair", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "_encode_pair", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 226, "end_line": 238, "docstring": null, "calls": ["len", "len", "len", "tokenizer", "tokenizer", "rstrip"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.eval_llada::loglikelihood", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "loglikelihood", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 240, "end_line": 271, "docstring": null, "calls": ["from_list", "map", "with_format", "empty_cache", "_encode_pair", "max", "no_grad", "tqdm", "len", "len", "get_loglikelihood", "suffix_greedy_prediction", "append"], "decorators": [], "num_calls": 13}
{"symbol_id": "llada.eval_llada::loglikelihood_rolling", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "loglikelihood_rolling", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 273, "end_line": 274, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.eval_llada::generate_until", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "generate_until", "kind": "Function", "parent": "LLaDAEvalHarness", "start_line": 277, "end_line": 394, "docstring": null, "calls": ["enumerate", "time", "tqdm", "time", "makedirs", "join", "print", "exists", "tqdm", "append", "len", "pop", "cat", "to", "extend", "range", "print", "print", "print", "print", "print", "print", "len", "append", "append", "max", "append", "cat", "zeros", "range"], "decorators": [], "num_calls": 72}
{"symbol_id": "llada.eval_llada::_tokenize", "path": "llada/eval_llada.py", "module": "llada.eval_llada", "name": "_tokenize", "kind": "Function", "parent": "loglikelihood", "start_line": 241, "end_line": 248, "docstring": null, "calls": ["_encode_pair"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.postprocess_code::pass_at_1", "path": "llada/postprocess_code.py", "module": "llada.postprocess_code", "name": "pass_at_1", "kind": "Function", "parent": null, "start_line": 26, "end_line": 31, "docstring": null, "calls": ["compute"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.postprocess_code::read_jsonl", "path": "llada/postprocess_code.py", "module": "llada.postprocess_code", "name": "read_jsonl", "kind": "Function", "parent": null, "start_line": 36, "end_line": 41, "docstring": null, "calls": ["open", "append", "loads"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.postprocess_code::write_jsonl", "path": "llada/postprocess_code.py", "module": "llada.postprocess_code", "name": "write_jsonl", "kind": "Function", "parent": null, "start_line": 55, "end_line": 58, "docstring": null, "calls": ["open", "write", "dumps"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.generate::add_gumbel_noise", "path": "llada/generate.py", "module": "llada.generate", "name": "add_gumbel_noise", "kind": "Function", "parent": null, "start_line": 27, "end_line": 38, "docstring": "The Gumbel max is a method for sampling categorical distributions.\nAccording to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\nThus, we use float64.", "calls": ["to", "rand_like", "exp", "log"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.generate::get_num_transfer_tokens", "path": "llada/generate.py", "module": "llada.generate", "name": "get_num_transfer_tokens", "kind": "Function", "parent": null, "start_line": 61, "end_line": 81, "docstring": "block_mask_index: (B, L) bool  which positions are masked in the current block\nreturns: (B, steps) int  how many tokens to transfer at each step per batch item", "calls": ["sum", "div", "to", "unsqueeze", "unsqueeze", "to", "expand", "arange", "unsqueeze"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.generate::generate", "path": "llada/generate.py", "module": "llada.generate", "name": "generate", "kind": "Function", "parent": null, "start_line": 86, "end_line": 127, "docstring": "Args:\n    model: Mask predictor.\n    prompt: A tensor of shape (1, L).\n    steps: Sampling steps, less than or equal to gen_length.\n    gen_length: Generated answer length.\n    block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n    temperature: Categorical distribution sampling temperature.\n    cfg_scale: Unsupervised classifier-free guidance scale.\n    remasking: Remasking strategy. 'low_confidence' or 'random'.\n    mask_id: The toke id of [MASK] is 126336.", "calls": ["no_grad", "to", "clone", "range", "get_num_transfer_tokens", "full", "model", "get_transfer_index", "get_transfer_index_dynamic", "sum"], "decorators": [null], "num_calls": 10}
{"symbol_id": "llada.generate::generate_with_prefix_cache", "path": "llada/generate.py", "module": "llada.generate", "name": "generate_with_prefix_cache", "kind": "Function", "parent": null, "start_line": 132, "end_line": 208, "docstring": "Args:\n    model: Mask predictor.\n    prompt: A tensor of shape (1, L).\n    steps: Sampling steps, less than or equal to gen_length.\n    gen_length: Generated answer length.\n    block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n    temperature: Categorical distribution sampling temperature.\n    cfg_scale: Unsupervised classifier-free guidance scale.\n    remasking: Remasking strategy. 'low_confidence' or 'random'.\n    mask_id: The toke id of [MASK] is 126336.", "calls": ["no_grad", "to", "clone", "range", "get_num_transfer_tokens", "model", "range", "full", "get_transfer_index", "get_transfer_index_dynamic", "len", "append", "range", "add_gumbel_noise", "argmax", "len", "sum", "model", "get_transfer_index", "get_transfer_index_dynamic"], "decorators": [null], "num_calls": 20}
{"symbol_id": "llada.generate::generate_with_dual_cache", "path": "llada/generate.py", "module": "llada.generate", "name": "generate_with_dual_cache", "kind": "Function", "parent": null, "start_line": 213, "end_line": 295, "docstring": null, "calls": ["no_grad", "compile", "int", "full", "range", "get_num_transfer_tokens", "model", "zeros_like", "where", "range", "get_transfer_index", "get_transfer_index_dynamic", "where", "cat", "model", "get_transfer_index", "get_transfer_index_dynamic"], "decorators": [null, null], "num_calls": 17}
{"symbol_id": "llada.generate::get_transfer_index", "path": "llada/generate.py", "module": "llada.generate", "name": "get_transfer_index", "kind": "Function", "parent": null, "start_line": 299, "end_line": 367, "docstring": "Returns:\n    x0: (B, L) long  proposed tokens\n    transfer_index: (B, L) bool  which positions to update this step", "calls": ["add_gumbel_noise", "argmax", "where", "tensor", "where", "to", "clamp", "sort", "expand", "expand", "zeros", "scatter", "softmax", "squeeze", "ValueError", "squeeze", "to", "bool", "to", "rand", "NotImplementedError", "finfo", "dim", "size", "unsqueeze", "unsqueeze", "gather", "arange", "unsqueeze"], "decorators": [], "num_calls": 29}
{"symbol_id": "llada.generate::get_transfer_index_dynamic", "path": "llada/generate.py", "module": "llada.generate", "name": "get_transfer_index_dynamic", "kind": "Function", "parent": null, "start_line": 369, "end_line": 406, "docstring": null, "calls": ["add_gumbel_noise", "argmax", "where", "where", "zeros_like", "sum", "range", "softmax", "squeeze", "list", "range", "topk", "to", "gather", "rand", "NotImplementedError", "range", "sort", "len", "len", "len", "unsqueeze", "len"], "decorators": [], "num_calls": 23}
{"symbol_id": "llada.generate::main", "path": "llada/generate.py", "module": "llada.generate", "name": "main", "kind": "Function", "parent": null, "start_line": 408, "end_line": 431, "docstring": null, "calls": ["eval", "from_pretrained", "apply_chat_template", "unsqueeze", "print", "tokenizer", "inference_mode", "range_push", "generate_with_dual_cache", "synchronize", "range_pop", "to", "to", "batch_decode", "from_pretrained", "tensor"], "decorators": [], "num_calls": 16}
{"symbol_id": "llada.chat::chat", "path": "llada/chat.py", "module": "llada.chat", "name": "chat", "kind": "Function", "parent": null, "start_line": 25, "end_line": 65, "docstring": null, "calls": ["eval", "from_pretrained", "print", "print", "print", "input", "apply_chat_template", "unsqueeze", "print", "print", "print", "unsqueeze", "print", "to", "tokenizer", "cat", "generate", "batch_decode", "to", "generate_with_dual_cache", "generate_with_prefix_cache", "from_pretrained", "tensor"], "decorators": [], "num_calls": 23}
{"symbol_id": "llada.app::parse_constraints", "path": "llada/app.py", "module": "llada.app", "name": "parse_constraints", "kind": "Function", "parent": null, "start_line": 56, "end_line": 75, "docstring": "Parse constraints in format: 'position:word, position:word, ...'", "calls": ["split", "split", "int", "strip", "strip"], "decorators": [], "num_calls": 5}
{"symbol_id": "llada.app::format_chat_history", "path": "llada/app.py", "module": "llada.app", "name": "format_chat_history", "kind": "Function", "parent": null, "start_line": 77, "end_line": 93, "docstring": "Format chat history for the LLaDA model\n\nArgs:\n    history: List of [user_message, assistant_message] pairs\n    \nReturns:\n    Formatted conversation for the model", "calls": ["append", "append"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.app::add_gumbel_noise", "path": "llada/app.py", "module": "llada.app", "name": "add_gumbel_noise", "kind": "Function", "parent": null, "start_line": 95, "end_line": 107, "docstring": "The Gumbel max is a method for sampling categorical distributions.\nAccording to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\nThus, we use float64.", "calls": ["to", "rand_like", "exp", "log"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.app::get_num_transfer_tokens", "path": "llada/app.py", "module": "llada.app", "name": "get_num_transfer_tokens", "kind": "Function", "parent": null, "start_line": 109, "end_line": 127, "docstring": "In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\nFurthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\nthe expected number of tokens transitioned at each step should be consistent.\n\nThis function is designed to precompute the number of tokens that need to be transitioned at each step.", "calls": ["sum", "range", "zeros", "size", "size"], "decorators": [], "num_calls": 5}
{"symbol_id": "llada.app::generate_response_with_visualization_cache_and_parallel", "path": "llada/app.py", "module": "llada.app", "name": "generate_response_with_visualization_cache_and_parallel", "kind": "Function", "parent": null, "start_line": 129, "end_line": 272, "docstring": "Generate text with LLaDA model with visualization using the same sampling as in generate.py\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content'\n    gen_length: Length of text to generate\n    steps: Number of denoising steps\n    constraints: Dictionary mapping positions to words\n    temperature: Sampling temperature\n    block_length: Block length for semi-autoregressive generation\n    remasking: Remasking strategy ('low_confidence' or 'random')\n    \nReturns:\n    List of visualization states showing the progression and final text", "calls": ["items", "apply_chat_template", "unsqueeze", "to", "clone", "append", "items", "range", "decode", "encode", "enumerate", "tokenizer", "get_num_transfer_tokens", "model", "get_transfer_index", "range", "range", "append", "to", "full", "range", "len", "append", "range", "add_gumbel_noise", "argmax", "get_transfer_index", "range", "append", "len"], "decorators": [], "num_calls": 41}
{"symbol_id": "llada.app::get_transfer_index", "path": "llada/app.py", "module": "llada.app", "name": "get_transfer_index", "kind": "Function", "parent": null, "start_line": 275, "end_line": 301, "docstring": null, "calls": ["add_gumbel_noise", "argmax", "where", "where", "zeros_like", "range", "softmax", "squeeze", "sum", "topk", "to", "gather", "rand", "NotImplementedError", "range", "unsqueeze"], "decorators": [], "num_calls": 16}
{"symbol_id": "llada.app::generate_response_with_visualization", "path": "llada/app.py", "module": "llada.app", "name": "generate_response_with_visualization", "kind": "Function", "parent": null, "start_line": 303, "end_line": 471, "docstring": "Generate text with LLaDA model with visualization using the same sampling as in generate.py\n\nArgs:\n    messages: List of message dictionaries with 'role' and 'content'\n    gen_length: Length of text to generate\n    steps: Number of denoising steps\n    constraints: Dictionary mapping positions to words\n    temperature: Sampling temperature\n    block_length: Block length for semi-autoregressive generation\n    remasking: Remasking strategy ('low_confidence' or 'random')\n    \nReturns:\n    List of visualization states showing the progression and final text", "calls": ["items", "apply_chat_template", "unsqueeze", "to", "clone", "append", "items", "range", "decode", "encode", "enumerate", "tokenizer", "min", "get_num_transfer_tokens", "range", "to", "full", "range", "any", "add_gumbel_noise", "argmax", "clone", "where", "where", "zeros_like", "range", "where", "items", "range", "append"], "decorators": [], "num_calls": 50}
{"symbol_id": "llada.app::create_chatbot_demo", "path": "llada/app.py", "module": "llada.app", "name": "create_chatbot_demo", "kind": "Function", "parent": null, "start_line": 482, "end_line": 791, "docstring": null, "calls": ["Blocks", "Markdown", "Markdown", "State", "State", "Markdown", "Textbox", "Button", "click", "submit", "click", "then", "then", "Row", "Row", "Group", "Textbox", "Button", "Textbox", "Examples", "Accordion", "copy", "append", "add_message", "add_message", "copy", "copy", "Column", "Chatbot", "Column"], "decorators": [], "num_calls": 70}
{"symbol_id": "llada.app::add_message", "path": "llada/app.py", "module": "llada.app", "name": "add_message", "kind": "Function", "parent": null, "start_line": 605, "end_line": 609, "docstring": "Add a message pair to the history and return the updated history", "calls": ["copy", "append"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.app::user_message_submitted", "path": "llada/app.py", "module": "llada.app", "name": "user_message_submitted", "kind": "Function", "parent": null, "start_line": 611, "end_line": 632, "docstring": "Process a submitted user message", "calls": ["add_message", "add_message", "copy", "copy", "strip", "copy", "copy"], "decorators": [], "num_calls": 7}
{"symbol_id": "llada.app::bot_response", "path": "llada/app.py", "module": "llada.app", "name": "bot_response", "kind": "Function", "parent": null, "start_line": 634, "end_line": 721, "docstring": "Generate bot response for the latest message", "calls": ["format_chat_history", "append", "parse_constraints", "time", "generate_response_with_visualization", "encode", "len", "time", "generate_response_with_visualization_cache_and_parallel", "encode", "len", "time", "time", "sleep", "sleep", "print", "str"], "decorators": [], "num_calls": 17}
{"symbol_id": "llada.app::clear_conversation", "path": "llada/app.py", "module": "llada.app", "name": "clear_conversation", "kind": "Function", "parent": null, "start_line": 723, "end_line": 743, "docstring": "Clear the conversation history", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.sanitize::refine_text", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "refine_text", "kind": "Function", "parent": null, "start_line": 32, "end_line": 35, "docstring": null, "calls": ["replace", "replace", "strip", "replace"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.sanitize::syntax_check", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "syntax_check", "kind": "Function", "parent": null, "start_line": 37, "end_line": 44, "docstring": null, "calls": ["parse", "print_exc"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.sanitize::extract_longest_valid_code", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "extract_longest_valid_code", "kind": "Function", "parent": null, "start_line": 46, "end_line": 63, "docstring": null, "calls": ["splitlines", "range", "len", "len", "range", "len", "join", "syntax_check", "sum", "strip"], "decorators": [], "num_calls": 10}
{"symbol_id": "llada.sanitize::get_deps", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "get_deps", "kind": "Function", "parent": null, "start_line": 65, "end_line": 80, "docstring": null, "calls": ["set", "pop", "iter_child_nodes", "isinstance", "add", "isinstance", "add", "append"], "decorators": [], "num_calls": 8}
{"symbol_id": "llada.sanitize::get_function_dependency", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "get_function_dependency", "kind": "Function", "parent": null, "start_line": 82, "end_line": 92, "docstring": null, "calls": ["set", "pop", "add", "extend", "get", "set"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.sanitize::get_definition_name", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "get_definition_name", "kind": "Function", "parent": null, "start_line": 94, "end_line": 101, "docstring": null, "calls": ["isinstance", "isinstance", "isinstance"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.sanitize::has_return_statement", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "has_return_statement", "kind": "Function", "parent": null, "start_line": 103, "end_line": 104, "docstring": null, "calls": ["any", "isinstance", "walk"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.sanitize::sanitize", "path": "llada/sanitize.py", "module": "llada.sanitize", "name": "sanitize", "kind": "Function", "parent": null, "start_line": 106, "end_line": 147, "docstring": null, "calls": ["refine_text", "extract_longest_valid_code", "parse", "items", "join", "isinstance", "get_deps", "get_function_dependency", "append", "append", "isinstance", "unparse", "append", "isinstance", "unparse", "has_return_statement", "isinstance", "items", "get_definition_name"], "decorators": [], "num_calls": 19}
{"symbol_id": "llada.model.modeling_llada::scaled_dot_product_attention", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "scaled_dot_product_attention", "kind": "Function", "parent": null, "start_line": 94, "end_line": 95, "docstring": null, "calls": ["compile", "scaled_dot_product_attention"], "decorators": [null], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::ModuleType", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "ModuleType", "kind": "Class", "parent": null, "start_line": 97, "end_line": 101, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::init_weights", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "init_weights", "kind": "Function", "parent": null, "start_line": 104, "end_line": 179, "docstring": "Initialize weights of a linear or embedding module.\n\n:param config: The model config.\n:param module: The linear or embedding submodule to initialize.\n:param d: The effective input dimensionality of the weights. This could be smaller than the actual dimensions\n    for fused layers.\n:param layer_id: When set, the standard deviation for the \"mitchell\" method will be adjusted by\n    ``1 / sqrt(2 * (layer_id + 1))``.", "calls": ["isinstance", "trunc_normal_", "normal_", "trunc_normal_", "zeros_", "getattr", "sqrt", "kaiming_normal_", "no_grad", "div_", "sqrt", "normal_", "sqrt", "sqrt", "trunc_normal_", "NotImplementedError", "RuntimeError", "sqrt", "RuntimeError"], "decorators": [], "num_calls": 19}
{"symbol_id": "llada.model.modeling_llada::ensure_finite_", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "ensure_finite_", "kind": "Function", "parent": null, "start_line": 182, "end_line": 190, "docstring": "Modify ``x`` in place to replace ``float(\"-inf\")`` with the minimum value of the dtype when ``check_neg_inf``\nis ``True`` and to replace ``float(\"inf\")`` with the maximum value of the dtype when ``check_pos_inf`` is ``True``.", "calls": ["masked_fill_", "masked_fill_", "float", "finfo", "float", "finfo"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::activation_checkpoint_function", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "activation_checkpoint_function", "kind": "Function", "parent": null, "start_line": 193, "end_line": 203, "docstring": null, "calls": ["partial"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::BufferCache", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "BufferCache", "kind": "Class", "parent": null, "start_line": 206, "end_line": 214, "docstring": "Cache for attention biases and other things that would normally be stored as buffers.\nWe avoid using buffers because we've run into various issues doing so with FSDP.\nIn general it appears the way FSDP handles buffers is not well-defined.\nIt doesn't shard them but apparently it does synchronize them across processes, which we want to avoid\nsince (A) it isn't necessary, and (B) we sometimes have `-inf` in these biases which might get turned into\nNaNs when they're synchronized due to casting or some other issue.", "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::_non_meta_init_device", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "_non_meta_init_device", "kind": "Function", "parent": null, "start_line": 217, "end_line": 221, "docstring": null, "calls": ["device", "device", "is_available"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.model.modeling_llada::Dropout", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "Dropout", "kind": "Class", "parent": null, "start_line": 224, "end_line": 229, "docstring": null, "calls": ["dropout"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::LayerNormBase", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LayerNormBase", "kind": "Class", "parent": null, "start_line": 232, "end_line": 290, "docstring": null, "calls": ["__init__", "Parameter", "register_parameter", "register_parameter", "LayerNorm", "is_autocast_enabled", "to", "ones_", "zeros_", "super", "ones", "Parameter", "register_parameter", "LayerNorm", "is_autocast_cpu_enabled", "to", "zeros", "RMSLayerNorm", "GemmaRMSLayerNorm", "NotImplementedError", "get_autocast_gpu_dtype", "get_autocast_cpu_dtype"], "decorators": [], "num_calls": 22}
{"symbol_id": "llada.model.modeling_llada::LayerNorm", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LayerNorm", "kind": "Class", "parent": null, "start_line": 293, "end_line": 322, "docstring": "The default :class:`LayerNorm` implementation which can optionally run in low precision.", "calls": ["__init__", "_cast_if_autocast_enabled", "layer_norm", "super", "_cast_if_autocast_enabled", "_cast_if_autocast_enabled", "autocast", "layer_norm"], "decorators": [], "num_calls": 8}
{"symbol_id": "llada.model.modeling_llada::RMSLayerNorm", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "RMSLayerNorm", "kind": "Class", "parent": null, "start_line": 325, "end_line": 353, "docstring": "RMS layer norm, a simplified :class:`LayerNorm` implementation", "calls": ["__init__", "autocast", "to", "mean", "to", "super", "rsqrt", "pow"], "decorators": [], "num_calls": 8}
{"symbol_id": "llada.model.modeling_llada::GemmaRMSLayerNorm", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "GemmaRMSLayerNorm", "kind": "Class", "parent": null, "start_line": 356, "end_line": 384, "docstring": "Gemma RMS layer norm, a simplified :class:`LayerNorm` implementation", "calls": ["__init__", "autocast", "to", "mean", "to", "super", "rsqrt", "pow"], "decorators": [], "num_calls": 8}
{"symbol_id": "llada.model.modeling_llada::RotaryEmbedding", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "RotaryEmbedding", "kind": "Class", "parent": null, "start_line": 387, "end_line": 467, "docstring": "[Rotary positional embeddings (RoPE)](https://arxiv.org/abs/2104.09864).", "calls": ["__init__", "get_rotary_embedding", "size", "view", "unbind", "cat", "to", "_non_meta_init_device", "autocast", "arange", "einsum", "cat", "autocast", "get_rotary_embedding", "type_as", "type_as", "arange", "index_select", "index_select", "apply_rotary_pos_emb", "apply_rotary_pos_emb", "type_as", "type_as", "super", "to", "to", "float", "float", "get", "get"], "decorators": [], "num_calls": 34}
{"symbol_id": "llada.model.modeling_llada::Activation", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "Activation", "kind": "Class", "parent": null, "start_line": 471, "end_line": 496, "docstring": null, "calls": ["__init__", "cast", "super", "GELU", "cast", "ReLU", "cast", "SiLU", "SwiGLU", "NotImplementedError"], "decorators": [], "num_calls": 10}
{"symbol_id": "llada.model.modeling_llada::GELU", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "GELU", "kind": "Class", "parent": null, "start_line": 499, "end_line": 502, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::ReLU", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "ReLU", "kind": "Class", "parent": null, "start_line": 505, "end_line": 508, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::SiLU", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "SiLU", "kind": "Class", "parent": null, "start_line": 510, "end_line": 513, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::SwiGLU", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "SwiGLU", "kind": "Class", "parent": null, "start_line": 515, "end_line": 522, "docstring": null, "calls": ["chunk", "silu"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::causal_attention_bias", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "causal_attention_bias", "kind": "Function", "parent": null, "start_line": 525, "end_line": 531, "docstring": null, "calls": ["triu", "masked_fill_", "view", "ones", "finfo"], "decorators": [], "num_calls": 5}
{"symbol_id": "llada.model.modeling_llada::get_causal_attention_bias", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "get_causal_attention_bias", "kind": "Function", "parent": null, "start_line": 534, "end_line": 543, "docstring": null, "calls": ["autocast", "causal_attention_bias", "to", "get"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::alibi_attention_bias", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "alibi_attention_bias", "kind": "Function", "parent": null, "start_line": 546, "end_line": 558, "docstring": null, "calls": ["view", "mul_", "arange", "mul_", "view", "arange", "abs_", "arange", "view"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::LLaDABlock", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDABlock", "kind": "Class", "parent": null, "start_line": 561, "end_line": 810, "docstring": "A base class for transformer block implementations.", "calls": ["__init__", "Dropout", "build", "Linear", "Linear", "init_weights", "init_weights", "size", "transpose", "transpose", "transpose", "_scaled_dot_product_attention", "view", "build", "build", "int", "RotaryEmbedding", "reset_parameters", "reset_parameters", "activation_checkpoint_function", "is_autocast_enabled", "get_autocast_gpu_dtype", "to", "ensure_finite_", "flash_attn_func", "transpose", "size", "size", "scaled_dot_product_attention", "to"], "decorators": [], "num_calls": 65}
{"symbol_id": "llada.model.modeling_llada::LLaDASequentialBlock", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDASequentialBlock", "kind": "Class", "parent": null, "start_line": 813, "end_line": 900, "docstring": "This is a typical transformer block where the output is computed as ``MLP(LN(x + Attention(LN(x))))``\n(plus another skip connection).", "calls": ["__init__", "build", "build", "Linear", "Linear", "reset_parameters", "reset_parameters", "reset_parameters", "init_weights", "init_weights", "ff_proj", "ff_out", "dropout", "sum", "split", "split", "_activation_checkpoint_fn", "attention", "dropout", "_activation_checkpoint_fn", "ff_norm", "_activation_checkpoint_fn", "act", "super", "super", "att_proj", "att_proj", "_activation_checkpoint_fn", "attn_norm"], "decorators": [], "num_calls": 29}
{"symbol_id": "llada.model.modeling_llada::LLaDALlamaBlock", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDALlamaBlock", "kind": "Class", "parent": null, "start_line": 903, "end_line": 1004, "docstring": "This is a transformer block where the output is computed as ``MLP(LN(x + Attention(LN(x))))``\n(plus another skip connection). This block is similar to `LLaDASequentialBlock`\nbut some operations have slightly different implementations to imitate the\nbehavior of Llama.", "calls": ["__init__", "build", "build", "Linear", "Linear", "Linear", "Linear", "Linear", "reset_parameters", "reset_parameters", "reset_parameters", "init_weights", "init_weights", "init_weights", "init_weights", "init_weights", "attn_norm", "q_proj", "k_proj", "v_proj", "ff_out", "dropout", "_activation_checkpoint_fn", "attention", "dropout", "_activation_checkpoint_fn", "ff_norm", "ff_proj", "up_proj", "_activation_checkpoint_fn"], "decorators": [], "num_calls": 33}
{"symbol_id": "llada.model.modeling_llada::LLaDABlockDiffBlock", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDABlockDiffBlock", "kind": "Class", "parent": null, "start_line": 1007, "end_line": 1112, "docstring": "This is a transformer block where the output is computed as ``MLP(LN(x + Attention(LN(x))))``\n(plus another skip connection). This block is similar to `LLaDASequentialBlock`\nbut some operations have slightly different implementations to imitate the\nbehavior of Llama.", "calls": ["__init__", "build", "build", "Linear", "Linear", "Linear", "Linear", "Linear", "reset_parameters", "reset_parameters", "reset_parameters", "init_weights", "init_weights", "init_weights", "init_weights", "init_weights", "rearrange", "fused_flex_attention", "rearrange", "attn_norm", "q_proj", "k_proj", "v_proj", "ff_out", "dropout", "_activation_checkpoint_fn", "attention", "dropout", "_activation_checkpoint_fn", "ff_norm"], "decorators": [], "num_calls": 36}
{"symbol_id": "llada.model.modeling_llada::LLaDAOutput", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDAOutput", "kind": "Class", "parent": null, "start_line": 1115, "end_line": 1130, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::LLaDAGenerateOutput", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDAGenerateOutput", "kind": "Class", "parent": null, "start_line": 1133, "end_line": 1143, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::LLaDABlockGroup", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDABlockGroup", "kind": "Class", "parent": null, "start_line": 1146, "end_line": 1199, "docstring": null, "calls": ["__init__", "activation_checkpoint_function", "enumerate", "reset_parameters", "set_activation_checkpointing", "super", "_activation_checkpoint_fn", "block", "append"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::LLaDAModel", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDAModel", "kind": "Class", "parent": null, "start_line": 1202, "end_line": 1538, "docstring": null, "calls": ["__init__", "BufferCache", "activation_checkpoint_function", "enable_flash_sdp", "enable_mem_efficient_sdp", "ModuleDict", "info", "init_weights", "hasattr", "reset_parameters", "hasattr", "emb_drop", "ln_f", "LLaDAOutput", "Exception", "Exception", "Exception", "dict", "build", "update", "update", "update", "update", "reset_parameters", "get_causal_attention_bias", "get_alibi_attention_bias", "_non_meta_init_device", "init_weights", "init_weights", "autocast"], "decorators": [], "num_calls": 88}
{"symbol_id": "llada.model.modeling_llada::create_model_config_from_pretrained_config", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "create_model_config_from_pretrained_config", "kind": "Function", "parent": null, "start_line": 1541, "end_line": 1551, "docstring": "Utility function", "calls": ["fields", "ModelConfig", "getattr"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.model.modeling_llada::LLaDAModelLM", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "LLaDAModelLM", "kind": "Class", "parent": null, "start_line": 1554, "end_line": 1661, "docstring": "Extremely barebones HF model wrapper.", "calls": ["__init__", "forward", "CausalLMOutputWithPast", "update", "pop", "create_model_config_from_pretrained_config", "LLaDAModel", "ValueError", "warn", "super"], "decorators": [], "num_calls": 10}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "Dropout", "start_line": 225, "end_line": 229, "docstring": null, "calls": ["dropout"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LayerNormBase", "start_line": 233, "end_line": 256, "docstring": null, "calls": ["__init__", "Parameter", "register_parameter", "register_parameter", "super", "ones", "Parameter", "register_parameter", "zeros"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LayerNormBase", "start_line": 259, "end_line": 260, "docstring": null, "calls": [], "decorators": ["abstractmethod"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::build", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "build", "kind": "Function", "parent": "LayerNormBase", "start_line": 263, "end_line": 273, "docstring": null, "calls": ["LayerNorm", "LayerNorm", "RMSLayerNorm", "GemmaRMSLayerNorm", "NotImplementedError"], "decorators": ["classmethod"], "num_calls": 5}
{"symbol_id": "llada.model.modeling_llada::_cast_if_autocast_enabled", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "_cast_if_autocast_enabled", "kind": "Function", "parent": "LayerNormBase", "start_line": 275, "end_line": 284, "docstring": null, "calls": ["is_autocast_enabled", "to", "is_autocast_cpu_enabled", "to", "get_autocast_gpu_dtype", "get_autocast_cpu_dtype"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LayerNormBase", "start_line": 286, "end_line": 290, "docstring": null, "calls": ["ones_", "zeros_"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LayerNorm", "start_line": 298, "end_line": 307, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LayerNorm", "start_line": 309, "end_line": 322, "docstring": null, "calls": ["_cast_if_autocast_enabled", "layer_norm", "_cast_if_autocast_enabled", "_cast_if_autocast_enabled", "autocast", "layer_norm"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "RMSLayerNorm", "start_line": 330, "end_line": 337, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "RMSLayerNorm", "start_line": 339, "end_line": 353, "docstring": null, "calls": ["autocast", "to", "mean", "to", "rsqrt", "pow"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "GemmaRMSLayerNorm", "start_line": 361, "end_line": 368, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "GemmaRMSLayerNorm", "start_line": 370, "end_line": 384, "docstring": null, "calls": ["autocast", "to", "mean", "to", "rsqrt", "pow"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "RotaryEmbedding", "start_line": 392, "end_line": 398, "docstring": null, "calls": ["__init__", "get_rotary_embedding", "_non_meta_init_device", "super"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::get_rotary_embedding", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "get_rotary_embedding", "kind": "Function", "parent": "RotaryEmbedding", "start_line": 400, "end_line": 424, "docstring": null, "calls": ["autocast", "arange", "einsum", "cat", "to", "to", "get", "get", "sin", "cos", "arange"], "decorators": [], "num_calls": 11}
{"symbol_id": "llada.model.modeling_llada::rotate_half", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "rotate_half", "kind": "Function", "parent": "RotaryEmbedding", "start_line": 426, "end_line": 430, "docstring": null, "calls": ["size", "view", "unbind", "cat"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::apply_rotary_pos_emb", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "apply_rotary_pos_emb", "kind": "Function", "parent": "RotaryEmbedding", "start_line": 432, "end_line": 433, "docstring": null, "calls": ["to", "rotate_half"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "RotaryEmbedding", "start_line": 435, "end_line": 467, "docstring": null, "calls": ["autocast", "get_rotary_embedding", "type_as", "type_as", "arange", "index_select", "index_select", "apply_rotary_pos_emb", "apply_rotary_pos_emb", "type_as", "type_as", "float", "float"], "decorators": [], "num_calls": 13}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "Activation", "start_line": 472, "end_line": 474, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "Activation", "start_line": 477, "end_line": 478, "docstring": null, "calls": [], "decorators": ["abstractmethod"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::output_multiplier", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "output_multiplier", "kind": "Function", "parent": "Activation", "start_line": 482, "end_line": 483, "docstring": null, "calls": [], "decorators": ["property", "abstractmethod"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::build", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "build", "kind": "Function", "parent": "Activation", "start_line": 486, "end_line": 496, "docstring": null, "calls": ["cast", "GELU", "cast", "ReLU", "cast", "SiLU", "SwiGLU", "NotImplementedError"], "decorators": ["classmethod"], "num_calls": 8}
{"symbol_id": "llada.model.modeling_llada::output_multiplier", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "output_multiplier", "kind": "Function", "parent": "GELU", "start_line": 501, "end_line": 502, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::output_multiplier", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "output_multiplier", "kind": "Function", "parent": "ReLU", "start_line": 507, "end_line": 508, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::output_multiplier", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "output_multiplier", "kind": "Function", "parent": "SiLU", "start_line": 512, "end_line": 513, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "SwiGLU", "start_line": 516, "end_line": 518, "docstring": null, "calls": ["chunk", "silu"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::output_multiplier", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "output_multiplier", "kind": "Function", "parent": "SwiGLU", "start_line": 521, "end_line": 522, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDABlock", "start_line": 566, "end_line": 621, "docstring": null, "calls": ["__init__", "Dropout", "build", "Linear", "Linear", "build", "build", "int", "RotaryEmbedding", "super"], "decorators": [], "num_calls": 10}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LLaDABlock", "start_line": 623, "end_line": 641, "docstring": null, "calls": ["init_weights", "init_weights", "reset_parameters", "reset_parameters"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::set_activation_checkpointing", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "set_activation_checkpointing", "kind": "Function", "parent": "LLaDABlock", "start_line": 643, "end_line": 647, "docstring": null, "calls": ["activation_checkpoint_function"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::_cast_attn_bias", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "_cast_attn_bias", "kind": "Function", "parent": "LLaDABlock", "start_line": 650, "end_line": 662, "docstring": null, "calls": ["is_autocast_enabled", "get_autocast_gpu_dtype", "to", "ensure_finite_", "is_autocast_cpu_enabled", "get_autocast_cpu_dtype"], "decorators": ["classmethod"], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::_scaled_dot_product_attention", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "_scaled_dot_product_attention", "kind": "Function", "parent": "LLaDABlock", "start_line": 664, "end_line": 700, "docstring": "Computes scaled dot product attention on query, key and value tensors, using an optional\nattention mask if passed, and applying dropout if a probability greater than 0.0 is specified.", "calls": ["flash_attn_func", "transpose", "size", "size", "scaled_dot_product_attention", "transpose", "transpose", "transpose", "size", "size", "repeat_interleave", "repeat_interleave"], "decorators": [], "num_calls": 12}
{"symbol_id": "llada.model.modeling_llada::attention", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "attention", "kind": "Function", "parent": "LLaDABlock", "start_line": 702, "end_line": 790, "docstring": null, "calls": ["size", "transpose", "transpose", "transpose", "_scaled_dot_product_attention", "view", "to", "to", "_cast_attn_bias", "attn_out", "view", "view", "view", "cat", "cat", "range", "rotary_emb", "rotary_emb", "contiguous", "q_norm", "k_norm", "any", "nonzero", "len", "max", "transpose", "len", "len", "nonzero"], "decorators": [], "num_calls": 29}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDABlock", "start_line": 794, "end_line": 801, "docstring": null, "calls": [], "decorators": ["abstractmethod"], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::build", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "build", "kind": "Function", "parent": "LLaDABlock", "start_line": 804, "end_line": 810, "docstring": null, "calls": ["LLaDASequentialBlock", "LLaDALlamaBlock", "NotImplementedError"], "decorators": ["classmethod"], "num_calls": 3}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDASequentialBlock", "start_line": 819, "end_line": 837, "docstring": null, "calls": ["__init__", "build", "build", "Linear", "Linear", "sum", "super"], "decorators": [], "num_calls": 7}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LLaDASequentialBlock", "start_line": 839, "end_line": 849, "docstring": null, "calls": ["reset_parameters", "reset_parameters", "reset_parameters", "init_weights", "init_weights", "super"], "decorators": [], "num_calls": 6}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDASequentialBlock", "start_line": 851, "end_line": 900, "docstring": null, "calls": ["ff_proj", "ff_out", "dropout", "split", "split", "_activation_checkpoint_fn", "attention", "dropout", "_activation_checkpoint_fn", "ff_norm", "_activation_checkpoint_fn", "act", "att_proj", "att_proj", "_activation_checkpoint_fn", "attn_norm"], "decorators": [], "num_calls": 16}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDALlamaBlock", "start_line": 911, "end_line": 940, "docstring": null, "calls": ["__init__", "build", "build", "Linear", "Linear", "Linear", "Linear", "Linear", "super"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LLaDALlamaBlock", "start_line": 942, "end_line": 951, "docstring": null, "calls": ["reset_parameters", "reset_parameters", "reset_parameters", "init_weights", "init_weights", "init_weights", "init_weights", "init_weights", "super"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDALlamaBlock", "start_line": 953, "end_line": 1004, "docstring": null, "calls": ["attn_norm", "q_proj", "k_proj", "v_proj", "ff_out", "dropout", "_activation_checkpoint_fn", "attention", "dropout", "_activation_checkpoint_fn", "ff_norm", "ff_proj", "up_proj", "_activation_checkpoint_fn", "act"], "decorators": [], "num_calls": 15}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDABlockDiffBlock", "start_line": 1015, "end_line": 1044, "docstring": null, "calls": ["__init__", "build", "build", "Linear", "Linear", "Linear", "Linear", "Linear", "super"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LLaDABlockDiffBlock", "start_line": 1046, "end_line": 1055, "docstring": null, "calls": ["reset_parameters", "reset_parameters", "reset_parameters", "init_weights", "init_weights", "init_weights", "init_weights", "init_weights", "super"], "decorators": [], "num_calls": 9}
{"symbol_id": "llada.model.modeling_llada::cross_attn_flex", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "cross_attn_flex", "kind": "Function", "parent": "LLaDABlockDiffBlock", "start_line": 1057, "end_line": 1062, "docstring": null, "calls": ["rearrange", "fused_flex_attention", "rearrange"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDABlockDiffBlock", "start_line": 1064, "end_line": 1112, "docstring": null, "calls": ["attn_norm", "q_proj", "k_proj", "v_proj", "ff_out", "dropout", "_activation_checkpoint_fn", "attention", "dropout", "_activation_checkpoint_fn", "ff_norm", "ff_proj", "up_proj", "_activation_checkpoint_fn", "act"], "decorators": [], "num_calls": 15}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDABlockGroup", "start_line": 1147, "end_line": 1152, "docstring": null, "calls": ["__init__", "activation_checkpoint_function", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDABlockGroup", "start_line": 1154, "end_line": 1190, "docstring": null, "calls": ["enumerate", "_activation_checkpoint_fn", "block", "append"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LLaDABlockGroup", "start_line": 1192, "end_line": 1194, "docstring": null, "calls": ["reset_parameters"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::set_activation_checkpointing", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "set_activation_checkpointing", "kind": "Function", "parent": "LLaDABlockGroup", "start_line": 1196, "end_line": 1199, "docstring": null, "calls": ["set_activation_checkpointing"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDAModel", "start_line": 1203, "end_line": 1279, "docstring": null, "calls": ["__init__", "BufferCache", "activation_checkpoint_function", "enable_flash_sdp", "enable_mem_efficient_sdp", "ModuleDict", "Exception", "Exception", "Exception", "dict", "build", "update", "update", "update", "update", "reset_parameters", "get_causal_attention_bias", "get_alibi_attention_bias", "super", "Exception", "range", "LLaDABlockGroup", "_non_meta_init_device", "_non_meta_init_device", "warn", "Embedding", "Dropout", "build", "range", "ModuleList"], "decorators": [], "num_calls": 33}
{"symbol_id": "llada.model.modeling_llada::set_activation_checkpointing", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "set_activation_checkpointing", "kind": "Function", "parent": "LLaDAModel", "start_line": 1282, "end_line": 1289, "docstring": null, "calls": ["set_activation_checkpointing", "set_activation_checkpointing"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::device", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "device", "kind": "Function", "parent": "LLaDAModel", "start_line": 1292, "end_line": 1297, "docstring": null, "calls": ["_non_meta_init_device"], "decorators": ["property"], "num_calls": 1}
{"symbol_id": "llada.model.modeling_llada::reset_parameters", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "reset_parameters", "kind": "Function", "parent": "LLaDAModel", "start_line": 1299, "end_line": 1324, "docstring": null, "calls": ["info", "init_weights", "hasattr", "reset_parameters", "hasattr", "init_weights", "init_weights", "reset_parameters", "reset_parameters", "sqrt"], "decorators": [], "num_calls": 10}
{"symbol_id": "llada.model.modeling_llada::get_alibi_attention_bias", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "get_alibi_attention_bias", "kind": "Function", "parent": "LLaDAModel", "start_line": 1326, "end_line": 1337, "docstring": null, "calls": ["autocast", "alibi_attention_bias", "to", "get"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDAModel", "start_line": 1339, "end_line": 1538, "docstring": ":param input_ids: A tensor of shape `(batch_size, seq_len)`.\n:param input_embeddings: A tensor of shape `(batch_size, seq_len, d_model)` with input\n    embeddings. When provided, it is treated as the output of the input embedding layer.\n:param attention_mask: A tensor of shape `(batch_size, seq_len)` that indicates\n    which input IDs are masked. A `1` value in the mask means that\n    the corresponding input ID should *not* be ignored. A `0` means\n    that the corresponding input ID is masked.\n\n    This has the same meaning as the `attention_mask` in HuggingFace's `transformers`\n    library.\n:param attention_bias: A tensor of shape `(batch_size, 1, seq_len, seq_len)`,\n    `(1, 1, seq_len, seq_len)`, or `(seq_len, seq_len)`. This is used\n    to introduce causal or other biases.\n\n    If the tensor is a bool or byte tensor, a `True` or `1` at `attention_bias[:, :, i, j]`\n    indicates that the i-th element in the sequence is allowed to attend to the j-th\n    element in the sequence.\n\n    If the tensor is a float tensor, it will just be added to the attention\n    scores before the softmax.\n\n    The default is causal, which corresponds to a lower-diagonal byte matrix of ones.\n:param past_key_values: Pre-computed keys and values for each attention block.\n    Can be used to speed up sequential decoding. The `input_ids` which have\n    their past given to this model should not be passed as `input_ids` as they have already been computed.\n:param use_cache: If `True`, return key and value tensors for each block.\n:param last_logits_only: If `True`, only compute the logits for the last token of each sequence.\n    This can speed up decoding when you only care about the next token.", "calls": ["emb_drop", "ln_f", "LLaDAOutput", "size", "size", "wte", "unsqueeze", "wpe", "to", "enumerate", "enumerate", "unsqueeze", "append", "linear", "ff_out", "mul_", "len", "size", "view", "ensure_finite_", "block_group", "arange", "finfo", "get_causal_attention_bias", "get_alibi_attention_bias", "get_causal_attention_bias", "append", "_activation_checkpoint_fn", "block", "append"], "decorators": [], "num_calls": 38}
{"symbol_id": "llada.model.modeling_llada::__init__", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "__init__", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1563, "end_line": 1572, "docstring": null, "calls": ["__init__", "create_model_config_from_pretrained_config", "LLaDAModel", "super"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::forward", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "forward", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1574, "end_line": 1623, "docstring": null, "calls": ["forward", "CausalLMOutputWithPast", "ValueError", "warn"], "decorators": [], "num_calls": 4}
{"symbol_id": "llada.model.modeling_llada::can_generate", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "can_generate", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1625, "end_line": 1626, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::prepare_inputs_for_generation", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "prepare_inputs_for_generation", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1628, "end_line": 1638, "docstring": null, "calls": ["update", "pop"], "decorators": [], "num_calls": 2}
{"symbol_id": "llada.model.modeling_llada::get_input_embeddings", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "get_input_embeddings", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1641, "end_line": 1642, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::set_input_embeddings", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "set_input_embeddings", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1644, "end_line": 1645, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::get_output_embeddings", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "get_output_embeddings", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1647, "end_line": 1651, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::set_output_embeddings", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "set_output_embeddings", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1653, "end_line": 1657, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.modeling_llada::tie_weights", "path": "llada/model/modeling_llada.py", "module": "llada.model.modeling_llada", "name": "tie_weights", "kind": "Function", "parent": "LLaDAModelLM", "start_line": 1659, "end_line": 1661, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::StrEnum", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "StrEnum", "kind": "Class", "parent": null, "start_line": 55, "end_line": 65, "docstring": "This is equivalent to Python's :class:`enum.StrEnum` since version 3.11.\nWe include this here for compatibility with older version of Python.", "calls": ["str"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.configuration_llada::LayerNormType", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "LayerNormType", "kind": "Class", "parent": null, "start_line": 68, "end_line": 94, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::ActivationType", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "ActivationType", "kind": "Class", "parent": null, "start_line": 97, "end_line": 101, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::BlockType", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "BlockType", "kind": "Class", "parent": null, "start_line": 104, "end_line": 112, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::InitFnType", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "InitFnType", "kind": "Class", "parent": null, "start_line": 115, "end_line": 143, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::ModelConfig", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "ModelConfig", "kind": "Class", "parent": null, "start_line": 147, "end_line": 406, "docstring": "LLaDA (model) configuration.", "calls": ["Exception"], "decorators": ["dataclass"], "num_calls": 1}
{"symbol_id": "llada.model.configuration_llada::ActivationCheckpointingStrategy", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "ActivationCheckpointingStrategy", "kind": "Class", "parent": null, "start_line": 408, "end_line": 452, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::LLaDAConfig", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "LLaDAConfig", "kind": "Class", "parent": null, "start_line": 455, "end_line": 481, "docstring": null, "calls": ["ModelConfig", "update", "update", "update", "__init__", "get", "super"], "decorators": [], "num_calls": 7}
{"symbol_id": "llada.model.configuration_llada::__str__", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "__str__", "kind": "Function", "parent": "StrEnum", "start_line": 61, "end_line": 62, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::__repr__", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "__repr__", "kind": "Function", "parent": "StrEnum", "start_line": 64, "end_line": 65, "docstring": null, "calls": ["str"], "decorators": [], "num_calls": 1}
{"symbol_id": "llada.model.configuration_llada::effective_n_kv_heads", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "effective_n_kv_heads", "kind": "Function", "parent": "ModelConfig", "start_line": 388, "end_line": 406, "docstring": null, "calls": ["Exception"], "decorators": ["property"], "num_calls": 1}
{"symbol_id": "llada.model.configuration_llada::__init__", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "__init__", "kind": "Function", "parent": "LLaDAConfig", "start_line": 459, "end_line": 469, "docstring": null, "calls": ["ModelConfig", "update", "update", "update", "__init__", "get", "super"], "decorators": [], "num_calls": 7}
{"symbol_id": "llada.model.configuration_llada::num_attention_heads", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "num_attention_heads", "kind": "Function", "parent": "LLaDAConfig", "start_line": 472, "end_line": 473, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::num_hidden_layers", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "num_hidden_layers", "kind": "Function", "parent": "LLaDAConfig", "start_line": 476, "end_line": 477, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "llada.model.configuration_llada::hidden_size", "path": "llada/model/configuration_llada.py", "module": "llada.model.configuration_llada", "name": "hidden_size", "kind": "Function", "parent": "LLaDAConfig", "start_line": 480, "end_line": 481, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "dream.postprocess_code::pass_at_1", "path": "dream/postprocess_code.py", "module": "dream.postprocess_code", "name": "pass_at_1", "kind": "Function", "parent": null, "start_line": 26, "end_line": 31, "docstring": null, "calls": ["compute"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.postprocess_code::read_jsonl", "path": "dream/postprocess_code.py", "module": "dream.postprocess_code", "name": "read_jsonl", "kind": "Function", "parent": null, "start_line": 36, "end_line": 41, "docstring": null, "calls": ["open", "append", "loads"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.postprocess_code::write_jsonl", "path": "dream/postprocess_code.py", "module": "dream.postprocess_code", "name": "write_jsonl", "kind": "Function", "parent": null, "start_line": 55, "end_line": 58, "docstring": null, "calls": ["open", "write", "dumps"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.demo_multiturn_chat::generation_tokens_hook_func", "path": "dream/demo_multiturn_chat.py", "module": "dream.demo_multiturn_chat", "name": "generation_tokens_hook_func", "kind": "Function", "parent": null, "start_line": 71, "end_line": 76, "docstring": null, "calls": ["print", "print", "sleep", "replace", "split", "decode", "tolist"], "decorators": [], "num_calls": 7}
{"symbol_id": "dream.eval::Dream", "path": "dream/eval.py", "module": "dream.eval", "name": "Dream", "kind": "Class", "parent": null, "start_line": 51, "end_line": 603, "docstring": null, "calls": ["register_model", "no_grad", "no_grad", "no_grad", "__init__", "isinstance", "isinstance", "isinstance", "device_count", "InitProcessGroupKwargs", "Accelerator", "isinstance", "_create_model_and_tokenizer", "isinstance", "to", "MethodType", "MethodType", "from_pretrained", "decode", "simple_parse_args_string", "cls", "apply_chat_template", "replace", "ne", "to", "to", "diffusion_generate", "item", "print", "print"], "decorators": [null], "num_calls": 219}
{"symbol_id": "dream.eval::__init__", "path": "dream/eval.py", "module": "dream.eval", "name": "__init__", "kind": "Function", "parent": "Dream", "start_line": 52, "end_line": 213, "docstring": null, "calls": ["__init__", "isinstance", "isinstance", "isinstance", "device_count", "InitProcessGroupKwargs", "Accelerator", "isinstance", "_create_model_and_tokenizer", "isinstance", "device_count", "set", "int", "warning", "super", "timedelta", "device", "info", "info", "info", "info", "hasattr", "device", "RuntimeError", "is_available", "device", "device", "str", "device", "parse"], "decorators": [], "num_calls": 40}
{"symbol_id": "dream.eval::batch_size", "path": "dream/eval.py", "module": "dream.eval", "name": "batch_size", "kind": "Function", "parent": "Dream", "start_line": 215, "end_line": 216, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "dream.eval::device", "path": "dream/eval.py", "module": "dream.eval", "name": "device", "kind": "Function", "parent": "Dream", "start_line": 219, "end_line": 220, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "dream.eval::rank", "path": "dream/eval.py", "module": "dream.eval", "name": "rank", "kind": "Function", "parent": "Dream", "start_line": 223, "end_line": 224, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "dream.eval::world_size", "path": "dream/eval.py", "module": "dream.eval", "name": "world_size", "kind": "Function", "parent": "Dream", "start_line": 227, "end_line": 228, "docstring": null, "calls": [], "decorators": ["property"], "num_calls": 0}
{"symbol_id": "dream.eval::_create_model_and_tokenizer", "path": "dream/eval.py", "module": "dream.eval", "name": "_create_model_and_tokenizer", "kind": "Function", "parent": "Dream", "start_line": 230, "end_line": 244, "docstring": null, "calls": ["to", "MethodType", "MethodType", "from_pretrained", "eval", "from_pretrained", "get_dtype"], "decorators": [], "num_calls": 7}
{"symbol_id": "dream.eval::tok_decode", "path": "dream/eval.py", "module": "dream.eval", "name": "tok_decode", "kind": "Function", "parent": "Dream", "start_line": 246, "end_line": 247, "docstring": null, "calls": ["decode"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.eval::tok_encode", "path": "dream/eval.py", "module": "dream.eval", "name": "tok_encode", "kind": "Function", "parent": "Dream", "start_line": 249, "end_line": 252, "docstring": null, "calls": ["tokenizer"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.eval::create_from_arg_string", "path": "dream/eval.py", "module": "dream.eval", "name": "create_from_arg_string", "kind": "Function", "parent": "Dream", "start_line": 254, "end_line": 270, "docstring": "Creates an instance of the LM class using the given argument string and additional config.\n\nParameters:\n- arg_string: A string containing arguments in the format key1=value1,key2=value2.\n- additional_config: Optional dictionary containing additional configuration parameters.\n\nReturns:\n- Instance of the LM class.", "calls": ["simple_parse_args_string", "cls", "items"], "decorators": ["classmethod"], "num_calls": 3}
{"symbol_id": "dream.eval::apply_chat_template", "path": "dream/eval.py", "module": "dream.eval", "name": "apply_chat_template", "kind": "Function", "parent": "Dream", "start_line": 272, "end_line": 285, "docstring": "Method to apply a chat template to a list of chat history between user and model.", "calls": ["apply_chat_template"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.eval::tokenizer_name", "path": "dream/eval.py", "module": "dream.eval", "name": "tokenizer_name", "kind": "Function", "parent": "Dream", "start_line": 288, "end_line": 289, "docstring": null, "calls": ["replace"], "decorators": ["property"], "num_calls": 1}
{"symbol_id": "dream.eval::_generate_batch", "path": "dream/eval.py", "module": "dream.eval", "name": "_generate_batch", "kind": "Function", "parent": "Dream", "start_line": 291, "end_line": 335, "docstring": null, "calls": ["ne", "to", "to", "diffusion_generate", "item", "print", "print", "print", "print", "print", "tokenizer", "len", "warning", "apply_chat_template", "sum", "split", "zip", "len", "decode", "tolist", "len"], "decorators": [], "num_calls": 21}
{"symbol_id": "dream.eval::generate_until", "path": "dream/eval.py", "module": "dream.eval", "name": "generate_until", "kind": "Function", "parent": "Dream", "start_line": 337, "end_line": 399, "docstring": null, "calls": ["tqdm", "time", "range", "time", "print", "print", "print", "MethodType", "MethodType", "MethodType", "MethodType", "makedirs", "join", "print", "exists", "len", "zip", "_generate_batch", "extend", "update", "print", "print", "len", "update", "enumerate", "len", "enumerate", "open", "len", "len"], "decorators": [], "num_calls": 35}
{"symbol_id": "dream.eval::_forward_process", "path": "dream/eval.py", "module": "dream.eval", "name": "_forward_process", "kind": "Function", "parent": "Dream", "start_line": 401, "end_line": 418, "docstring": null, "calls": ["rand", "float", "repeat", "where", "rand", "arange"], "decorators": [], "num_calls": 6}
{"symbol_id": "dream.eval::get_logits", "path": "dream/eval.py", "module": "dream.eval", "name": "get_logits", "kind": "Function", "parent": "Dream", "start_line": 421, "end_line": 442, "docstring": "prompt_index : 1D bool tensor, length=batch.shape[1]", "calls": ["no_grad", "repeat", "clone", "cat", "autocast", "cat", "chunk", "len", "model", "unsqueeze"], "decorators": [null], "num_calls": 10}
{"symbol_id": "dream.eval::_eval_target_nll_mc", "path": "dream/eval.py", "module": "dream.eval", "name": "_eval_target_nll_mc", "kind": "Function", "parent": "Dream", "start_line": 445, "end_line": 478, "docstring": null, "calls": ["no_grad", "to", "range", "max", "clone", "_forward_process", "get_logits", "append", "sum", "len", "concatenate", "repeat", "arange", "len", "arange", "len", "cross_entropy", "sum", "item", "NotImplementedError", "len", "len", "len", "len"], "decorators": [null], "num_calls": 24}
{"symbol_id": "dream.eval::_eval_target_nll_ar", "path": "dream/eval.py", "module": "dream.eval", "name": "_eval_target_nll_ar", "kind": "Function", "parent": "Dream", "start_line": 481, "end_line": 534, "docstring": null, "calls": ["no_grad", "ones", "range", "cat", "ones", "unsqueeze", "unsqueeze", "contiguous", "contiguous", "triu", "tril", "cat", "cat", "to", "get_logits", "append", "triu", "tril", "cat", "cat", "item", "item", "arange", "arange", "len", "len", "len", "unsqueeze", "cpu", "clone"], "decorators": [null], "num_calls": 44}
{"symbol_id": "dream.eval::_encode_pair", "path": "dream/eval.py", "module": "dream.eval", "name": "_encode_pair", "kind": "Function", "parent": "Dream", "start_line": 536, "end_line": 562, "docstring": null, "calls": ["encode", "len", "max", "len", "len", "encode", "warning", "rstrip", "len", "warning", "len"], "decorators": [], "num_calls": 11}
{"symbol_id": "dream.eval::loglikelihood", "path": "dream/eval.py", "module": "dream.eval", "name": "loglikelihood", "kind": "Function", "parent": "Dream", "start_line": 564, "end_line": 600, "docstring": null, "calls": ["from_list", "print", "map", "with_format", "_encode_pair", "no_grad", "tqdm", "append", "_eval_target_nll_mc", "NotImplementedError", "_eval_target_nll_ar", "len", "len"], "decorators": [], "num_calls": 13}
{"symbol_id": "dream.eval::loglikelihood_rolling", "path": "dream/eval.py", "module": "dream.eval", "name": "loglikelihood_rolling", "kind": "Function", "parent": "Dream", "start_line": 602, "end_line": 603, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.eval::_tokenize", "path": "dream/eval.py", "module": "dream.eval", "name": "_tokenize", "kind": "Function", "parent": "loglikelihood", "start_line": 565, "end_line": 572, "docstring": null, "calls": ["_encode_pair"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.sanitize::refine_text", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "refine_text", "kind": "Function", "parent": null, "start_line": 31, "end_line": 34, "docstring": null, "calls": ["replace", "replace", "strip", "replace"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.sanitize::syntax_check", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "syntax_check", "kind": "Function", "parent": null, "start_line": 36, "end_line": 43, "docstring": null, "calls": ["parse", "print_exc"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.sanitize::extract_longest_valid_code", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "extract_longest_valid_code", "kind": "Function", "parent": null, "start_line": 45, "end_line": 62, "docstring": null, "calls": ["splitlines", "range", "len", "len", "range", "len", "join", "syntax_check", "sum", "strip"], "decorators": [], "num_calls": 10}
{"symbol_id": "dream.sanitize::get_deps", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "get_deps", "kind": "Function", "parent": null, "start_line": 64, "end_line": 79, "docstring": null, "calls": ["set", "pop", "iter_child_nodes", "isinstance", "add", "isinstance", "add", "append"], "decorators": [], "num_calls": 8}
{"symbol_id": "dream.sanitize::get_function_dependency", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "get_function_dependency", "kind": "Function", "parent": null, "start_line": 81, "end_line": 91, "docstring": null, "calls": ["set", "pop", "add", "extend", "get", "set"], "decorators": [], "num_calls": 6}
{"symbol_id": "dream.sanitize::get_definition_name", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "get_definition_name", "kind": "Function", "parent": null, "start_line": 93, "end_line": 100, "docstring": null, "calls": ["isinstance", "isinstance", "isinstance"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.sanitize::has_return_statement", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "has_return_statement", "kind": "Function", "parent": null, "start_line": 102, "end_line": 103, "docstring": null, "calls": ["any", "isinstance", "walk"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.sanitize::sanitize", "path": "dream/sanitize.py", "module": "dream.sanitize", "name": "sanitize", "kind": "Function", "parent": null, "start_line": 105, "end_line": 146, "docstring": null, "calls": ["refine_text", "extract_longest_valid_code", "parse", "items", "join", "isinstance", "get_deps", "get_function_dependency", "append", "append", "isinstance", "unparse", "append", "isinstance", "unparse", "has_return_statement", "isinstance", "items", "get_definition_name"], "decorators": [], "num_calls": 19}
{"symbol_id": "dream.model.modeling_dream::BaseModelOutputWithPast", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "BaseModelOutputWithPast", "kind": "Class", "parent": null, "start_line": 57, "end_line": 60, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::MaskedLMOutputWithPastKeyValues", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "MaskedLMOutputWithPastKeyValues", "kind": "Class", "parent": null, "start_line": 63, "end_line": 66, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::DreamRMSNorm", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamRMSNorm", "kind": "Class", "parent": null, "start_line": 70, "end_line": 87, "docstring": null, "calls": ["__init__", "Parameter", "to", "mean", "ones", "rsqrt", "to", "super", "pow", "tuple"], "decorators": [], "num_calls": 10}
{"symbol_id": "dream.model.modeling_dream::DreamRotaryEmbedding", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamRotaryEmbedding", "kind": "Class", "parent": null, "start_line": 91, "end_line": 181, "docstring": null, "calls": ["no_grad", "__init__", "rope_init_fn", "register_buffer", "rope_init_fn", "register_buffer", "expand", "float", "warning_once", "max", "rope_init_fn", "register_buffer", "register_buffer", "_dynamic_frequency_update", "autocast", "transpose", "cat", "cos", "sin", "to", "to", "super", "get", "float", "isinstance", "get", "float", "float"], "decorators": [], "num_calls": 28}
{"symbol_id": "dream.model.modeling_dream::rotate_half", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "rotate_half", "kind": "Function", "parent": null, "start_line": 185, "end_line": 189, "docstring": "Rotates half the hidden dims of the input.", "calls": ["cat"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.model.modeling_dream::apply_rotary_pos_emb", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "apply_rotary_pos_emb", "kind": "Function", "parent": null, "start_line": 193, "end_line": 222, "docstring": "Applies Rotary Position Embedding to the query and key tensors.\n\nArgs:\n    q (`torch.Tensor`): The query tensor.\n    k (`torch.Tensor`): The key tensor.\n    cos (`torch.Tensor`): The cosine part of the rotary embedding.\n    sin (`torch.Tensor`): The sine part of the rotary embedding.\n    position_ids (`torch.Tensor`, *optional*):\n        Deprecated and unused.\n    unsqueeze_dim (`int`, *optional*, defaults to 1):\n        The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n        sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n        that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n        k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n        cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n        the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\nReturns:\n    `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.", "calls": ["unsqueeze", "unsqueeze", "rotate_half", "rotate_half", "rotate_half", "item", "item", "item", "item"], "decorators": [], "num_calls": 9}
{"symbol_id": "dream.model.modeling_dream::DreamMLP", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamMLP", "kind": "Class", "parent": null, "start_line": 226, "end_line": 237, "docstring": null, "calls": ["__init__", "Linear", "Linear", "Linear", "down_proj", "super", "act_fn", "up_proj", "gate_proj"], "decorators": [], "num_calls": 9}
{"symbol_id": "dream.model.modeling_dream::repeat_kv", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "repeat_kv", "kind": "Function", "parent": null, "start_line": 241, "end_line": 250, "docstring": "This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\nnum_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)", "calls": ["expand", "reshape"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::DreamAttention", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamAttention", "kind": "Class", "parent": null, "start_line": 253, "end_line": 357, "docstring": "Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\nand \"Generating Long Sequences with Sparse Transformers\".", "calls": ["__init__", "Linear", "Linear", "Linear", "Linear", "DreamRotaryEmbedding", "size", "q_proj", "k_proj", "v_proj", "transpose", "transpose", "transpose", "apply_rotary_pos_emb", "repeat_kv", "repeat_kv", "to", "dropout", "matmul", "contiguous", "reshape", "o_proj", "warning_once", "ValueError", "warning_once", "rotary_emb", "update", "matmul", "sqrt", "size"], "decorators": [], "num_calls": 39}
{"symbol_id": "dream.model.modeling_dream::DreamSdpaAttention", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamSdpaAttention", "kind": "Class", "parent": null, "start_line": 360, "end_line": 470, "docstring": "Dream attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n`DreamAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\nSDPA API.", "calls": ["size", "q_proj", "k_proj", "v_proj", "transpose", "transpose", "transpose", "repeat_kv", "repeat_kv", "scaled_dot_product_attention", "contiguous", "view", "o_proj", "warning_once", "forward", "warning_once", "rotary_emb", "apply_rotary_pos_emb", "apply_rotary_pos_emb", "contiguous", "contiguous", "contiguous", "cat", "cat", "view", "view", "view", "transpose", "super", "nonzero"], "decorators": [], "num_calls": 32}
{"symbol_id": "dream.model.modeling_dream::DreamDecoderLayer", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamDecoderLayer", "kind": "Class", "parent": null, "start_line": 473, "end_line": 560, "docstring": null, "calls": ["__init__", "DreamSdpaAttention", "DreamMLP", "DreamRMSNorm", "DreamRMSNorm", "input_layernorm", "self_attn", "post_attention_layernorm", "mlp", "warning_once", "super"], "decorators": [], "num_calls": 11}
{"symbol_id": "dream.model.modeling_dream::DreamPreTrainedModel", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamPreTrainedModel", "kind": "Class", "parent": null, "start_line": 562, "end_line": 636, "docstring": null, "calls": ["isinstance", "from_pretrained", "get", "get", "get", "get", "get", "from_pretrained", "normal_", "isinstance", "zero_", "normal_", "super", "zero_"], "decorators": [], "num_calls": 14}
{"symbol_id": "dream.model.modeling_dream::DreamBaseModel", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamBaseModel", "kind": "Class", "parent": null, "start_line": 638, "end_line": 788, "docstring": "Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DreamDecoderLayer`]\n\nArgs:\n    config: DreamConfig", "calls": ["__init__", "Embedding", "ModuleList", "DreamRMSNorm", "DreamRotaryEmbedding", "post_init", "rotary_emb", "enumerate", "norm", "BaseModelOutputWithPast", "ValueError", "embed_tokens", "unsqueeze", "tuple", "super", "DreamDecoderLayer", "warning_once", "unsqueeze", "unsqueeze", "_gradient_checkpointing_func", "decoder_layer", "append", "range", "arange", "arange", "arange"], "decorators": [], "num_calls": 26}
{"symbol_id": "dream.model.modeling_dream::DreamModel", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "DreamModel", "kind": "Class", "parent": null, "start_line": 791, "end_line": 883, "docstring": null, "calls": ["__init__", "DreamBaseModel", "Linear", "post_init", "reset_parameters", "model", "lm_head", "MaskedLMOutputWithPastKeyValues", "reset_parameters", "loss_function", "super"], "decorators": [], "num_calls": 11}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "BaseModelOutputWithPast", "start_line": 58, "end_line": 60, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "MaskedLMOutputWithPastKeyValues", "start_line": 64, "end_line": 66, "docstring": null, "calls": ["__init__", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamRMSNorm", "start_line": 71, "end_line": 77, "docstring": "DreamRMSNorm is equivalent to T5LayerNorm", "calls": ["__init__", "Parameter", "ones", "super"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamRMSNorm", "start_line": 79, "end_line": 84, "docstring": null, "calls": ["to", "mean", "rsqrt", "to", "pow"], "decorators": [], "num_calls": 5}
{"symbol_id": "dream.model.modeling_dream::extra_repr", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "extra_repr", "kind": "Function", "parent": "DreamRMSNorm", "start_line": 86, "end_line": 87, "docstring": null, "calls": ["tuple"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamRotaryEmbedding", "start_line": 92, "end_line": 134, "docstring": null, "calls": ["__init__", "rope_init_fn", "register_buffer", "warning_once", "super", "get", "get"], "decorators": [], "num_calls": 7}
{"symbol_id": "dream.model.modeling_dream::reset_parameters", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "reset_parameters", "kind": "Function", "parent": "DreamRotaryEmbedding", "start_line": 136, "end_line": 139, "docstring": null, "calls": ["rope_init_fn", "register_buffer"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::_dynamic_frequency_update", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "_dynamic_frequency_update", "kind": "Function", "parent": "DreamRotaryEmbedding", "start_line": 142, "end_line": 158, "docstring": "dynamic RoPE layers should recompute `inv_freq` in the following situations:\n1 - growing beyond the cached sequence length (allow scaling)\n2 - the current sequence length is in the original scale (avoid losing precision with small sequences)", "calls": ["max", "rope_init_fn", "register_buffer", "register_buffer"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamRotaryEmbedding", "start_line": 161, "end_line": 181, "docstring": null, "calls": ["no_grad", "expand", "float", "_dynamic_frequency_update", "autocast", "transpose", "cat", "cos", "sin", "to", "to", "float", "isinstance", "float", "float"], "decorators": [null], "num_calls": 15}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamMLP", "start_line": 227, "end_line": 234, "docstring": null, "calls": ["__init__", "Linear", "Linear", "Linear", "super"], "decorators": [], "num_calls": 5}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamMLP", "start_line": 236, "end_line": 237, "docstring": null, "calls": ["down_proj", "act_fn", "up_proj", "gate_proj"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamAttention", "start_line": 259, "end_line": 290, "docstring": null, "calls": ["__init__", "Linear", "Linear", "Linear", "Linear", "DreamRotaryEmbedding", "warning_once", "ValueError", "super"], "decorators": [], "num_calls": 9}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamAttention", "start_line": 292, "end_line": 357, "docstring": null, "calls": ["size", "q_proj", "k_proj", "v_proj", "transpose", "transpose", "transpose", "apply_rotary_pos_emb", "repeat_kv", "repeat_kv", "to", "dropout", "matmul", "contiguous", "reshape", "o_proj", "warning_once", "rotary_emb", "update", "matmul", "sqrt", "size", "ValueError", "view", "view", "view", "transpose", "softmax", "transpose", "size"], "decorators": [], "num_calls": 30}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamSdpaAttention", "start_line": 368, "end_line": 470, "docstring": null, "calls": ["size", "q_proj", "k_proj", "v_proj", "transpose", "transpose", "transpose", "repeat_kv", "repeat_kv", "scaled_dot_product_attention", "contiguous", "view", "o_proj", "warning_once", "forward", "warning_once", "rotary_emb", "apply_rotary_pos_emb", "apply_rotary_pos_emb", "contiguous", "contiguous", "contiguous", "cat", "cat", "view", "view", "view", "transpose", "super", "nonzero"], "decorators": [], "num_calls": 32}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamDecoderLayer", "start_line": 474, "end_line": 489, "docstring": null, "calls": ["__init__", "DreamSdpaAttention", "DreamMLP", "DreamRMSNorm", "DreamRMSNorm", "warning_once", "super"], "decorators": [], "num_calls": 7}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamDecoderLayer", "start_line": 491, "end_line": 560, "docstring": "Args:\n    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n    attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n        `(batch, sequence_length)` where padding elements are indicated by 0.\n    output_attentions (`bool`, *optional*):\n        Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n        returned tensors for more detail.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n        (see `past_key_values`).\n    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n    cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n        Indices depicting the position of the input sequence tokens in the sequence.\n    position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n        Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n        with `head_dim` being the embedding dimension of each attention head.\n    kwargs (`dict`, *optional*):\n        Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n        into the model", "calls": ["input_layernorm", "self_attn", "post_attention_layernorm", "mlp"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.modeling_dream::_init_weights", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "_init_weights", "kind": "Function", "parent": "DreamPreTrainedModel", "start_line": 574, "end_line": 583, "docstring": null, "calls": ["isinstance", "normal_", "isinstance", "zero_", "normal_", "zero_"], "decorators": [], "num_calls": 6}
{"symbol_id": "dream.model.modeling_dream::from_pretrained", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "from_pretrained", "kind": "Function", "parent": "DreamPreTrainedModel", "start_line": 586, "end_line": 636, "docstring": null, "calls": ["from_pretrained", "get", "get", "get", "get", "get", "from_pretrained", "super"], "decorators": ["classmethod"], "num_calls": 8}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamBaseModel", "start_line": 646, "end_line": 661, "docstring": null, "calls": ["__init__", "Embedding", "ModuleList", "DreamRMSNorm", "DreamRotaryEmbedding", "post_init", "super", "DreamDecoderLayer", "range"], "decorators": [], "num_calls": 9}
{"symbol_id": "dream.model.modeling_dream::get_input_embeddings", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "get_input_embeddings", "kind": "Function", "parent": "DreamBaseModel", "start_line": 663, "end_line": 664, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::set_input_embeddings", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "set_input_embeddings", "kind": "Function", "parent": "DreamBaseModel", "start_line": 666, "end_line": 667, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamBaseModel", "start_line": 669, "end_line": 788, "docstring": null, "calls": ["rotary_emb", "enumerate", "norm", "BaseModelOutputWithPast", "ValueError", "embed_tokens", "unsqueeze", "tuple", "warning_once", "unsqueeze", "unsqueeze", "_gradient_checkpointing_func", "decoder_layer", "append", "arange", "arange", "arange"], "decorators": [], "num_calls": 17}
{"symbol_id": "dream.model.modeling_dream::__init__", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "__init__", "kind": "Function", "parent": "DreamModel", "start_line": 794, "end_line": 801, "docstring": null, "calls": ["__init__", "DreamBaseModel", "Linear", "post_init", "super"], "decorators": [], "num_calls": 5}
{"symbol_id": "dream.model.modeling_dream::reset_rope_parameters", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "reset_rope_parameters", "kind": "Function", "parent": "DreamModel", "start_line": 803, "end_line": 806, "docstring": null, "calls": ["reset_parameters", "reset_parameters"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.modeling_dream::get_input_embeddings", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "get_input_embeddings", "kind": "Function", "parent": "DreamModel", "start_line": 808, "end_line": 809, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::set_input_embeddings", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "set_input_embeddings", "kind": "Function", "parent": "DreamModel", "start_line": 811, "end_line": 812, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::get_output_embeddings", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "get_output_embeddings", "kind": "Function", "parent": "DreamModel", "start_line": 814, "end_line": 815, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::set_output_embeddings", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "set_output_embeddings", "kind": "Function", "parent": "DreamModel", "start_line": 817, "end_line": 818, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::set_decoder", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "set_decoder", "kind": "Function", "parent": "DreamModel", "start_line": 820, "end_line": 821, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::get_decoder", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "get_decoder", "kind": "Function", "parent": "DreamModel", "start_line": 823, "end_line": 824, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.modeling_dream::forward", "path": "dream/model/modeling_dream.py", "module": "dream.model.modeling_dream", "name": "forward", "kind": "Function", "parent": "DreamModel", "start_line": 826, "end_line": 883, "docstring": null, "calls": ["model", "lm_head", "MaskedLMOutputWithPastKeyValues", "loss_function"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.generation_utils::top_p_logits", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "top_p_logits", "kind": "Function", "parent": null, "start_line": 41, "end_line": 52, "docstring": null, "calls": ["sort", "cumsum", "clone", "zeros_like", "scatter_", "masked_fill", "softmax", "finfo"], "decorators": [], "num_calls": 8}
{"symbol_id": "dream.model.generation_utils::top_k_logits", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "top_k_logits", "kind": "Function", "parent": null, "start_line": 54, "end_line": 59, "docstring": null, "calls": ["min", "masked_fill", "size", "finfo", "topk"], "decorators": [], "num_calls": 5}
{"symbol_id": "dream.model.generation_utils::sample_tokens", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "sample_tokens", "kind": "Function", "parent": null, "start_line": 62, "end_line": 94, "docstring": null, "calls": ["softmax", "top_p_logits", "top_k_logits", "max", "sort", "log", "sum", "sample", "squeeze", "max", "Categorical", "gather", "unsqueeze"], "decorators": [], "num_calls": 13}
{"symbol_id": "dream.model.generation_utils::DreamModelOutput", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "DreamModelOutput", "kind": "Class", "parent": null, "start_line": 98, "end_line": 100, "docstring": null, "calls": [], "decorators": ["dataclass"], "num_calls": 0}
{"symbol_id": "dream.model.generation_utils::DreamGenerationConfig", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "DreamGenerationConfig", "kind": "Class", "parent": null, "start_line": 103, "end_line": 151, "docstring": null, "calls": ["pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "validate", "items", "setattr", "error"], "decorators": [], "num_calls": 24}
{"symbol_id": "dream.model.generation_utils::DreamGenerationMixin", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "DreamGenerationMixin", "kind": "Class", "parent": null, "start_line": 153, "end_line": 508, "docstring": null, "calls": ["no_grad", "is_torchdynamo_compiling", "_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "_prepare_generation_config", "pop", "pop", "pop", "_prepare_special_tokens", "_prepare_generated_length", "_validate_generated_length", "_expand_inputs_for_generation", "get", "_sample", "time", "pad", "linspace", "generation_tokens_hook_func", "print", "time", "print", "repeat_interleave", "repeat_interleave", "warn", "ValueError", "from_model_config", "is_torchdynamo_compiling", "deepcopy"], "decorators": [], "num_calls": 96}
{"symbol_id": "dream.model.generation_utils::__init__", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "__init__", "kind": "Function", "parent": "DreamGenerationConfig", "start_line": 104, "end_line": 148, "docstring": null, "calls": ["pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "validate", "items", "setattr", "error"], "decorators": [], "num_calls": 24}
{"symbol_id": "dream.model.generation_utils::validate", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "validate", "kind": "Function", "parent": "DreamGenerationConfig", "start_line": 150, "end_line": 151, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.generation_utils::_expand_inputs_for_generation", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_expand_inputs_for_generation", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 155, "end_line": 169, "docstring": "Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]", "calls": ["repeat_interleave", "repeat_interleave"], "decorators": ["staticmethod"], "num_calls": 2}
{"symbol_id": "dream.model.generation_utils::_validate_generated_length", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_validate_generated_length", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 171, "end_line": 193, "docstring": "Performs validation related to the resulting generated length", "calls": ["is_torchdynamo_compiling", "warn", "ValueError"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.model.generation_utils::_prepare_generated_length", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_prepare_generated_length", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 195, "end_line": 220, "docstring": "Prepared max and min length in generation configs to avoid clashes between similar attributes", "calls": ["warning", "getattr", "DreamGenerationConfig", "min"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.generation_utils::_prepare_generation_config", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_prepare_generation_config", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 222, "end_line": 252, "docstring": "Prepares the base generation config, then applies any generation configuration options from kwargs. This\nfunction handles retrocompatibility with respect to configuration files.", "calls": ["from_model_config", "is_torchdynamo_compiling", "deepcopy", "update"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.generation_utils::_prepare_special_tokens", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_prepare_special_tokens", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 254, "end_line": 298, "docstring": "Prepares the special tokens for generation, overwriting the generation config with their processed versions\nconverted to tensor.\nNote that `generation_config` is changed in place and stops being serializable after this method is called.\nThat is no problem if called within `generate` (`generation_config` is a local copy that doesn't leave the\nfunction). However, if called outside `generate`, consider creating a copy of `generation_config` first.", "calls": ["_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "isinstance", "tensor", "unsqueeze", "warning", "to"], "decorators": [], "num_calls": 9}
{"symbol_id": "dream.model.generation_utils::diffusion_generate", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "diffusion_generate", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 301, "end_line": 367, "docstring": null, "calls": ["no_grad", "_prepare_generation_config", "pop", "pop", "pop", "_prepare_special_tokens", "_prepare_generated_length", "_validate_generated_length", "_expand_inputs_for_generation", "get", "_sample", "warn", "hasattr", "any", "warn", "get", "is_torchdynamo_compiling"], "decorators": [null], "num_calls": 17}
{"symbol_id": "dream.model.generation_utils::_sample", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_sample", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 369, "end_line": 508, "docstring": null, "calls": ["time", "pad", "linspace", "generation_tokens_hook_func", "print", "time", "print", "any", "pad", "masked_fill_", "logical_and", "cat", "generation_logits_hook_func", "generation_tokens_hook_func", "DreamModelOutput", "cumsum", "unsqueeze", "unsqueeze", "item", "self", "sample_tokens", "clone", "append", "sum", "zeros_like", "rand", "sample_tokens", "clone", "full_like", "topk"], "decorators": [], "num_calls": 57}
{"symbol_id": "dream.model.generation_utils::_tensor_or_none", "path": "dream/model/generation_utils.py", "module": "dream.model.generation_utils", "name": "_tensor_or_none", "kind": "Function", "parent": "_prepare_special_tokens", "start_line": 268, "end_line": 275, "docstring": null, "calls": ["isinstance", "tensor", "to"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.model.tokenization_dream::bytes_to_unicode", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "bytes_to_unicode", "kind": "Function", "parent": null, "start_line": 48, "end_line": 69, "docstring": "Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\ncharacters the bpe code barfs on.\n\nThe reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\nif you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\ndecent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\ntables between utf-8 bytes and unicode strings.", "calls": ["lru_cache", "range", "dict", "list", "chr", "zip", "list", "list", "range", "append", "append", "range", "range", "ord", "ord", "ord", "ord", "ord", "ord"], "decorators": [null], "num_calls": 19}
{"symbol_id": "dream.model.tokenization_dream::get_pairs", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "get_pairs", "kind": "Function", "parent": null, "start_line": 73, "end_line": 84, "docstring": "Return set of symbol pairs in a word.\n\nWord is represented as tuple of symbols (symbols being variable-length strings).", "calls": ["set", "add"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.tokenization_dream::DreamTokenizer", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "DreamTokenizer", "kind": "Class", "parent": null, "start_line": 87, "end_line": 343, "docstring": "Construct a Dream tokenizer. Based on byte-level Byte-Pair-Encoding.\n\nSame with GPT2Tokenizer, this tokenizer has been trained to treat spaces like parts of the tokens so a word will\nbe encoded differently whether it is at the beginning of the sentence (without space) or not:\n\n```python\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Dream-org/Dream-v0-Base-7B\", trust_remote_code=True)\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[9707, 1879]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[21927, 1879]\n```\nThis is expected.\n\nYou should not use GPT2Tokenizer instead, because of the different pretokenization rules.\n\nThis tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\nthis superclass for more information regarding those methods.\n\nArgs:\n    vocab_file (`str`):\n        Path to the vocabulary file.\n    merges_file (`str`):\n        Path to the merges file.\n    errors (`str`, *optional*, defaults to `\"replace\"`):\n        Paradigm to follow when decoding bytes to UTF-8. See\n        [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n    unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n        token instead.\n    bos_token (`str`, *optional*):\n        The beginning of sequence token. Not applicable for this tokenizer.\n    eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n        The end of sequence token.\n    pad_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n        The token used for padding, for example when batching sequences of different lengths.\n    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n        Whether or not the model should cleanup the spaces that were added when splitting the input text during the\n        tokenization process. Not applicable to this tokenizer, since tokenization does not add spaces.\n    split_special_tokens (`bool`, *optional*, defaults to `False`):\n        Whether or not the special tokens should be split during the tokenization process. The default behavior is\n        to not split special tokens. This means that if `<|endoftext|>` is the `eos_token`, then `tokenizer.tokenize(\"<|endoftext|>\") =\n        ['<|endoftext|>`]. Otherwise, if `split_special_tokens=True`, then `tokenizer.tokenize(\"<|endoftext|>\")` will be give `['<',\n        '|', 'endo', 'ft', 'ext', '|', '>']`. This argument is only supported for `slow` tokenizers for the moment.", "calls": ["bytes_to_unicode", "dict", "compile", "get", "__init__", "len", "dict", "tuple", "get_pairs", "join", "findall", "get", "get", "join", "decode", "decode", "join", "join", "normalize", "isinstance", "AddedToken", "isinstance", "AddedToken", "isinstance", "AddedToken", "isinstance", "AddedToken", "open", "load", "open"], "decorators": [], "num_calls": 76}
{"symbol_id": "dream.model.tokenization_dream::__init__", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "__init__", "kind": "Function", "parent": "DreamTokenizer", "start_line": 141, "end_line": 212, "docstring": null, "calls": ["bytes_to_unicode", "dict", "compile", "get", "__init__", "isinstance", "AddedToken", "isinstance", "AddedToken", "isinstance", "AddedToken", "isinstance", "AddedToken", "open", "load", "open", "enumerate", "zip", "warning_once", "items", "items", "strip", "append", "range", "super", "tuple", "len", "startswith", "split"], "decorators": [], "num_calls": 29}
{"symbol_id": "dream.model.tokenization_dream::vocab_size", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "vocab_size", "kind": "Function", "parent": "DreamTokenizer", "start_line": 215, "end_line": 216, "docstring": null, "calls": ["len"], "decorators": ["property"], "num_calls": 1}
{"symbol_id": "dream.model.tokenization_dream::get_vocab", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "get_vocab", "kind": "Function", "parent": "DreamTokenizer", "start_line": 219, "end_line": 220, "docstring": null, "calls": ["dict"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.model.tokenization_dream::bpe", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "bpe", "kind": "Function", "parent": "DreamTokenizer", "start_line": 223, "end_line": 263, "docstring": null, "calls": ["tuple", "get_pairs", "join", "min", "tuple", "len", "len", "get_pairs", "index", "extend", "append", "append", "get", "extend", "float", "len"], "decorators": [], "num_calls": 16}
{"symbol_id": "dream.model.tokenization_dream::_tokenize", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "_tokenize", "kind": "Function", "parent": "DreamTokenizer", "start_line": 266, "end_line": 274, "docstring": "Tokenize a string.", "calls": ["findall", "join", "extend", "encode", "split", "bpe"], "decorators": [], "num_calls": 6}
{"symbol_id": "dream.model.tokenization_dream::_convert_token_to_id", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "_convert_token_to_id", "kind": "Function", "parent": "DreamTokenizer", "start_line": 277, "end_line": 279, "docstring": "Converts a token (str) in an id using the vocab.", "calls": ["get", "get"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.tokenization_dream::_convert_id_to_token", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "_convert_id_to_token", "kind": "Function", "parent": "DreamTokenizer", "start_line": 282, "end_line": 284, "docstring": "Converts an index (integer) in a token (str) using the vocab.", "calls": ["get"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.model.tokenization_dream::convert_tokens_to_string", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "convert_tokens_to_string", "kind": "Function", "parent": "DreamTokenizer", "start_line": 287, "end_line": 291, "docstring": "Converts a sequence of tokens (string) in a single string.", "calls": ["join", "decode", "bytearray"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.model.tokenization_dream::decode", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "decode", "kind": "Function", "parent": "DreamTokenizer", "start_line": 293, "end_line": 309, "docstring": null, "calls": ["decode", "super"], "decorators": [], "num_calls": 2}
{"symbol_id": "dream.model.tokenization_dream::save_vocabulary", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "save_vocabulary", "kind": "Function", "parent": "DreamTokenizer", "start_line": 312, "end_line": 339, "docstring": null, "calls": ["join", "join", "isdir", "error", "open", "write", "open", "write", "sorted", "items", "write", "dumps", "warning", "join"], "decorators": [], "num_calls": 14}
{"symbol_id": "dream.model.tokenization_dream::prepare_for_tokenization", "path": "dream/model/tokenization_dream.py", "module": "dream.model.tokenization_dream", "name": "prepare_for_tokenization", "kind": "Function", "parent": "DreamTokenizer", "start_line": 341, "end_line": 343, "docstring": null, "calls": ["normalize"], "decorators": [], "num_calls": 1}
{"symbol_id": "dream.model.generation_utils_block::get_num_transfer_tokens", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "get_num_transfer_tokens", "kind": "Function", "parent": null, "start_line": 38, "end_line": 56, "docstring": "In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\nFurthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\nthe expected number of tokens transitioned at each step should be consistent.\n\nThis function is designed to precompute the number of tokens that need to be transitioned at each step.", "calls": ["sum", "range", "zeros", "size", "size"], "decorators": [], "num_calls": 5}
{"symbol_id": "dream.model.generation_utils_block::top_p_logits", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "top_p_logits", "kind": "Function", "parent": null, "start_line": 59, "end_line": 70, "docstring": null, "calls": ["sort", "cumsum", "clone", "zeros_like", "scatter_", "masked_fill", "softmax", "finfo"], "decorators": [], "num_calls": 8}
{"symbol_id": "dream.model.generation_utils_block::top_k_logits", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "top_k_logits", "kind": "Function", "parent": null, "start_line": 72, "end_line": 77, "docstring": null, "calls": ["min", "masked_fill", "size", "finfo", "topk"], "decorators": [], "num_calls": 5}
{"symbol_id": "dream.model.generation_utils_block::sample_tokens", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "sample_tokens", "kind": "Function", "parent": null, "start_line": 80, "end_line": 112, "docstring": null, "calls": ["softmax", "top_p_logits", "top_k_logits", "max", "sort", "log", "sum", "sample", "squeeze", "max", "Categorical", "gather", "unsqueeze"], "decorators": [], "num_calls": 13}
{"symbol_id": "dream.model.generation_utils_block::DreamModelOutput", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "DreamModelOutput", "kind": "Class", "parent": null, "start_line": 116, "end_line": 118, "docstring": null, "calls": [], "decorators": ["dataclass"], "num_calls": 0}
{"symbol_id": "dream.model.generation_utils_block::DreamGenerationConfig", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "DreamGenerationConfig", "kind": "Class", "parent": null, "start_line": 121, "end_line": 169, "docstring": null, "calls": ["pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "validate", "items", "setattr", "error"], "decorators": [], "num_calls": 24}
{"symbol_id": "dream.model.generation_utils_block::DreamGenerationMixin", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "DreamGenerationMixin", "kind": "Class", "parent": null, "start_line": 171, "end_line": 571, "docstring": null, "calls": ["no_grad", "is_torchdynamo_compiling", "_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "_prepare_generation_config", "pop", "_prepare_special_tokens", "_prepare_generated_length", "_validate_generated_length", "_expand_inputs_for_generation", "get", "get", "get", "_sample", "pad", "linspace", "range", "repeat_interleave", "repeat_interleave", "warn", "ValueError", "from_model_config", "is_torchdynamo_compiling", "deepcopy", "update", "isinstance", "tensor", "unsqueeze"], "decorators": [], "num_calls": 93}
{"symbol_id": "dream.model.generation_utils_block::__init__", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "__init__", "kind": "Function", "parent": "DreamGenerationConfig", "start_line": 122, "end_line": 166, "docstring": null, "calls": ["pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "pop", "validate", "items", "setattr", "error"], "decorators": [], "num_calls": 24}
{"symbol_id": "dream.model.generation_utils_block::validate", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "validate", "kind": "Function", "parent": "DreamGenerationConfig", "start_line": 168, "end_line": 169, "docstring": null, "calls": [], "decorators": [], "num_calls": 0}
{"symbol_id": "dream.model.generation_utils_block::_expand_inputs_for_generation", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_expand_inputs_for_generation", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 173, "end_line": 187, "docstring": "Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]", "calls": ["repeat_interleave", "repeat_interleave"], "decorators": ["staticmethod"], "num_calls": 2}
{"symbol_id": "dream.model.generation_utils_block::_validate_generated_length", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_validate_generated_length", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 189, "end_line": 211, "docstring": "Performs validation related to the resulting generated length", "calls": ["is_torchdynamo_compiling", "warn", "ValueError"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.model.generation_utils_block::_prepare_generated_length", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_prepare_generated_length", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 213, "end_line": 238, "docstring": "Prepared max and min length in generation configs to avoid clashes between similar attributes", "calls": ["warning", "getattr", "DreamGenerationConfig", "min"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.generation_utils_block::_prepare_generation_config", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_prepare_generation_config", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 240, "end_line": 270, "docstring": "Prepares the base generation config, then applies any generation configuration options from kwargs. This\nfunction handles retrocompatibility with respect to configuration files.", "calls": ["from_model_config", "is_torchdynamo_compiling", "deepcopy", "update"], "decorators": [], "num_calls": 4}
{"symbol_id": "dream.model.generation_utils_block::_prepare_special_tokens", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_prepare_special_tokens", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 272, "end_line": 316, "docstring": "Prepares the special tokens for generation, overwriting the generation config with their processed versions\nconverted to tensor.\nNote that `generation_config` is changed in place and stops being serializable after this method is called.\nThat is no problem if called within `generate` (`generation_config` is a local copy that doesn't leave the\nfunction). However, if called outside `generate`, consider creating a copy of `generation_config` first.", "calls": ["_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "_tensor_or_none", "isinstance", "tensor", "unsqueeze", "warning", "to"], "decorators": [], "num_calls": 9}
{"symbol_id": "dream.model.generation_utils_block::diffusion_generate", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "diffusion_generate", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 319, "end_line": 385, "docstring": null, "calls": ["no_grad", "_prepare_generation_config", "pop", "_prepare_special_tokens", "_prepare_generated_length", "_validate_generated_length", "_expand_inputs_for_generation", "get", "get", "get", "_sample", "warn", "hasattr", "any", "warn", "get", "is_torchdynamo_compiling"], "decorators": [null], "num_calls": 17}
{"symbol_id": "dream.model.generation_utils_block::_sample", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_sample", "kind": "Function", "parent": "DreamGenerationMixin", "start_line": 387, "end_line": 571, "docstring": null, "calls": ["pad", "linspace", "range", "any", "pad", "masked_fill_", "logical_and", "self", "cat", "sample_tokens", "DreamModelOutput", "cumsum", "unsqueeze", "unsqueeze", "range", "zeros_like", "cat", "len", "append", "range", "self", "self", "sample_tokens", "clone", "sum", "topk", "zeros_like", "to", "range", "sample_tokens"], "decorators": [], "num_calls": 54}
{"symbol_id": "dream.model.generation_utils_block::_tensor_or_none", "path": "dream/model/generation_utils_block.py", "module": "dream.model.generation_utils_block", "name": "_tensor_or_none", "kind": "Function", "parent": "_prepare_special_tokens", "start_line": 286, "end_line": 293, "docstring": null, "calls": ["isinstance", "tensor", "to"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.model.configuration_dream::DreamConfig", "path": "dream/model/configuration_dream.py", "module": "dream.model.configuration_dream", "name": "DreamConfig", "kind": "Class", "parent": null, "start_line": 28, "end_line": 89, "docstring": null, "calls": ["rope_config_validation", "__init__", "super"], "decorators": [], "num_calls": 3}
{"symbol_id": "dream.model.configuration_dream::__init__", "path": "dream/model/configuration_dream.py", "module": "dream.model.configuration_dream", "name": "__init__", "kind": "Function", "parent": "DreamConfig", "start_line": 32, "end_line": 89, "docstring": null, "calls": ["rope_config_validation", "__init__", "super"], "decorators": [], "num_calls": 3}
