{
  "tasks": [
    {
      "task_title": "Basic crawl with markdown output",
      "task_description": "ä½¿ç”¨crawl4aiåº“è¿›è¡ŒåŸºæœ¬çš„ç½‘é¡µæŠ“å–ï¼Œå¹¶ä»¥markdownæ ¼å¼è¾“å‡ºç»“æžœã€‚",
      "example_code": null,
      "running_command": "crwl https://www.nbcnews.com/business -o markdown",
      "expected_input": "https://www.nbcnews.com/business",
      "expected_output": "æŠ“å–çš„å†…å®¹ä»¥markdownæ ¼å¼è¾“å‡º"
    },
    {
      "task_title": "Deep crawl with BFS strategy",
      "task_description": "ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ç­–ç•¥è¿›è¡Œæ·±åº¦æŠ“å–ï¼Œé™åˆ¶æœ€å¤§é¡µé¢æ•°ä¸º10ã€‚",
      "example_code": null,
      "running_command": "crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10",
      "expected_input": "https://docs.crawl4ai.com",
      "expected_output": "æŠ“å–çš„å†…å®¹ï¼Œæœ€å¤š10ä¸ªé¡µé¢"
    },
    {
      "task_title": "Use LLM extraction with a specific question",
      "task_description": "ä½¿ç”¨LLMæå–åŠŸèƒ½ï¼Œé’ˆå¯¹ç‰¹å®šé—®é¢˜ä»Žç½‘é¡µä¸­æå–ä¿¡æ¯ã€‚",
      "example_code": null,
      "running_command": "crwl https://www.example.com/products -q \"Extract all product prices\"",
      "expected_input": "https://www.example.com/products",
      "expected_output": "æå–çš„æ‰€æœ‰äº§å“ä»·æ ¼"
    },
    {
      "task_title": "Submit a crawl job via API",
      "task_description": "é€šè¿‡HTTP POSTè¯·æ±‚æäº¤æŠ“å–ä»»åŠ¡ï¼Œå¹¶èŽ·å–ç»“æžœã€‚",
      "example_code": "response = requests.post(\n    \"http://localhost:11235/crawl\",\n    json={\"urls\": [\"https://example.com\"], \"priority\": 10}\n)",
      "running_command": null,
      "expected_input": "{\"urls\": [\"https://example.com\"], \"priority\": 10}",
      "expected_output": "Crawl job submitted successfully."
    },
    {
      "task_title": "Crawl with custom extraction strategy",
      "task_description": "ä½¿ç”¨è‡ªå®šä¹‰çš„æå–ç­–ç•¥æŠ“å–ç½‘é¡µå†…å®¹ï¼Œå¹¶è¾“å‡ºæå–çš„å†…å®¹ã€‚",
      "example_code": "async def main():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    result = await crawler.arun(url=\"https://www.kidocode.com/degrees/technology\", config=run_config)",
      "running_command": null,
      "expected_input": "https://www.kidocode.com/degrees/technology",
      "expected_output": "Successfully extracted companies"
    },
    {
      "task_title": "Extract model fees using LLM",
      "task_description": "ä½¿ç”¨LLMæå–åŠŸèƒ½ä»Žç½‘é¡µä¸­æå–æ¨¡åž‹åç§°åŠå…¶è´¹ç”¨ã€‚",
      "example_code": "result = await crawler.arun(url='https://openai.com/api/pricing/', config=run_config)",
      "running_command": null,
      "expected_input": "https://openai.com/api/pricing/",
      "expected_output": "æå–çš„æ¨¡åž‹åŠè´¹ç”¨ä¿¡æ¯"
    },
    {
      "task_title": "Crawl with persistent user data",
      "task_description": "ä½¿ç”¨æŒä¹…ç”¨æˆ·æ•°æ®ç›®å½•è¿›è¡ŒæŠ“å–ï¼Œä»¥é¿å…ç™»å½•ç­‰æŒ‘æˆ˜ã€‚",
      "example_code": "async with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url, config=run_config, magic=True)",
      "running_command": null,
      "expected_input": "ADDRESS_OF_A_CHALLENGING_WEBSITE",
      "expected_output": "Successfully crawled {url}"
    },
    {
      "task_title": "Custom hooks for crawling",
      "task_description": "å®šä¹‰è‡ªå®šä¹‰é’©å­ä»¥ä¼˜åŒ–æŠ“å–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨Dockerå®¢æˆ·ç«¯è¿›è¡ŒæŠ“å–ã€‚",
      "example_code": "results = await client.crawl(\n    urls=[\"https://httpbin.org/html\"],\n    hooks={\n        \"on_page_context_created\": on_page_context_created,\n        \"before_goto\": before_goto\n    }\n)",
      "running_command": null,
      "expected_input": "https://httpbin.org/html",
      "expected_output": "æŠ“å–ç»“æžœ"
    },
    {
      "task_title": "Intelligent table extraction",
      "task_description": "ä½¿ç”¨LLMæå–ç­–ç•¥æ™ºèƒ½æå–ç½‘é¡µä¸­çš„è¡¨æ ¼æ•°æ®ã€‚",
      "example_code": "result = await crawler.arun(\"https://complex-tables-site.com\", config=config)",
      "running_command": null,
      "expected_input": "https://complex-tables-site.com",
      "expected_output": "Extracted table: {len(table['data'])} rows"
    },
    {
      "task_title": "Bypass bot detection",
      "task_description": "ä½¿ç”¨undetected Chromeé…ç½®æŠ“å–å—ä¿æŠ¤çš„ç½‘ç«™ï¼Œç»•è¿‡Cloudflareç­‰æ£€æµ‹ã€‚",
      "example_code": "result = await crawler.arun(\"https://protected-site.com\")",
      "running_command": null,
      "expected_input": "https://protected-site.com",
      "expected_output": "æˆåŠŸæŠ“å–å†…å®¹"
    },
    {
      "task_title": "Adaptive crawling",
      "task_description": "ä½¿ç”¨è‡ªé€‚åº”é…ç½®è¿›è¡ŒæŠ“å–ï¼Œå­¦ä¹ æ¨¡å¼ä»¥æé«˜æå–æ•ˆæžœã€‚",
      "example_code": "state = await adaptive_crawler.digest(start_url=\"https://news.example.com\", query=\"latest news content\")",
      "running_command": null,
      "expected_input": "https://news.example.com",
      "expected_output": "æŠ“å–çš„æœ€æ–°æ–°é—»å†…å®¹"
    },
    {
      "task_title": "Virtual scrolling",
      "task_description": "å®žçŽ°è™šæ‹Ÿæ»šåŠ¨ä»¥æŠ“å–åŠ¨æ€åŠ è½½çš„å†…å®¹ã€‚",
      "example_code": "result = await crawler.arun(url, config=CrawlerRunConfig(virtual_scroll_config=scroll_config))",
      "running_command": null,
      "expected_input": "åŠ¨æ€åŠ è½½çš„ç½‘é¡µURL",
      "expected_output": "æŠ“å–çš„åŠ¨æ€å†…å®¹"
    },
    {
      "task_title": "Link preview configuration",
      "task_description": "é…ç½®é“¾æŽ¥é¢„è§ˆï¼ŒæŒ‰ç›¸å…³æ€§å’Œè´¨é‡å¯¹é“¾æŽ¥è¿›è¡Œè¯„åˆ†ã€‚",
      "example_code": "result = await crawler.arun(url, config=CrawlerRunConfig(link_preview_config=link_config, score_links=True))",
      "running_command": null,
      "expected_input": "åŒ…å«é“¾æŽ¥çš„ç½‘é¡µURL",
      "expected_output": "é“¾æŽ¥çš„ç›¸å…³æ€§è¯„åˆ†"
    }
  ],
  "setup": {
    "setup_commands": [
      "# Install the package\npip install -U crawl4ai\n\n# For pre release versions\npip install crawl4ai --pre\n\n# Run post-installation setup\ncrawl4ai-setup\n\n# Verify your installation\ncrawl4ai-doctor",
      "pip install crawl4ai\ncrawl4ai-setup # Setup the browser",
      "pip install crawl4ai[sync]",
      "git clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\npip install -e .                    # Basic installation in editable mode",
      "pip install -e \".[torch]\"           # With PyTorch features\npip install -e \".[transformer]\"     # With Transformer features\npip install -e \".[cosine]\"          # With cosine similarity features\npip install -e \".[sync]\"            # With synchronous crawling (Selenium)\npip install -e \".[all]\"             # Install all optional features",
      "  seeder = AsyncUrlSeeder(SeedingConfig(\n      source=\"sitemap+cc\",\n      pattern=\"*/blog/*\",\n      query=\"python tutorials\",\n      score_threshold=0.4\n  ))\n  \n  urls = await seeder.discover(\"https://example.com\")",
      "  pip install -U crawl4ai",
      "  pip install crawl4ai --pre",
      "  pip install crawl4ai==0.4.3b1"
    ],
    "docker_commands": [
      "# Pull and run the latest release\ndocker pull unclecode/crawl4ai:latest\ndocker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:latest\n\n# Visit the playground at http://localhost:11235/playground"
    ],
    "docker_files": {
      "Dockerfile": "FROM python:3.12-slim-bookworm AS build\n\n# C4ai version\nARG C4AI_VER=0.7.6\nENV C4AI_VERSION=$C4AI_VER\nLABEL c4ai.version=$C4AI_VER\n\n# Set build arguments\nARG APP_HOME=/app\nARG GITHUB_REPO=https://github.com/unclecode/crawl4ai.git\nARG GITHUB_BRANCH=main\nARG USE_LOCAL=true\n\nENV PYTHONFAULTHANDLER=1 \\\n    PYTHONHASHSEED=random \\\n    PYTHONUNBUFFERED=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n    PIP_DEFAULT_TIMEOUT=100 \\\n    DEBIAN_FRONTEND=noninteractive \\\n    REDIS_HOST=localhost \\\n    REDIS_PORT=6379\n\nARG PYTHON_VERSION=3.12\nARG INSTALL_TYPE=default\nARG ENABLE_GPU=false\nARG TARGETARCH\n\nLABEL maintainer=\"unclecode\"\nLABEL description=\"ðŸ”¥ðŸ•·ï¸ Crawl4AI: Open-source LLM Friendly Web Crawler & scraper\"\nLABEL version=\"1.0\"\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    build-essential \\\n    curl \\\n    wget \\\n    gnupg \\\n    git \\\n    cmake \\\n    pkg-config \\\n    python3-dev \\\n    libjpeg-dev \\\n    redis-server \\\n    supervisor \\\n    && apt-get clean \\ \n    && rm -rf /var/lib/apt/lists/*\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    libglib2.0-0 \\\n    libnss3 \\\n    libnspr4 \\\n    libatk1.0-0 \\\n    libatk-bridge2.0-0 \\\n    libcups2 \\\n    libdrm2 \\\n    libdbus-1-3 \\\n    libxcb1 \\\n    libxkbcommon0 \\\n    libx11-6 \\\n    libxcomposite1 \\\n    libxdamage1 \\\n    libxext6 \\\n    libxfixes3 \\\n    libxrandr2 \\\n    libgbm1 \\\n    libpango-1.0-0 \\\n    libcairo2 \\\n    libasound2 \\\n    libatspi2.0-0 \\\n    && apt-get clean \\ \n    && rm -rf /var/lib/apt/lists/*\n\nRUN apt-get update && apt-get dist-upgrade -y \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN if [ \"$ENABLE_GPU\" = \"true\" ] && [ \"$TARGETARCH\" = \"amd64\" ] ; then \\\n    apt-get update && apt-get install -y --no-install-recommends \\\n    nvidia-cuda-toolkit \\\n    && apt-get clean \\ \n    && rm -rf /var/lib/apt/lists/* ; \\\nelse \\\n    echo \"Skipping NVIDIA CUDA Toolkit installation (unsupported platform or GPU disabled)\"; \\\nfi\n\nRUN if [ \"$TARGETARCH\" = \"arm64\" ]; then \\\n    echo \"ðŸ¦¾ Installing ARM-specific optimizations\"; \\\n    apt-get update && apt-get install -y --no-install-recommends \\\n    libopenblas-dev \\\n    && apt-get clean \\ \n    && rm -rf /var/lib/apt/lists/*; \\\nelif [ \"$TARGETARCH\" = \"amd64\" ]; then \\\n    echo \"ðŸ–¥ï¸ Installing AMD64-specific optimizations\"; \\\n    apt-get update && apt-get install -y --no-install-recommends \\\n    libomp-dev \\\n    && apt-get clean \\ \n    && rm -rf /var/lib/apt/lists/*; \\\nelse \\\n    echo \"Skipping platform-specific optimizations (unsupported platform)\"; \\\nfi\n\n# Create a non-root user and group\nRUN groupadd -r appuser && useradd --no-log-init -r -g appuser appuser\n\n# Create and set permissions for appuser home directory\nRUN mkdir -p /home/appuser && chown -R appuser:appuser /home/appuser\n\nWORKDIR ${APP_HOME}\n\nRUN echo '#!/bin/bash\\n\\\nif [ \"$USE_LOCAL\" = \"true\" ]; then\\n\\\n    echo \"ðŸ“¦ Installing from local source...\"\\n\\\n    pip install --no-cache-dir /tmp/project/\\n\\\nelse\\n\\\n    echo \"ðŸŒ Installing from GitHub...\"\\n\\\n    for i in {1..3}; do \\n\\\n        git clone --branch ${GITHUB_BRANCH} ${GITHUB_REPO} /tmp/crawl4ai && break || \\n\\\n        { echo \"Attempt $i/3 failed! Taking a short break... â˜•\"; sleep 5; }; \\n\\\n    done\\n\\\n    pip install --no-cache-dir /tmp/crawl4ai\\n\\\nfi' > /tmp/install.sh && chmod +x /tmp/install.sh\n\nCOPY . /tmp/project/\n\n# Copy supervisor config first (might need root later, but okay for now)\nCOPY deploy/docker/supervisord.conf .\n\nCOPY deploy/docker/requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nRUN if [ \"$INSTALL_TYPE\" = \"all\" ] ; then \\\n        pip install --no-cache-dir \\\n            torch \\\n            torchvision \\\n            torchaudio \\\n            scikit-learn \\\n            nltk \\\n            transformers \\\n            tokenizers && \\\n        python -m nltk.downloader punkt stopwords ; \\\n    fi\n\nRUN if [ \"$INSTALL_TYPE\" = \"all\" ] ; then \\\n        pip install \"/tmp/project/[all]\" && \\\n        python -m crawl4ai.model_loader ; \\\n    elif [ \"$INSTALL_TYPE\" = \"torch\" ] ; then \\\n        pip install \"/tmp/project/[torch]\" ; \\\n    elif [ \"$INSTALL_TYPE\" = \"transformer\" ] ; then \\\n        pip install \"/tmp/project/[transformer]\" && \\\n        python -m crawl4ai.model_loader ; \\\n    else \\\n        pip install \"/tmp/project\" ; \\\n    fi\n\nRUN pip install --no-cache-dir --upgrade pip && \\\n    /tmp/install.sh && \\\n    python -c \"import crawl4ai; print('âœ… crawl4ai is ready to rock!')\" && \\\n    python -c \"from playwright.sync_api import sync_playwright; print('âœ… Playwright is feeling dramatic!')\"\n\nRUN crawl4ai-setup\n\nRUN playwright install --with-deps\n\nRUN mkdir -p /home/appuser/.cache/ms-playwright \\\n    && cp -r /root/.cache/ms-playwright/chromium-* /home/appuser/.cache/ms-playwright/ \\\n    && chown -R appuser:appuser /home/appuser/.cache/ms-playwright\n\nRUN crawl4ai-doctor\n\n# Copy application code\nCOPY deploy/docker/* ${APP_HOME}/\n\n# copy the playground + any future static assets\nCOPY deploy/docker/static ${APP_HOME}/static\n\n# Change ownership of the application directory to the non-root user\nRUN chown -R appuser:appuser ${APP_HOME}\n\n# give permissions to redis persistence dirs if used\nRUN mkdir -p /var/lib/redis /var/log/redis && chown -R appuser:appuser /var/lib/redis /var/log/redis\n\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD bash -c '\\\n    MEM=$(free -m | awk \"/^Mem:/{print \\$2}\"); \\\n    if [ $MEM -lt 2048 ]; then \\\n        echo \"âš ï¸ Warning: Less than 2GB RAM available! Your container might need a memory boost! ðŸš€\"; \\\n        exit 1; \\\n    fi && \\\n    redis-cli ping > /dev/null && \\\n    curl -f http://localhost:11235/health || exit 1'\n\nEXPOSE 6379\n# Switch to the non-root user before starting the application\nUSER appuser\n\n# Set environment variables to ptoduction\nENV PYTHON_ENV=production \n\n# Start the application using supervisord\nCMD [\"supervisord\", \"-c\", \"supervisord.conf\"]",
      "docker-compose.yml": "version: '3.8'\n\n# Shared configuration for all environments\nx-base-config: &base-config\n  ports:\n    - \"11235:11235\"  # Gunicorn port\n  env_file:\n    - .llm.env       # API keys (create from .llm.env.example)\n  environment:\n    - OPENAI_API_KEY=${OPENAI_API_KEY:-}\n    - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}\n    - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}\n    - GROQ_API_KEY=${GROQ_API_KEY:-}\n    - TOGETHER_API_KEY=${TOGETHER_API_KEY:-}\n    - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}\n    - GEMINI_API_TOKEN=${GEMINI_API_TOKEN:-}\n    - LLM_PROVIDER=${LLM_PROVIDER:-}  # Optional: Override default provider (e.g., \"anthropic/claude-3-opus\")\n  volumes:\n    - /dev/shm:/dev/shm  # Chromium performance\n  deploy:\n    resources:\n      limits:\n        memory: 4G\n      reservations:\n        memory: 1G\n  restart: unless-stopped\n  healthcheck:\n    test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:11235/health\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n    start_period: 40s\n  user: \"appuser\"\n\nservices:\n  crawl4ai:\n    # 1. Default: Pull multi-platform test image from Docker Hub\n    # 2. Override with local image via: IMAGE=local-test docker compose up\n    image: ${IMAGE:-unclecode/crawl4ai:${TAG:-latest}}\n    \n    # Local build config (used with --build)\n    build:\n      context: .\n      dockerfile: Dockerfile\n      args:\n        INSTALL_TYPE: ${INSTALL_TYPE:-default}\n        ENABLE_GPU: ${ENABLE_GPU:-false}\n    \n    # Inherit shared config\n    <<: *base-config"
    },
    "docker_setup_descriptions": {
      "Dockerfile": "To build and run the Docker image for the \"Crawl4AI\" project from the repository `unclecode/crawl4ai`, follow these steps:\n\n### Building the Docker Image\n\n1. **Base Image**: The Docker image is built on top of a slim version of Python 3.12, ensuring a lightweight environment suitable for running Python applications.\n\n2. **Dependencies**: The image installs essential system packages and libraries required for the application, including:\n   - Development tools (e.g., `build-essential`, `git`, `cmake`)\n   - Libraries for image processing and web scraping (e.g., `libjpeg-dev`, `libglib2.0-0`, `libnss3`)\n   - Redis server for caching and data storage.\n   - Supervisor for process management.\n\n3. **Environment Variables**: Several environment variables are set to optimize the Python environment and configure the application, including settings for Redis connection and Python execution behavior.\n\n4. **User Creation**: A non-root user (`appuser`) is created for enhanced security, and the application is configured to run under this user.\n\n5. **Application Setup**: The image includes logic to install the application either from a local source or directly from the GitHub repository. It also installs necessary Python packages defined in a `requirements.txt` file, along with optional packages based on the installation type specified.\n\n6. **Health Check**: A health check is defined to ensure that the application has sufficient memory and that the Redis service is running.\n\n7. **Final Configuration**: The application is set to run using Supervisor, which manages the application processes and ensures they are restarted if they fail.\n\n### Running the Docker Container\n\n1. **Pull the Image**: Use the following command to pull the latest release of the Crawl4AI image from Docker Hub:\n   ```bash\n   docker pull unclecode/crawl4ai:latest\n   ```\n\n2. **Run the Container**: Start a new container from the pulled image, mapping the necessary ports and allocating shared memory:\n   ```bash\n   docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:latest\n   ```\n\n3. **Access the Application**: Once the container is running, you can access the application playground by navigating to `http://localhost:11235/playground` in your web browser.\n\n### Summary\n\nThe Crawl4AI Docker image is designed to provide a robust environment for running a web crawler and scraper that is compatible with large language models (LLMs). It includes all necessary dependencies, is optimized for security and performance, and is easy to deploy using Docker.",
      "docker-compose.yml": "To build and run the Docker image for the `crawl4ai` project from the `unclecode/crawl4ai` repository using the provided `docker-compose.yml` file, follow these steps:\n\n### Building the Docker Image\n\n1. **Clone the Repository**: Start by cloning the `unclecode/crawl4ai` repository to your local machine. This will give you access to the `docker-compose.yml` file and any other necessary resources.\n\n2. **Prepare Environment Variables**: Create a `.llm.env` file in the root of the cloned repository. This file should contain your API keys for various services (e.g., OpenAI, DeepSeek, Anthropic, etc.). You can use the `.llm.env.example` file as a template to set up your environment variables. Ensure that the keys are correctly populated.\n\n3. **Build the Image**: Navigate to the directory containing the `docker-compose.yml` file in your terminal. Use the Docker Compose command to build the image. The build process will pull the necessary base images and install dependencies required for the application, such as libraries for web serving and any specific tools needed for crawling AI-related tasks. The resulting image will be tagged as `unclecode/crawl4ai` with the specified version.\n\n### Running the Docker Container\n\n4. **Run the Container**: After successfully building the image, you can run the container using Docker Compose. The command will start the `crawl4ai` service, mapping the internal port 11235 to the same port on your host machine. This allows you to access the application via your web browser.\n\n5. **Access the Application**: Once the container is running, open your web browser and navigate to `http://localhost:11235/playground`. This will take you to the application's playground interface, where you can interact with the features provided by the `crawl4ai` service.\n\n### Additional Notes\n\n- **Resource Management**: The Docker Compose configuration includes resource limits to ensure the container does not exceed 4GB of memory, with a reservation of 1GB. This is important for maintaining performance, especially when handling large datasets or multiple requests.\n\n- **Health Checks**: The configuration includes a health check that periodically verifies if the service is running correctly. If the service fails, Docker will attempt to restart it automatically.\n\n- **User Permissions**: The container runs under a non-root user (`appuser`) for improved security.\n\nBy following these steps, you will have a fully operational instance of the `crawl4ai` application running in a Docker container, ready for use and experimentation."
    }
  },
  "input_to_gpt": {
    "repo_name": "unclecode/crawl4ai",
    "num_code_blocks": 19,
    "total_length": 12117,
    "code_blocks": [
      "python -m playwright install --with-deps chromium",
      "import asyncio\nfrom crawl4ai import *\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "# Basic crawl with markdown output\ncrwl https://www.nbcnews.com/business -o markdown\n\n# Deep crawl with BFS strategy, max 10 pages\ncrwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10\n\n# Use LLM extraction with a specific question\ncrwl https://www.example.com/products -q \"Extract all product prices\"",
      "   playwright install",
      "   python -m playwright install chromium",
      "import requests\n\n# Submit a crawl job\nresponse = requests.post(\n    \"http://localhost:11235/crawl\",\n    json={\"urls\": [\"https://example.com\"], \"priority\": 10}\n)\nif response.status_code == 200:\n    print(\"Crawl job submitted successfully.\")\n    \nif \"results\" in response.json():\n    results = response.json()[\"results\"]\n    print(\"Crawl job completed. Results:\")\n    for result in results:\n        print(result)\nelse:\n    task_id = response.json()[\"task_id\"]\n    print(f\"Crawl job submitted. Task ID:: {task_id}\")\n    result = requests.get(f\"http://localhost:11235/task/{task_id}\")",
      "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    browser_config = BrowserConfig(\n        headless=True,  \n        verbose=True,\n    )\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0)\n        ),\n        # markdown_generator=DefaultMarkdownGenerator(\n        #     content_filter=BM25ContentFilter(user_query=\"WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY\", bm25_threshold=1.0)\n        # ),\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://docs.micronaut.io/4.9.9/guide/\",\n            config=run_config\n        )\n        print(len(result.markdown.raw_markdown))\n        print(len(result.markdown.fit_markdown))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    schema = {\n    \"name\": \"KidoCode Courses\",\n    \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n    \"fields\": [\n        {\n            \"name\": \"section_title\",\n            \"selector\": \"h3.heading-50\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"section_description\",\n            \"selector\": \".charge-content\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_name\",\n            \"selector\": \".text-block-93\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_description\",\n            \"selector\": \".course-content-text\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_icon\",\n            \"selector\": \".image-92\",\n            \"type\": \"attribute\",\n            \"attribute\": \"src\"\n        }\n    ]\n}\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    browser_config = BrowserConfig(\n        headless=False,\n        verbose=True\n    )\n    run_config = CrawlerRunConfig(\n        extraction_strategy=extraction_strategy,\n        js_code=[\"\"\"(async () => {const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r => setTimeout(r, 500));}})();\"\"\"],\n        cache_mode=CacheMode.BYPASS\n    )\n        \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        \n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\",\n            config=run_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "import os\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        word_count_threshold=1,\n        extraction_strategy=LLMExtractionStrategy(\n            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2\n            # provider=\"ollama/qwen2\", api_token=\"no-token\", \n            llm_config = LLMConfig(provider=\"openai/gpt-4o\", api_token=os.getenv('OPENAI_API_KEY')), \n            schema=OpenAIModelFee.schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content. One extracted model JSON format should look like this: \n            {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}.\"\"\"\n        ),            \n        cache_mode=CacheMode.BYPASS,\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url='https://openai.com/api/pricing/',\n            config=run_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
      "import os, sys\nfrom pathlib import Path\nimport asyncio, time\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def test_news_crawl():\n    # Create a persistent user data directory\n    user_data_dir = os.path.join(Path.home(), \".crawl4ai\", \"browser_profile\")\n    os.makedirs(user_data_dir, exist_ok=True)\n\n    browser_config = BrowserConfig(\n        verbose=True,\n        headless=True,\n        user_data_dir=user_data_dir,\n        use_persistent_context=True,\n    )\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        url = \"ADDRESS_OF_A_CHALLENGING_WEBSITE\"\n        \n        result = await crawler.arun(\n            url,\n            config=run_config,\n            magic=True,\n        )\n        \n        print(f\"Successfully crawled {url}\")\n        print(f\"Content length: {len(result.markdown)}\")",
      "  from crawl4ai import hooks_to_string\n  from crawl4ai.docker_client import Crawl4aiDockerClient\n\n  # Define hooks as regular Python functions\n  async def on_page_context_created(page, context, **kwargs):\n      \"\"\"Block images to speed up crawling\"\"\"\n      await context.route(\"**/*.{png,jpg,jpeg,gif,webp}\", lambda route: route.abort())\n      await page.set_viewport_size({\"width\": 1920, \"height\": 1080})\n      return page\n\n  async def before_goto(page, context, url, **kwargs):\n      \"\"\"Add custom headers\"\"\"\n      await page.set_extra_http_headers({'X-Crawl4AI': 'v0.7.5'})\n      return page\n\n  # Option 1: Use hooks_to_string() utility for REST API\n  hooks_code = hooks_to_string({\n      \"on_page_context_created\": on_page_context_created,\n      \"before_goto\": before_goto\n  })\n\n  # Option 2: Docker client with automatic conversion (Recommended)\n  client = Crawl4aiDockerClient(base_url=\"http://localhost:11235\")\n  results = await client.crawl(\n      urls=[\"https://httpbin.org/html\"],\n      hooks={\n          \"on_page_context_created\": on_page_context_created,\n          \"before_goto\": before_goto\n      }\n  )\n  # âœ“ Full IDE support, type checking, and reusability!",
      "  from crawl4ai import LLMTableExtraction, LLMConfig\n  \n  # Configure intelligent table extraction\n  table_strategy = LLMTableExtraction(\n      llm_config=LLMConfig(provider=\"openai/gpt-4.1-mini\"),\n      enable_chunking=True,           # Handle massive tables\n      chunk_token_threshold=5000,     # Smart chunking threshold\n      overlap_threshold=100,          # Maintain context between chunks\n      extraction_type=\"structured\"    # Get structured data output\n  )\n  \n  config = CrawlerRunConfig(table_extraction_strategy=table_strategy)\n  result = await crawler.arun(\"https://complex-tables-site.com\", config=config)\n  \n  # Tables are automatically chunked, processed, and merged\n  for table in result.tables:\n      print(f\"Extracted table: {len(table['data'])} rows\")",
      "  from crawl4ai import AsyncWebCrawler, BrowserConfig\n  \n  browser_config = BrowserConfig(\n      browser_type=\"undetected\",  # Use undetected Chrome\n      headless=True,              # Can run headless with stealth\n      extra_args=[\n          \"--disable-blink-features=AutomationControlled\",\n          \"--disable-web-security\"\n      ]\n  )\n  \n  async with AsyncWebCrawler(config=browser_config) as crawler:\n      result = await crawler.arun(\"https://protected-site.com\")\n  # Successfully bypass Cloudflare, Akamai, and custom bot detection",
      "  from crawl4ai import CrawlerRunConfig, MatchMode\n  \n  configs = [\n      # Documentation sites - aggressive caching\n      CrawlerRunConfig(\n          url_matcher=[\"*docs*\", \"*documentation*\"],\n          cache_mode=\"write\",\n          markdown_generator_options={\"include_links\": True}\n      ),\n      \n      # News/blog sites - fresh content\n      CrawlerRunConfig(\n          url_matcher=lambda url: 'blog' in url or 'news' in url,\n          cache_mode=\"bypass\"\n      ),\n      \n      # Fallback for everything else\n      CrawlerRunConfig()\n  ]\n  \n  results = await crawler.arun_many(urls, config=configs)\n  # Each URL gets the perfect configuration automatically",
      "  from crawl4ai.memory_utils import MemoryMonitor\n  \n  monitor = MemoryMonitor()\n  monitor.start_monitoring()\n  \n  results = await crawler.arun_many(large_url_list)\n  \n  report = monitor.get_report()\n  print(f\"Peak memory: {report['peak_mb']:.1f} MB\")\n  print(f\"Efficiency: {report['efficiency']:.1f}%\")\n  # Get optimization recommendations",
      "  result = await crawler.arun(\"https://site-with-tables.com\")\n  \n  # New way - direct table access\n  if result.tables:\n      import pandas as pd\n      for table in result.tables:\n          df = pd.DataFrame(table['data'])\n          print(f\"Table: {df.shape[0]} rows Ã— {df.shape[1]} columns\")",
      "  config = AdaptiveConfig(\n      confidence_threshold=0.7, # Min confidence to stop crawling\n      max_depth=5, # Maximum crawl depth\n      max_pages=20, # Maximum number of pages to crawl\n      strategy=\"statistical\"\n  )\n  \n  async with AsyncWebCrawler() as crawler:\n      adaptive_crawler = AdaptiveCrawler(crawler, config)\n      state = await adaptive_crawler.digest(\n          start_url=\"https://news.example.com\",\n          query=\"latest news content\"\n      )\n  # Crawler learns patterns and improves extraction over time",
      "  scroll_config = VirtualScrollConfig(\n      container_selector=\"[data-testid='feed']\",\n      scroll_count=20,\n      scroll_by=\"container_height\",\n      wait_after_scroll=1.0\n  )\n  \n  result = await crawler.arun(url, config=CrawlerRunConfig(\n      virtual_scroll_config=scroll_config\n  ))",
      "  link_config = LinkPreviewConfig(\n      query=\"machine learning tutorials\",\n      score_threshold=0.3,\n      concurrent_requests=10\n  )\n  \n  result = await crawler.arun(url, config=CrawlerRunConfig(\n      link_preview_config=link_config,\n      score_links=True\n  ))\n  # Links ranked by relevance and quality"
    ]
  }
}