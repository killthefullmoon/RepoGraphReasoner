{
  "tasks": [
    {
      "task_title": "准备莎士比亚字符数据",
      "task_description": "执行数据准备脚本以处理莎士比亚字符数据集。",
      "example_code": null,
      "running_command": "python data/shakespeare_char/prepare.py",
      "expected_input": null,
      "expected_output": "数据准备完成的消息"
    },
    {
      "task_title": "训练莎士比亚字符模型",
      "task_description": "使用配置文件训练莎士比亚字符模型。",
      "example_code": null,
      "running_command": "python train.py config/train_shakespeare_char.py",
      "expected_input": null,
      "expected_output": "训练过程的日志信息"
    },
    {
      "task_title": "生成莎士比亚字符样本",
      "task_description": "生成基于莎士比亚字符模型的文本样本。",
      "example_code": null,
      "running_command": "python sample.py --out_dir=out-shakespeare-char",
      "expected_input": null,
      "expected_output": "生成的文本样本"
    },
    {
      "task_title": "训练莎士比亚字符模型（带参数）",
      "task_description": "使用多种参数训练莎士比亚字符模型。",
      "example_code": null,
      "running_command": "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0",
      "expected_input": null,
      "expected_output": "训练过程的详细日志信息"
    },
    {
      "task_title": "生成莎士比亚字符样本（带设备参数）",
      "task_description": "生成基于莎士比亚字符模型的文本样本，并指定设备。",
      "example_code": null,
      "running_command": "python sample.py --out_dir=out-shakespeare-char --device=cpu",
      "expected_input": null,
      "expected_output": "生成的文本样本"
    },
    {
      "task_title": "准备OpenWebText数据",
      "task_description": "执行数据准备脚本以处理OpenWebText数据集。",
      "example_code": null,
      "running_command": "python data/openwebtext/prepare.py",
      "expected_input": null,
      "expected_output": "数据准备完成的消息"
    },
    {
      "task_title": "分布式训练GPT-2模型",
      "task_description": "在多个节点上分布式训练GPT-2模型。",
      "example_code": null,
      "running_command": "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py",
      "expected_input": null,
      "expected_output": "训练过程的日志信息"
    },
    {
      "task_title": "分布式训练GPT-2模型（主节点）",
      "task_description": "在主节点上启动分布式训练GPT-2模型。",
      "example_code": null,
      "running_command": "torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py",
      "expected_input": null,
      "expected_output": "训练过程的日志信息"
    },
    {
      "task_title": "分布式训练GPT-2模型（工作节点）",
      "task_description": "在工作节点上启动分布式训练GPT-2模型。",
      "example_code": null,
      "running_command": "torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py",
      "expected_input": null,
      "expected_output": "训练过程的日志信息"
    },
    {
      "task_title": "评估GPT-2模型",
      "task_description": "使用不同配置文件评估GPT-2模型。",
      "example_code": null,
      "running_command": "$ python train.py config/eval_gpt2.py\n$ python train.py config/eval_gpt2_medium.py\n$ python train.py config/eval_gpt2_large.py\n$ python train.py config/eval_gpt2_xl.py",
      "expected_input": null,
      "expected_output": "评估结果的日志信息"
    },
    {
      "task_title": "微调莎士比亚模型",
      "task_description": "使用配置文件微调莎士比亚模型。",
      "example_code": null,
      "running_command": "python train.py config/finetune_shakespeare.py",
      "expected_input": null,
      "expected_output": "微调过程的日志信息"
    },
    {
      "task_title": "生成文本样本（指定初始文本）",
      "task_description": "从指定的初始文本生成多个新文本样本。",
      "example_code": null,
      "running_command": "python sample.py --init_from=gpt2-xl --start=\"What is the answer to life, the universe, and everything?\" --num_samples=5 --max_new_tokens=100",
      "expected_input": "What is the answer to life, the universe, and everything?",
      "expected_output": "生成的文本样本"
    }
  ],
  "setup": {
    "setup_commands": [],
    "docker_commands": [],
    "docker_files": {},
    "docker_setup_descriptions": null
  },
  "input_to_gpt": {
    "repo_name": "karpathy/nanoGPT",
    "num_code_blocks": 11,
    "total_length": 1284,
    "code_blocks": [
      "python data/shakespeare_char/prepare.py",
      "python train.py config/train_shakespeare_char.py",
      "python sample.py --out_dir=out-shakespeare-char",
      "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0",
      "python sample.py --out_dir=out-shakespeare-char --device=cpu",
      "python data/openwebtext/prepare.py",
      "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py",
      "# Run on the first (master) node with example IP 123.456.123.456:\ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n# Run on the worker node:\ntorchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py",
      "$ python train.py config/eval_gpt2.py\n$ python train.py config/eval_gpt2_medium.py\n$ python train.py config/eval_gpt2_large.py\n$ python train.py config/eval_gpt2_xl.py",
      "python train.py config/finetune_shakespeare.py",
      "python sample.py \\\n    --init_from=gpt2-xl \\\n    --start=\"What is the answer to life, the universe, and everything?\" \\\n    --num_samples=5 --max_new_tokens=100"
    ]
  }
}