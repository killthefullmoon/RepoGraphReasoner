{
  "tasks": [
    {
      "task_title": "MRPC Classification Training",
      "task_description": "训练一个用于MRPC任务的BERT分类器，进行训练和评估。",
      "example_code": null,
      "running_command": "python run_classifier.py --task_name=MRPC --do_train=true --do_eval=true --data_dir=$GLUE_DIR/MRPC --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=/tmp/mrpc_output/",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "MRPC Classification Prediction",
      "task_description": "使用已训练的分类器对MRPC任务进行预测。",
      "example_code": null,
      "running_command": "python run_classifier.py --task_name=MRPC --do_predict=true --data_dir=$GLUE_DIR/MRPC --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$TRAINED_CLASSIFIER --max_seq_length=128 --output_dir=/tmp/mrpc_output/",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "SQuAD Training and Prediction",
      "task_description": "在SQuAD数据集上训练和预测问答模型。",
      "example_code": null,
      "running_command": "python run_squad.py --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v1.1.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v1.1.json --train_batch_size=12 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=/tmp/squad_base/",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "SQuAD Evaluation",
      "task_description": "评估SQuAD模型的预测结果。",
      "example_code": null,
      "running_command": "python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json",
      "expected_input": null,
      "expected_output": "{\"f1\": 88.41249612335034, \"exact_match\": 81.2488174077578}"
    },
    {
      "task_title": "SQuAD Large Training and Prediction",
      "task_description": "在SQuAD数据集上使用大模型进行训练和预测。",
      "example_code": null,
      "running_command": "python run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v1.1.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v1.1.json --train_batch_size=24 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://some_bucket/squad_large/ --use_tpu=True --tpu_name=$TPU_NAME",
      "expected_input": null,
      "expected_output": "{\"f1\": 90.87081895814865, \"exact_match\": 84.38978240302744}"
    },
    {
      "task_title": "SQuAD v2.0 Training and Prediction",
      "task_description": "在SQuAD v2.0数据集上进行训练和预测，支持负样本。",
      "example_code": null,
      "running_command": "python run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v2.0.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v2.0.json --train_batch_size=24 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://some_bucket/squad_large/ --use_tpu=True --tpu_name=$TPU_NAME --version_2_with_negative=True",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "SQuAD v2.0 Inference with Negative Samples",
      "task_description": "在SQuAD v2.0数据集上进行推理，支持负样本，并设置阈值。",
      "example_code": null,
      "running_command": "python run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt --do_train=False --train_file=$SQUAD_DIR/train-v2.0.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v2.0.json --train_batch_size=24 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://some_bucket/squad_large/ --use_tpu=True --tpu_name=$TPU_NAME --version_2_with_negative=True --null_score_diff_threshold=$THRESH",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Feature Extraction",
      "task_description": "从输入文本中提取特征并保存为JSONL格式。",
      "example_code": "echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt\n\npython extract_features.py --input_file=/tmp/input.txt --output_file=/tmp/output.jsonl --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --layers=-1,-2,-3,-4 --max_seq_length=128 --batch_size=8",
      "running_command": null,
      "expected_input": "Who was Jim Henson ? ||| Jim Henson was a puppeteer",
      "expected_output": null
    },
    {
      "task_title": "Create Pretraining Data",
      "task_description": "创建用于预训练的TFRecord数据。",
      "example_code": null,
      "running_command": "python create_pretraining_data.py --input_file=./sample_text.txt --output_file=/tmp/tf_examples.tfrecord --vocab_file=$BERT_BASE_DIR/vocab.txt --do_lower_case=True --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5",
      "expected_input": null,
      "expected_output": null
    },
    {
      "task_title": "Run Pretraining",
      "task_description": "运行BERT模型的预训练过程。",
      "example_code": null,
      "running_command": "python run_pretraining.py --input_file=/tmp/tf_examples.tfrecord --output_dir=/tmp/pretraining_output --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --train_batch_size=32 --max_seq_length=128 --max_predictions_per_seq=20 --num_train_steps=20 --num_warmup_steps=10 --learning_rate=2e-5",
      "expected_input": null,
      "expected_output": null
    }
  ],
  "setup": {
    "setup_commands": [],
    "docker_commands": [],
    "docker_files": {},
    "docker_setup_descriptions": null
  },
  "input_to_gpt": {
    "repo_name": "google-research/bert",
    "num_code_blocks": 13,
    "total_length": 5320,
    "code_blocks": [
      "export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\nexport GLUE_DIR=/path/to/glue\n\npython run_classifier.py \\\n  --task_name=MRPC \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$GLUE_DIR/MRPC \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=2e-5 \\\n  --num_train_epochs=3.0 \\\n  --output_dir=/tmp/mrpc_output/",
      "export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\nexport GLUE_DIR=/path/to/glue\nexport TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier\n\npython run_classifier.py \\\n  --task_name=MRPC \\\n  --do_predict=true \\\n  --data_dir=$GLUE_DIR/MRPC \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$TRAINED_CLASSIFIER \\\n  --max_seq_length=128 \\\n  --output_dir=/tmp/mrpc_output/",
      "python run_squad.py \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --do_train=True \\\n  --train_file=$SQUAD_DIR/train-v1.1.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v1.1.json \\\n  --train_batch_size=12 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=/tmp/squad_base/",
      "python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json",
      "{\"f1\": 88.41249612335034, \"exact_match\": 81.2488174077578}",
      "python run_squad.py \\\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n  --do_train=True \\\n  --train_file=$SQUAD_DIR/train-v1.1.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v1.1.json \\\n  --train_batch_size=24 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=gs://some_bucket/squad_large/ \\\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME",
      "{\"f1\": 90.87081895814865, \"exact_match\": 84.38978240302744}",
      "python run_squad.py \\\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n  --do_train=True \\\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\n  --train_batch_size=24 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=gs://some_bucket/squad_large/ \\\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME \\\n  --version_2_with_negative=True",
      "python run_squad.py \\\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n  --do_train=False \\\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\n  --train_batch_size=24 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=gs://some_bucket/squad_large/ \\\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME \\\n  --version_2_with_negative=True \\\n  --null_score_diff_threshold=$THRESH",
      "# Sentence A and Sentence B are separated by the ||| delimiter for sentence\n# pair tasks like question answering and entailment.\n# For single sentence inputs, put one sentence per line and DON'T use the\n# delimiter.\necho 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt\n\npython extract_features.py \\\n  --input_file=/tmp/input.txt \\\n  --output_file=/tmp/output.jsonl \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --layers=-1,-2,-3,-4 \\\n  --max_seq_length=128 \\\n  --batch_size=8",
      "### Input\norig_tokens = [\"John\", \"Johanson\", \"'s\",  \"house\"]\nlabels      = [\"NNP\",  \"NNP\",      \"POS\", \"NN\"]\n\n### Output\nbert_tokens = []\n\n# Token map will be an int -> int mapping between the `orig_tokens` index and\n# the `bert_tokens` index.\norig_to_tok_map = []\n\ntokenizer = tokenization.FullTokenizer(\n    vocab_file=vocab_file, do_lower_case=True)\n\nbert_tokens.append(\"[CLS]\")\nfor orig_token in orig_tokens:\n  orig_to_tok_map.append(len(bert_tokens))\n  bert_tokens.extend(tokenizer.tokenize(orig_token))\nbert_tokens.append(\"[SEP]\")\n\n# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\n# orig_to_tok_map == [1, 2, 4, 6]",
      "python create_pretraining_data.py \\\n  --input_file=./sample_text.txt \\\n  --output_file=/tmp/tf_examples.tfrecord \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --do_lower_case=True \\\n  --max_seq_length=128 \\\n  --max_predictions_per_seq=20 \\\n  --masked_lm_prob=0.15 \\\n  --random_seed=12345 \\\n  --dupe_factor=5",
      "python run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=128 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=20 \\\n  --num_warmup_steps=10 \\\n  --learning_rate=2e-5"
    ]
  }
}