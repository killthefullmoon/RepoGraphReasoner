{
  "tasks": [
    {
      "task_title": "Load a YOLOv5 model",
      "task_description": "加载YOLOv5模型以进行推理，支持不同的模型选项。",
      "example_code": "import torch\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")",
      "running_command": null,
      "expected_input": null,
      "expected_output": "模型对象"
    },
    {
      "task_title": "Perform inference",
      "task_description": "对输入图像进行推理，自动处理批量、调整大小和归一化。",
      "example_code": "results = model(img)",
      "running_command": null,
      "expected_input": "https://ultralytics.com/images/zidane.jpg",
      "expected_output": "推理结果对象"
    },
    {
      "task_title": "Process inference results",
      "task_description": "处理推理结果，支持打印、显示、保存等功能。",
      "example_code": "results.print()\nresults.show()\nresults.save()",
      "running_command": null,
      "expected_input": null,
      "expected_output": "结果打印到控制台、显示在窗口或保存到文件"
    },
    {
      "task_title": "Run inference using a webcam",
      "task_description": "使用命令行工具从摄像头进行推理。",
      "example_code": null,
      "running_command": "python detect.py --weights yolov5s.pt --source 0",
      "expected_input": null,
      "expected_output": "实时推理结果"
    },
    {
      "task_title": "Run inference on a local image file",
      "task_description": "使用命令行工具对本地图像文件进行推理。",
      "example_code": null,
      "running_command": "python detect.py --weights yolov5s.pt --source img.jpg",
      "expected_input": "img.jpg",
      "expected_output": "推理结果"
    },
    {
      "task_title": "Train YOLOv5 model",
      "task_description": "使用命令行工具训练YOLOv5模型，指定数据集和训练参数。",
      "example_code": null,
      "running_command": "python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64",
      "expected_input": null,
      "expected_output": "训练过程日志和最终模型"
    },
    {
      "task_title": "Validate the model",
      "task_description": "使用命令行工具验证训练后的模型。",
      "example_code": null,
      "running_command": "python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640",
      "expected_input": null,
      "expected_output": "验证结果和性能指标"
    },
    {
      "task_title": "Run prediction",
      "task_description": "使用命令行工具对指定图像进行预测。",
      "example_code": null,
      "running_command": "python segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg",
      "expected_input": "data/images/bus.jpg",
      "expected_output": "预测结果"
    },
    {
      "task_title": "Export model",
      "task_description": "将训练好的模型导出为其他格式。",
      "example_code": null,
      "running_command": "python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0",
      "expected_input": null,
      "expected_output": "导出的模型文件"
    }
  ],
  "setup": {
    "setup_commands": [
      "# Install the ultralytics package\npip install ultralytics",
      "# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt"
    ],
    "docker_commands": [],
    "docker_files": {
      ".dockerignore": "# Repo-specific DockerIgnore -------------------------------------------------------------------------------------------\n.git\n.cache\n.idea\nruns\noutput\ncoco\nstorage.googleapis.com\n\ndata/samples/*\n**/results*.csv\n*.jpg\n\n# Neural Network weights -----------------------------------------------------------------------------------------------\n**/*.pt\n**/*.pth\n**/*.onnx\n**/*.engine\n**/*.mlmodel\n**/*.torchscript\n**/*.torchscript.pt\n**/*.tflite\n**/*.h5\n**/*.pb\n*_saved_model/\n*_web_model/\n*_openvino_model/\n\n# Below Copied From .gitignore -----------------------------------------------------------------------------------------\n# Below Copied From .gitignore -----------------------------------------------------------------------------------------\n\n\n# GitHub Python GitIgnore ----------------------------------------------------------------------------------------------\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\nwandb/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv*\nvenv*/\nENV*/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n\n# https://github.com/github/gitignore/blob/master/Global/macOS.gitignore -----------------------------------------------\n\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\nIcon?\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n\n# https://github.com/github/gitignore/blob/master/Global/JetBrains.gitignore\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and WebStorm\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff:\n.idea/*\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/dictionaries\n.html  # Bokeh Plots\n.pg  # TensorFlow Frozen Graphs\n.avi # videos\n\n# Sensitive or high-churn files:\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n\n# Gradle:\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# CMake\ncmake-build-debug/\ncmake-build-release/\n\n# Mongo Explorer plugin:\n.idea/**/mongoSettings.xml\n\n## File-based project format:\n*.iws\n\n## Plugin-specific files:\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n"
    }
  },
  "input_to_gpt": {
    "repo_name": "ultralytics/yolov5",
    "num_code_blocks": 13,
    "total_length": 4189,
    "code_blocks": [
      "import torch\n\n# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # Default: yolov5s\n\n# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # Example image\n\n# Perform inference (handles batching, resizing, normalization automatically)\nresults = model(img)\n\n# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())\nresults.print()  # Print results to console\nresults.show()  # Display results in a window\nresults.save()  # Save results to runs/detect/exp",
      "# Run inference using a webcam\npython detect.py --weights yolov5s.pt --source 0\n\n# Run inference on a local image file\npython detect.py --weights yolov5s.pt --source img.jpg\n\n# Run inference on a local video file\npython detect.py --weights yolov5s.pt --source vid.mp4\n\n# Run inference on a screen capture\npython detect.py --weights yolov5s.pt --source screen\n\n# Run inference on a directory of images\npython detect.py --weights yolov5s.pt --source path/to/images/\n\n# Run inference on a text file listing image paths\npython detect.py --weights yolov5s.pt --source list.txt\n\n# Run inference on a text file listing stream URLs\npython detect.py --weights yolov5s.pt --source list.streams\n\n# Run inference using a glob pattern for images\npython detect.py --weights yolov5s.pt --source 'path/to/*.jpg'\n\n# Run inference on a YouTube video URL\npython detect.py --weights yolov5s.pt --source 'https://youtu.be/LNwODJXcvt4'\n\n# Run inference on an RTSP, RTMP, or HTTP stream\npython detect.py --weights yolov5s.pt --source 'rtsp://example.com/media.mp4'",
      "# Train YOLOv5n on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5n.yaml --batch-size 128\n\n# Train YOLOv5s on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64\n\n# Train YOLOv5m on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5m.yaml --batch-size 40\n\n# Train YOLOv5l on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5l.yaml --batch-size 24\n\n# Train YOLOv5x on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5x.yaml --batch-size 16",
      "# Train on a single GPU\npython segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640\n\n# Train using Multi-GPU Distributed Data Parallel (DDP)\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640 --device 0,1,2,3",
      "# Download COCO validation segments split (780MB, 5000 images)\nbash data/scripts/get_coco.sh --val --segments\n\n# Validate the model\npython segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640",
      "# Run prediction\npython segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg",
      "# Load model from PyTorch Hub (Note: Inference support might vary)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5m-seg.pt\")",
      "# Export model\npython export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0",
      "# Train on a single GPU using CIFAR-100 dataset\npython classify/train.py --model yolov5s-cls.pt --data cifar100 --epochs 5 --img 224 --batch 128\n\n# Train using Multi-GPU DDP on ImageNet dataset\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3",
      "# Download ImageNet validation split (6.3GB, 50,000 images)\nbash data/scripts/get_imagenet.sh --val\n\n# Validate the model\npython classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224",
      "# Run prediction\npython classify/predict.py --weights yolov5s-cls.pt --source data/images/bus.jpg",
      "# Load model from PyTorch Hub\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s-cls.pt\")",
      "# Export models\npython export.py --weights yolov5s-cls.pt resnet50.pt efficientnet_b0.pt --include onnx engine --img 224"
    ]
  }
}