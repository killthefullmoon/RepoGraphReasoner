{
  "tasks": [
    {
      "task_title": "加载文档数据",
      "task_description": "使用SimpleDirectoryReader从指定目录加载文档数据。",
      "example_code": "from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()",
      "running_command": null,
      "expected_input": "YOUR_DATA_DIRECTORY",
      "expected_output": "文档数据列表"
    },
    {
      "task_title": "创建向量存储索引",
      "task_description": "从加载的文档数据创建一个VectorStoreIndex。",
      "example_code": "from llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(documents)",
      "running_command": null,
      "expected_input": "documents",
      "expected_output": "VectorStoreIndex对象"
    },
    {
      "task_title": "查询索引",
      "task_description": "使用创建的索引作为查询引擎来查询问题。",
      "example_code": "query_engine = index.as_query_engine()\nquery_engine.query(\"YOUR_QUESTION\")",
      "running_command": null,
      "expected_input": "YOUR_QUESTION",
      "expected_output": "查询结果"
    },
    {
      "task_title": "持久化索引",
      "task_description": "将当前索引的存储上下文持久化到磁盘。",
      "example_code": "index.storage_context.persist()",
      "running_command": null,
      "expected_input": null,
      "expected_output": "索引已持久化"
    },
    {
      "task_title": "从存储加载索引",
      "task_description": "从持久化存储中加载索引。",
      "example_code": "from llama_index.core import StorageContext, load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)",
      "running_command": null,
      "expected_input": "./storage",
      "expected_output": "加载的VectorStoreIndex对象"
    }
  ],
  "setup": {
    "setup_commands": [
      "# custom selection of integrations to work with core\npip install llama-index-core\npip install llama-index-llms-openai\npip install llama-index-llms-replicate\npip install llama-index-embeddings-huggingface",
      "cd <desired-package-folder>\npip install poetry\npoetry install --with dev"
    ],
    "docker_commands": [],
    "docker_files": {}
  },
  "input_to_gpt": {
    "repo_name": "run-llama/llama_index",
    "num_code_blocks": 7,
    "total_length": 2103,
    "code_blocks": [
      "# typical pattern\nfrom llama_index.core.xxx import ClassABC  # core submodule xxx\nfrom llama_index.xxx.yyy import (\n    SubclassABC,\n)  # integration yyy for submodule xxx\n\n# concrete example\nfrom llama_index.core.llms import LLM\nfrom llama_index.llms.openai import OpenAI",
      "import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)",
      "import os\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_API_TOKEN\"\n\nfrom llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.replicate import Replicate\nfrom transformers import AutoTokenizer\n\n# set the LLM\nllama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\nSettings.llm = Replicate(\n    model=llama2_7b_chat,\n    temperature=0.01,\n    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n)\n\n# set tokenizer to match LLM\nSettings.tokenizer = AutoTokenizer.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\"\n)\n\n# set the embed model\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents,\n)",
      "query_engine = index.as_query_engine()\nquery_engine.query(\"YOUR_QUESTION\")",
      "index.storage_context.persist()",
      "from llama_index.core import StorageContext, load_index_from_storage\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n# load index\nindex = load_index_from_storage(storage_context)",
      "#!/bin/bash\nSTATIC_DIR=\"venv/lib/python3.13/site-packages/llama_index/core/_static\"\nREPO=\"run-llama/llama_index\"\n\nfind \"$STATIC_DIR\" -type f | while read -r file; do\n    echo \"Verifying: $file\"\n    gh attestation verify \"$file\" -R \"$REPO\" || echo \"Failed to verify: $file\"\ndone"
    ]
  }
}