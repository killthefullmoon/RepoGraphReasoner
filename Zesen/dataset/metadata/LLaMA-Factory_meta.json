{
  "repo": {
    "full_name": "hiyouga/LLaMA-Factory",
    "stars": 61297,
    "forks": 7421,
    "language": "Python",
    "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
    "topics": [
      "agent",
      "ai",
      "deepseek",
      "fine-tuning",
      "gemma",
      "gpt",
      "instruction-tuning",
      "large-language-models",
      "llama",
      "llama3",
      "llm",
      "lora",
      "moe",
      "nlp",
      "peft",
      "qlora",
      "quantization",
      "qwen",
      "rlhf",
      "transformers"
    ],
    "url": "https://github.com/hiyouga/LLaMA-Factory",
    "created_at": "2023-05-28T10:09:12Z",
    "updated_at": "2025-10-31T03:32:24Z"
  },
  "readme_file": "dataset/readmes/LLaMA-Factory_README.md",
  "task_file": "dataset/tasks/LLaMA-Factory_tasks.json",
  "num_tasks": 12,
  "num_setup_commands": 7,
  "num_docker_commands": 4,
  "has_docker_files": true,
  "docker_files_list": [
    ".dockerignore"
  ],
  "docker_setup_descriptions": {
    ".dockerignore": "To build and run the Docker image for the `hiyouga/LLaMA-Factory` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image created from this repository is designed to facilitate the development and deployment of the LLaMA-Factory project, which is likely focused on machine learning or natural language processing tasks. The image includes necessary dependencies for running the application efficiently, particularly leveraging GPU resources for enhanced performance.\n\n### Building the Image\n1. **Prepare Your Environment**: Ensure you have Docker installed on your machine. You should also have access to a terminal or command prompt.\n\n2. **Clone the Repository**: Clone the `hiyouga/LLaMA-Factory` repository to your local machine. This will provide you with the necessary files, including the Dockerfile and the `.dockerignore` file.\n\n3. **Understand the `.dockerignore` File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This helps keep the image lightweight and avoids unnecessary files being included. The entries in this file include common development artifacts like `.git`, `.venv`, and cache directories.\n\n4. **Build the Docker Image**: Navigate to the directory containing the Dockerfile. Use the Docker build command to create the image. The build process will utilize the specified Dockerfile and any build arguments necessary for your environment (e.g., package index or additional metrics).\n\n5. **Tag the Image**: During the build process, tag the image appropriately (e.g., `llamafactory:latest`) to easily reference it later.\n\n### Running the Image\n1. **Run the Container**: Once the image is built, you can run it using Docker. Use the `docker run` command to start a new container from the image. Ensure to allocate the necessary resources, such as GPU access and specific ports for application interaction (e.g., 7860 and 8000).\n\n2. **Interactive Shell Access**: If you need to interact with the container, you can execute a bash shell within the running container using the `docker exec` command. This allows you to run commands directly in the container environment.\n\n3. **Manage Resources**: Depending on your hardware and application requirements, you may need to adjust device mappings or volume mounts to ensure the container has access to necessary resources.\n\n### Conclusion\nBy following these steps, you will successfully build and run the Docker image for the LLaMA-Factory project. This setup allows for efficient development and testing of machine learning models, leveraging Docker's capabilities to manage dependencies and resources effectively."
  },
  "processed_at": "2025-10-30T23:37:16.905472"
}