{
  "tasks": [
    {
      "task_title": "FP8 to BF16 Weight Conversion",
      "task_description": "将FP8格式的模型权重转换为BF16格式，使用了Python脚本进行转换。",
      "example_code": "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights",
      "running_command": null,
      "expected_input": "/path/to/fp8_weights, /path/to/bf16_weights",
      "expected_output": "权重转换成功，输出路径为/path/to/bf16_weights"
    },
    {
      "task_title": "Chat with DeepSeek-V3",
      "task_description": "与DeepSeek-V3进行对话，使用了CLI工具进行交互。",
      "example_code": null,
      "running_command": "python chat_with_deepseek.py",
      "expected_input": "用户输入的对话内容",
      "expected_output": "DeepSeek-V3的回复内容"
    },
    {
      "task_title": "Batch Inference",
      "task_description": "对给定文件进行批量推理，使用CLI工具处理多个输入。",
      "example_code": null,
      "running_command": "python batch_inference.py --input-file /path/to/input_file",
      "expected_input": "/path/to/input_file",
      "expected_output": "批量推理结果"
    }
  ],
  "setup": {
    "setup_commands": [
      "\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.\n",
      "\nDownload the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert Hugging Face model weights to a specific format:\n",
      "\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nSGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.\n\nMulti-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. \n\n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Inference with LightLLM (recommended)\n\n[LightLLM](https://github.com/ModelTC/lightllm/tree/main) v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to [LightLLM instructions](https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html). Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.\n\n### 6.7 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.8 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation"
    ],
    "docker_commands": [],
    "docker_files": {}
  },
  "input_to_gpt": {
    "repo_name": "deepseek-ai/DeepSeek-V3",
    "num_code_blocks": 4,
    "total_length": 318,
    "code_blocks": [
      "cd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights",
      "#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n",
      "\n#### Run\n\nThen you can chat with DeepSeek-V3:\n",
      "\nOr batch inference on a given file:\n"
    ]
  }
}