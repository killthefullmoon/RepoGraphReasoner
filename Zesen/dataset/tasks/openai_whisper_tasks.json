{
  "tasks": [
    {
      "task_title": "Transcribe audio to text",
      "task_description": "This task transcribes the audio file into text using the 'turbo' model.",
      "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])",
      "running_command": null,
      "expected_input": "audio.mp3",
      "expected_output": "Transcribed text from audio.mp3"
    },
    {
      "task_title": "Specify language for transcription",
      "task_description": "This task transcribes an audio file while specifying the spoken language as Japanese.",
      "example_code": null,
      "running_command": "whisper japanese.wav --language Japanese",
      "expected_input": "japanese.wav",
      "expected_output": "Transcribed text in Japanese"
    },
    {
      "task_title": "Translate audio to text",
      "task_description": "This task translates the spoken language in an audio file to text while specifying the model as medium.",
      "example_code": null,
      "running_command": "whisper japanese.wav --model medium --language Japanese --task translate",
      "expected_input": "japanese.wav",
      "expected_output": "Translated text from Japanese audio"
    },
    {
      "task_title": "Display help information",
      "task_description": "This task displays the help information for the whisper CLI tool.",
      "example_code": null,
      "running_command": "whisper --help",
      "expected_input": null,
      "expected_output": "Help information for the whisper CLI"
    },
    {
      "task_title": "Load and process audio for transcription",
      "task_description": "This task loads an audio file, processes it, detects the language, and decodes it to get the recognized text.",
      "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\nprint(result.text)",
      "running_command": null,
      "expected_input": "audio.mp3",
      "expected_output": "Detected language: <language>\nRecognized text"
    }
  ],
  "setup": {
    "setup_commands": [
      "# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg",
      "pip install setuptools-rust"
    ],
    "docker_commands": [],
    "docker_files": {}
  },
  "input_to_gpt": {
    "repo_name": "openai/whisper",
    "num_code_blocks": 6,
    "total_length": 926,
    "code_blocks": [
      "whisper audio.flac audio.mp3 audio.wav --model turbo",
      "whisper japanese.wav --language Japanese",
      "whisper japanese.wav --model medium --language Japanese --task translate",
      "whisper --help",
      "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])",
      "import whisper\n\nmodel = whisper.load_model(\"turbo\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)"
    ]
  }
}