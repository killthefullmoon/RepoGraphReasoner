{
  "tasks": [
    {
      "task_title": "Chat with interpreter",
      "task_description": "Executes a single command to chat with the interpreter.",
      "example_code": "interpreter.chat(\"Plot AAPL and META's normalized stock prices\")",
      "running_command": null,
      "expected_input": "Plot AAPL and META's normalized stock prices",
      "expected_output": "Plots of normalized stock prices for AAPL and META."
    },
    {
      "task_title": "Interactive chat session",
      "task_description": "Starts an interactive chat session with the interpreter.",
      "example_code": "interpreter.chat()",
      "running_command": null,
      "expected_input": null,
      "expected_output": "Interactive chat interface."
    },
    {
      "task_title": "Stream chat response",
      "task_description": "Streams the response for a given message without displaying it immediately.",
      "example_code": "for chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)",
      "running_command": null,
      "expected_input": "What operating system are we on?",
      "expected_output": "Chunks of responses from the interpreter."
    },
    {
      "task_title": "Reset messages",
      "task_description": "Resets the interpreter's memory of previous messages.",
      "example_code": "interpreter.messages = []",
      "running_command": null,
      "expected_input": null,
      "expected_output": "Interpreter memory reset."
    },
    {
      "task_title": "Resume chat from messages",
      "task_description": "Resumes chat from saved messages.",
      "example_code": "messages = interpreter.chat(\"My name is Killian.\")\ninterpreter.messages = messages",
      "running_command": null,
      "expected_input": "My name is Killian.",
      "expected_output": "Continued conversation remembering 'Killian'."
    },
    {
      "task_title": "Add subtitles to videos",
      "task_description": "Adds subtitles to all videos in a specified directory.",
      "example_code": "interpreter.chat(\"Add subtitles to all videos in /videos.\")",
      "running_command": null,
      "expected_input": "Add subtitles to all videos in /videos.",
      "expected_output": "Subtitles added to all videos in the specified directory."
    },
    {
      "task_title": "Modify subtitles size",
      "task_description": "Requests to modify the size of subtitles that were just added.",
      "example_code": "interpreter.chat(\"These look great but can you make the subtitles bigger?\")",
      "running_command": null,
      "expected_input": "These look great but can you make the subtitles bigger?",
      "expected_output": "Subtitles size modified."
    },
    {
      "task_title": "Run shell commands",
      "task_description": "Allows the interpreter to run shell commands without user confirmation.",
      "example_code": "interpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"",
      "running_command": null,
      "expected_input": null,
      "expected_output": "System message updated to allow shell commands."
    },
    {
      "task_title": "Set model for interpreter",
      "task_description": "Sets the language model to be used by the interpreter.",
      "example_code": "interpreter.llm.model = \"gpt-3.5-turbo\"",
      "running_command": null,
      "expected_input": null,
      "expected_output": "Model set to gpt-3.5-turbo."
    },
    {
      "task_title": "Chat endpoint in FastAPI",
      "task_description": "Creates an endpoint to handle chat requests in a FastAPI application.",
      "example_code": "@app.get(\"/chat\")\ndef chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")",
      "running_command": null,
      "expected_input": "message",
      "expected_output": "Streaming response of chat results."
    },
    {
      "task_title": "History endpoint in FastAPI",
      "task_description": "Creates an endpoint to retrieve the history of messages.",
      "example_code": "@app.get(\"/history\")\ndef history_endpoint():\n    return interpreter.messages",
      "running_command": null,
      "expected_input": null,
      "expected_output": "List of previous messages."
    }
  ],
  "setup": {
    "setup_commands": [
      "pip install git+https://github.com/OpenInterpreter/open-interpreter.git",
      "pip install open-interpreter",
      "pip install fastapi uvicorn\nuvicorn server:app --reload"
    ],
    "docker_commands": [],
    "docker_files": {
      "Dockerfile": "###########################################################################################\n# This Dockerfile runs an LMC-compatible websocket server at / on port 8000.              #\n# To learn more about LMC, visit https://docs.openinterpreter.com/protocols/lmc-messages. #\n###########################################################################################\n\nFROM python:3.11.8\n\n# Set environment variables\n# ENV OPENAI_API_KEY ...\n\nENV HOST 0.0.0.0\n# ^ Sets the server host to 0.0.0.0, Required for the server to be accessible outside the container\n\n# Copy required files into container\nRUN mkdir -p interpreter scripts\nCOPY interpreter/ interpreter/\nCOPY scripts/ scripts/\nCOPY poetry.lock pyproject.toml README.md ./\n\n# Expose port 8000\nEXPOSE 8000\n\n# Install server dependencies\nRUN pip install \".[server]\"\n\n# Start the server\nENTRYPOINT [\"interpreter\", \"--server\"]"
    },
    "docker_setup_descriptions": {
      "Dockerfile": "To build and run the Docker image for the openinterpreter/open-interpreter repository, follow these steps:\n\n### Building the Docker Image\n\n1. **Set Up Your Environment**: Ensure you have Docker installed on your machine. You can download it from the official Docker website.\n\n2. **Clone the Repository**: Use Git to clone the openinterpreter/open-interpreter repository to your local machine. This will give you access to the Dockerfile and necessary files.\n\n3. **Navigate to the Directory**: Open a terminal and change to the directory where the Dockerfile is located.\n\n4. **Build the Image**: Execute the Docker build command. This process will:\n   - Use the official Python 3.11.8 image as the base, ensuring you have a compatible environment for running Python applications.\n   - Set environment variables, including the server host to `0.0.0.0`, allowing the server to be accessible from outside the container.\n   - Create necessary directories and copy required files (such as the interpreter code, scripts, and dependency files) into the container.\n   - Install the server dependencies specified in the `pyproject.toml` and `poetry.lock` files, ensuring all necessary libraries and packages are available for the application to run.\n   - Expose port 8000, which is the designated port for the websocket server.\n\n### Running the Docker Container\n\n1. **Run the Container**: After successfully building the image, you can run the container. This will start the LMC-compatible websocket server, which listens on port 8000.\n\n2. **Access the Server**: Once the container is running, you can access the websocket server from your browser or any websocket client by connecting to `http://<your-docker-host>:8000`.\n\n### Purpose of the Image\n\nThe purpose of this Docker image is to provide a lightweight, isolated environment for running an LMC-compatible websocket server. This server facilitates communication based on the LMC protocol, allowing for real-time interactions and data exchanges. The image includes all necessary dependencies to ensure the server operates correctly and efficiently.\n\nBy following these steps, you will have a fully functional Docker container running the websocket server from the openinterpreter/open-interpreter repository."
    }
  },
  "input_to_gpt": {
    "repo_name": "openinterpreter/open-interpreter",
    "num_code_blocks": 21,
    "total_length": 2640,
    "code_blocks": [
      "interpreter",
      "interpreter",
      "from interpreter import interpreter\n\ninterpreter.chat(\"Plot AAPL and META's normalized stock prices\") # Executes a single command\ninterpreter.chat() # Starts an interactive chat",
      "message = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)",
      "interpreter",
      "interpreter.chat()",
      "message = \"What operating system are we on?\"\n\nfor chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)",
      "interpreter.chat(\"Add subtitles to all videos in /videos.\")\n\n# ... Streams output to your terminal, completes task ...\n\ninterpreter.chat(\"These look great but can you make the subtitles bigger?\")\n\n# ...",
      "interpreter.messages = []",
      "messages = interpreter.chat(\"My name is Killian.\") # Save messages to 'messages'\ninterpreter.messages = [] # Reset interpreter (\"Killian\" will be forgotten)\n\ninterpreter.messages = messages # Resume chat from 'messages' (\"Killian\" will be remembered)",
      "interpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"\nprint(interpreter.system_message)",
      "interpreter --model gpt-3.5-turbo\ninterpreter --model claude-2\ninterpreter --model command-nightly",
      "interpreter.llm.model = \"gpt-3.5-turbo\"",
      "interpreter --api_base \"http://localhost:1234/v1\" --api_key \"fake_key\"",
      "interpreter --local",
      "from interpreter import interpreter\n\ninterpreter.offline = True # Disables online features like Open Procedures\ninterpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI's format\ninterpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires this\ninterpreter.llm.api_base = \"http://localhost:1234/v1\" # Point this at any OpenAI compatible server\n\ninterpreter.chat()",
      "interpreter --local --max_tokens 1000 --context_window 3000",
      "$ interpreter\n...\n> %verbose true <- Turns on verbose mode\n\n> %verbose false <- Turns off verbose mode",
      "# server.py\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom interpreter import interpreter\n\napp = FastAPI()\n\n@app.get(\"/chat\")\ndef chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n\n@app.get(\"/history\")\ndef history_endpoint():\n    return interpreter.messages",
      "npm i -g mintlify@latest",
      "# Assuming you're at the project's root directory\ncd ./docs\n\n# Run the documentation server\nmintlify dev"
    ]
  }
}