{"repo_name": "Significant-Gravitas/AutoGPT", "stars": 179383, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†", "task_description": "ä½¿ç”¨ agent å‘½ä»¤æ¥åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†ã€‚", "example_code": null, "running_command": "cli.py agent", "expected_input": null, "expected_output": null}, {"task_title": "åŸºå‡†æµ‹è¯•", "task_description": "ä½¿ç”¨ benchmark å‘½ä»¤æ¥å¯åŠ¨åŸºå‡†æµ‹è¯•å¹¶åˆ—å‡ºæµ‹è¯•å’Œç±»åˆ«ã€‚", "example_code": null, "running_command": "cli.py benchmark", "expected_input": null, "expected_output": null}, {"task_title": "å®‰è£…ä¾èµ–", "task_description": "ä½¿ç”¨ setup å‘½ä»¤æ¥å®‰è£…ç³»ç»Ÿæ‰€éœ€çš„ä¾èµ–ã€‚", "example_code": null, "running_command": "cli.py setup", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image for the Significant-Gravitas/AutoGPT project, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image is designed to encapsulate the AutoGPT application, which is an advanced AI-driven tool for various tasks. This image includes all necessary dependencies and configurations to ensure the application runs smoothly in a containerized environment. It supports multiple components, including the backend, frontend, market, and classic modules, each contributing to the overall functionality of AutoGPT.\n\n### Building Steps\n1. **Prepare the Environment**: Ensure you have Docker installed on your machine. Verify the installation by running `docker --version`.\n\n2. **Clone the Repository**: Use Git to clone the Significant-Gravitas/AutoGPT repository to your local machine. This will give you access to the project files.\n\n3. **Set Up .dockerignore**: The `.dockerignore` file is crucial as it specifies which files and directories should be excluded from the Docker build context. This helps to keep the image lightweight and focused only on necessary components.\n\n4. **Build the Image**: \n   - Navigate to the root directory of the cloned repository.\n   - Use the Docker build command to create the image. The build process will read the `.dockerignore` file to determine which files to include. It will compile the application, install dependencies specified in the `pyproject.toml` and `poetry.lock` files, and set up the necessary environment configurations.\n\n5. **Verify Dependencies**: The image will include various dependencies for the backend (e.g., web frameworks, database connectors), frontend (e.g., JavaScript libraries), and other components as defined in their respective `pyproject.toml` files. Ensure that the image is built successfully without errors.\n\n6. **Run the Container**: After the image is built, you can run a container from this image. This will start the AutoGPT application, allowing you to interact with it as intended.\n\n### Built Image Details\n- **Dependencies**: The image will contain libraries and tools required for both backend and frontend operations, including but not limited to Python packages, Node.js packages, and any other necessary runtime dependencies.\n- **Configuration**: Environment variables and configuration files are included to ensure that the application can connect to databases and other services as needed.\n\nBy following these steps, you will successfully build and run the Docker image for the AutoGPT project, enabling you to leverage its capabilities in a controlled and reproducible environment."}}, "timestamp": "2025-10-30T22:20:10.268470"}
{"repo_name": "ytdl-org/youtube-dl", "stars": 138632, "language": "Python", "tasks": [{"task_title": "è·å–è§†é¢‘æ–‡ä»¶å", "task_description": "ä½¿ç”¨youtube-dlè·å–è§†é¢‘çš„æ–‡ä»¶åï¼Œæ”¯æŒç‰¹æ®Šå­—ç¬¦ã€‚", "example_code": null, "running_command": "youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl test video ''_Ã¤â†­ğ•.mp4"}, {"task_title": "è·å–ç®€åŒ–æ–‡ä»¶å", "task_description": "ä½¿ç”¨youtube-dlè·å–è§†é¢‘çš„æ–‡ä»¶åï¼Œé™åˆ¶æ–‡ä»¶åä¸­çš„å­—ç¬¦ã€‚", "example_code": null, "running_command": "youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl_test_video_.mp4"}, {"task_title": "ä¸‹è½½YouTubeæ’­æ”¾åˆ—è¡¨", "task_description": "ä¸‹è½½YouTubeæ’­æ”¾åˆ—è¡¨ä¸­çš„è§†é¢‘ï¼Œå¹¶æŒ‰æ’­æ”¾é¡ºåºä¿å­˜åˆ°ç‹¬ç«‹ç›®å½•ã€‚", "example_code": null, "running_command": "youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_input": "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_output": "ä¸‹è½½çš„æ–‡ä»¶æŒ‰é¡ºåºä¿å­˜"}, {"task_title": "ä¸‹è½½ç”¨æˆ·çš„æ‰€æœ‰æ’­æ”¾åˆ—è¡¨", "task_description": "ä¸‹è½½YouTubeé¢‘é“/ç”¨æˆ·çš„æ‰€æœ‰æ’­æ”¾åˆ—è¡¨ï¼Œå¹¶å°†æ¯ä¸ªæ’­æ”¾åˆ—è¡¨ä¿å­˜åœ¨ç‹¬ç«‹ç›®å½•ä¸­ã€‚", "example_code": null, "running_command": "youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists", "expected_input": "https://www.youtube.com/user/TheLinuxFoundation/playlists", "expected_output": "ä¸‹è½½çš„æ–‡ä»¶æŒ‰ç”¨æˆ·å’Œæ’­æ”¾åˆ—è¡¨ç»“æ„ä¿å­˜"}, {"task_title": "ä¸‹è½½Udemyè¯¾ç¨‹", "task_description": "ä¸‹è½½Udemyè¯¾ç¨‹ï¼Œå°†æ¯ä¸ªç« èŠ‚ä¿å­˜åœ¨ç‹¬ç«‹ç›®å½•ä¸­ã€‚", "example_code": null, "running_command": "youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/", "expected_input": "https://www.udemy.com/java-tutorial/", "expected_output": "ä¸‹è½½çš„æ–‡ä»¶æŒ‰ç« èŠ‚ç»“æ„ä¿å­˜"}, {"task_title": "ä¸‹è½½æ•´ä¸ªç³»åˆ—çš„å­£", "task_description": "ä¸‹è½½è§†é¢‘ç³»åˆ—ï¼Œå°†æ¯ä¸ªç³»åˆ—å’Œæ¯ä¸ªå­£ä¿å­˜åœ¨ç‹¬ç«‹ç›®å½•ä¸­ã€‚", "example_code": null, "running_command": "youtube-dl -o 'C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s' https://videomore.ru/kino_v_detalayah/5_sezon/367617", "expected_input": "https://videomore.ru/kino_v_detalayah/5_sezon/367617", "expected_output": "ä¸‹è½½çš„æ–‡ä»¶æŒ‰ç³»åˆ—å’Œå­£ç»“æ„ä¿å­˜"}, {"task_title": "å°†è§†é¢‘æµç›´æ¥è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º", "task_description": "ä½¿ç”¨youtube-dlå°†ä¸‹è½½çš„è§†é¢‘æµç›´æ¥è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºã€‚", "example_code": null, "running_command": "youtube-dl -o - BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "è§†é¢‘æµæ•°æ®"}, {"task_title": "ä¸‹è½½æœ€ä½³mp4æ ¼å¼", "task_description": "ä¸‹è½½å¯ç”¨çš„æœ€ä½³mp4æ ¼å¼è§†é¢‘ï¼Œå¦‚æœæ²¡æœ‰mp4æ ¼å¼åˆ™ä¸‹è½½å…¶ä»–æœ€ä½³æ ¼å¼ã€‚", "example_code": null, "running_command": "youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³mp4æ ¼å¼è§†é¢‘"}, {"task_title": "ä¸‹è½½æœ€é«˜480pçš„è§†é¢‘", "task_description": "ä¸‹è½½æœ€ä½³æ ¼å¼çš„è§†é¢‘ï¼Œä½†ä¸è¶…è¿‡480pã€‚", "example_code": null, "running_command": "youtube-dl -f 'bestvideo[height<=480]+bestaudio/best[height<=480]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€é«˜480pæ ¼å¼è§†é¢‘"}, {"task_title": "ä¸‹è½½æœ€å¤§50MBçš„è§†é¢‘", "task_description": "ä¸‹è½½æœ€ä½³è§†é¢‘æ ¼å¼ï¼Œä½†æ–‡ä»¶å¤§å°ä¸è¶…è¿‡50MBã€‚", "example_code": null, "running_command": "youtube-dl -f 'best[filesize<50M]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€å¤§50MBæ ¼å¼è§†é¢‘"}, {"task_title": "é€šè¿‡HTTP/HTTPSåè®®ä¸‹è½½æœ€ä½³æ ¼å¼", "task_description": "ä¸‹è½½æœ€ä½³æ ¼å¼çš„è§†é¢‘ï¼Œé€šè¿‡ç›´æ¥é“¾æ¥ä½¿ç”¨HTTP/HTTPSåè®®ã€‚", "example_code": null, "running_command": "youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³æ ¼å¼è§†é¢‘"}, {"task_title": "åˆ†åˆ«ä¸‹è½½æœ€ä½³è§†é¢‘å’Œæœ€ä½³éŸ³é¢‘æ ¼å¼", "task_description": "ä¸‹è½½æœ€ä½³è§†é¢‘æ ¼å¼å’Œæœ€ä½³éŸ³é¢‘æ ¼å¼ï¼Œä½†ä¸åˆå¹¶å®ƒä»¬ã€‚", "example_code": null, "running_command": "youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³è§†é¢‘å’ŒéŸ³é¢‘æ ¼å¼"}, {"task_title": "ä¸‹è½½æœ€è¿‘6ä¸ªæœˆä¸Šä¼ çš„è§†é¢‘", "task_description": "ä½¿ç”¨youtube-dlä¸‹è½½æœ€è¿‘6ä¸ªæœˆå†…ä¸Šä¼ çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "youtube-dl --dateafter now-6months", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€è¿‘6ä¸ªæœˆçš„è§†é¢‘"}, {"task_title": "ä¸‹è½½ç‰¹å®šæ—¥æœŸä¸Šä¼ çš„è§†é¢‘", "task_description": "ä½¿ç”¨youtube-dlä¸‹è½½åœ¨ç‰¹å®šæ—¥æœŸï¼ˆ1970å¹´1æœˆ1æ—¥ï¼‰ä¸Šä¼ çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "youtube-dl --date 19700101", "expected_input": null, "expected_output": "ä¸‹è½½çš„ç‰¹å®šæ—¥æœŸçš„è§†é¢‘"}, {"task_title": "ä¸‹è½½ç‰¹å®šåå¹´ä¸Šä¼ çš„è§†é¢‘", "task_description": "ä½¿ç”¨youtube-dlä¸‹è½½åœ¨2000å¹´ä»£ä¸Šä¼ çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "youtube-dl --dateafter 20000101 --datebefore 20091231", "expected_input": null, "expected_output": "ä¸‹è½½çš„ç‰¹å®šåå¹´çš„è§†é¢‘"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:21:07.946939"}
{"repo_name": "langflow-ai/langflow", "stars": 136684, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒLangFlow", "task_description": "ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è¿è¡ŒLangFlowï¼Œå¯åŠ¨åº”ç”¨ç¨‹åºã€‚", "example_code": null, "running_command": "uv run langflow run", "expected_input": null, "expected_output": "åº”ç”¨ç¨‹åºå¯åŠ¨å¹¶è¿è¡Œ"}], "setup": {"setup_commands": ["uv pip install langflow -U"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image for the `langflow-ai/langflow` repository, follow these steps, which leverage the provided `.dockerignore` file to optimize the build process by excluding unnecessary files and directories.\n\n### Purpose of the Docker Image\nThe Docker image for the `langflow-ai/langflow` project is designed to encapsulate the application environment, ensuring that all necessary dependencies are included for running the application seamlessly. This image typically contains the application code, runtime libraries, and any tools required for development or production use.\n\n### Dependencies\nThe image will likely include:\n- A base operating system (e.g., Ubuntu or Alpine).\n- Language runtimes (e.g., Python, Node.js) depending on the application stack.\n- Frameworks and libraries required for the application to function (e.g., Flask, React).\n- Any additional tools or utilities necessary for building or running the application.\n\n### Building Steps\n1. **Prepare the Environment**: Ensure that Docker is installed and running on your machine.\n\n2. **Clone the Repository**: Start by cloning the `langflow-ai/langflow` repository to your local machine.\n\n3. **Navigate to the Directory**: Change into the directory of the cloned repository where the Dockerfile is located.\n\n4. **Review the `.dockerignore` File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This helps to reduce the size of the image and speeds up the build process by omitting unnecessary files such as:\n   - Node modules and build artifacts from the frontend.\n   - Test results and coverage reports.\n   - Python cache files and virtual environments.\n   - System-specific files like `.DS_Store`.\n\n5. **Build the Docker Image**: Use the Docker build command to create the image. This process will read the Dockerfile, install the necessary dependencies, and package the application into a single image.\n\n6. **Tag the Image**: Optionally, tag the image with a meaningful name and version for easier identification.\n\n### Running the Docker Image\n1. **Run the Container**: After successfully building the image, use the Docker run command to start a container from the image. Ensure to specify any necessary environment variables and port mappings as required by the application.\n\n2. **Access the Application**: Once the container is running, you can access the application through the specified ports, typically via a web browser or API client.\n\n3. **Monitor Logs**: Use Docker commands to view logs and monitor the applicationâ€™s performance and behavior within the container.\n\nBy following these steps, you can effectively build and run the Docker image for the `langflow-ai/langflow` repository, ensuring a consistent and isolated environment for your application."}}, "timestamp": "2025-10-30T22:21:28.645772"}
{"repo_name": "yt-dlp/yt-dlp", "stars": 133294, "language": "Python", "tasks": [{"task_title": "Download video with custom filename", "task_description": "ä½¿ç”¨æŒ‡å®šçš„æ¨¡æ¿ä¸‹è½½è§†é¢‘ï¼Œå¹¶æ‰“å°æ–‡ä»¶åã€‚", "example_code": null, "running_command": "yt-dlp --print filename -o \"test video.%(ext)s\" BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "test video.webm"}, {"task_title": "Download video with title as filename", "task_description": "ä½¿ç”¨è§†é¢‘æ ‡é¢˜ä½œä¸ºæ–‡ä»¶åï¼Œå¹¶å¤„ç†ç‰¹æ®Šå­—ç¬¦ã€‚", "example_code": null, "running_command": "yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl test video ''_Ã¤â†­ğ•.webm"}, {"task_title": "Restrict filenames", "task_description": "ä¸‹è½½è§†é¢‘å¹¶é™åˆ¶æ–‡ä»¶åçš„å­—ç¬¦ã€‚", "example_code": null, "running_command": "yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc --restrict-filenames", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl_test_video_.webm"}, {"task_title": "Download YouTube playlist videos in separate directory", "task_description": "ä¸‹è½½YouTubeæ’­æ”¾åˆ—è¡¨ä¸­çš„è§†é¢‘ï¼Œå¹¶æŒ‰æ’­æ”¾é¡ºåºåœ¨ä¸åŒç›®å½•ä¸­ä¿å­˜ã€‚", "example_code": null, "running_command": "yt-dlp -o \"%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"", "expected_input": "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_output": "Videos downloaded in separate directories."}, {"task_title": "Download Udemy course in structured directories", "task_description": "ä¸‹è½½Udemyè¯¾ç¨‹å¹¶åœ¨æŒ‡å®šç›®å½•ä¸­ä¿æŒç« èŠ‚ç»“æ„ã€‚", "example_code": null, "running_command": "yt-dlp -u user -p password -P \"~/MyVideos\" -o \"%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s\" \"https://www.udemy.com/java-tutorial\"", "expected_input": "https://www.udemy.com/java-tutorial", "expected_output": "Course downloaded in structured directories."}, {"task_title": "Download video and subtitles to specified locations", "task_description": "ä¸‹è½½è§†é¢‘å’Œå­—å¹•åˆ°æŒ‡å®šçš„ä¸åŒç›®å½•ã€‚", "example_code": null, "running_command": "yt-dlp -P \"C:/MyVideos\" -P \"temp:tmp\" -P \"subtitle:subs\" -o \"%(uploader)s/%(title)s.%(ext)s\" BaW_jenozKc --write-subs", "expected_input": "BaW_jenozKc", "expected_output": "Video and subtitles downloaded successfully."}, {"task_title": "Stream video to stdout", "task_description": "å°†ä¸‹è½½çš„è§†é¢‘æµè¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºã€‚", "example_code": null, "running_command": "yt-dlp -o - BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "Streaming video content."}, {"task_title": "Download best video and audio formats", "task_description": "ä¸‹è½½æœ€ä½³è§†é¢‘å’ŒéŸ³é¢‘æ ¼å¼ï¼Œå¹¶è¿›è¡Œåˆå¹¶ã€‚", "example_code": null, "running_command": "yt-dlp -f \"bv+ba/b\"", "expected_input": "BaW_jenozKc", "expected_output": "Best video and audio formats downloaded and merged."}, {"task_title": "Download video with specific codec", "task_description": "ä¸‹è½½æŒ‡å®šç¼–ç æ ¼å¼çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "yt-dlp -f \"(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)\"", "expected_input": "BaW_jenozKc", "expected_output": "Best video with specified codec downloaded."}, {"task_title": "Parse metadata from video", "task_description": "ä»è§†é¢‘ä¸­è§£æå…ƒæ•°æ®å¹¶è¿›è¡Œå¤„ç†ã€‚", "example_code": null, "running_command": "yt-dlp --parse-metadata \"title:%(artist)s - %(title)s\"", "expected_input": "BaW_jenozKc", "expected_output": "Metadata parsed successfully."}, {"task_title": "Download videos based on duration filter", "task_description": "ä»…ä¸‹è½½æ—¶é•¿è¶…è¿‡ä¸€åˆ†é’Ÿçš„è§†é¢‘ã€‚", "example_code": null, "running_command": "yt-dlp -o \"%(title)s.%(ext)s\" --match-filter \"duration > 60\"", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Videos longer than a minute downloaded."}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:22:25.031041"}
{"repo_name": "deepseek-ai/DeepSeek-V3", "stars": 100074, "language": "Python", "tasks": [{"task_title": "FP8æƒé‡è½¬æ¢ä¸ºBF16æ ¼å¼", "task_description": "è¯¥ä»»åŠ¡å°†FP8æ ¼å¼çš„æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºBF16æ ¼å¼ï¼Œä½¿ç”¨äº†å‘½ä»¤è¡Œå·¥å…·è¿›è¡Œæ–‡ä»¶è·¯å¾„çš„æŒ‡å®šã€‚", "example_code": null, "running_command": "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights", "expected_input": "/path/to/fp8_weights, /path/to/bf16_weights", "expected_output": "è½¬æ¢æˆåŠŸçš„BF16æƒé‡æ–‡ä»¶"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:22:40.791487"}
{"repo_name": "nvbn/thefuck", "stars": 94519, "language": "Python", "tasks": [{"task_title": "Fix git push error", "task_description": "Automatically sets the upstream branch for a git push command when the current branch has no upstream branch.", "example_code": null, "running_command": "fuck", "expected_input": "git push", "expected_output": "git push --set-upstream origin master"}, {"task_title": "Correct a misspelled command", "task_description": "Suggests the correct command when a misspelled command is input, such as 'puthon' instead of 'python'.", "example_code": null, "running_command": "fuck", "expected_input": "puthon", "expected_output": "python"}, {"task_title": "Fix unknown git command", "task_description": "Provides the correct git command when an unknown command is entered, such as 'brnch'.", "example_code": null, "running_command": "fuck", "expected_input": "git brnch", "expected_output": "git branch"}, {"task_title": "Fix unknown Leiningen task", "task_description": "Suggests the correct Leiningen task when an unknown task is provided, such as 'rpl'.", "example_code": null, "running_command": "fuck", "expected_input": "lein rpl", "expected_output": "lein repl"}, {"task_title": "Run thefuck with confirmation", "task_description": "Uses thefuck with a confirmation prompt, allowing the user to approve the suggested correction.", "example_code": null, "running_command": "fuck --yeah", "expected_input": null, "expected_output": null}, {"task_title": "Re-run the last command", "task_description": "Re-runs the last suggested command without requiring user input.", "example_code": null, "running_command": "fuck -r", "expected_input": null, "expected_output": null}, {"task_title": "Match command for permission errors", "task_description": "Matches commands that result in permission denied errors and suggests a fix by prepending 'sudo'.", "example_code": "def match(command):\n    return ('permission denied' in command.output.lower()\n            or 'EACCES' in command.output)\n\ndef get_new_command(command):\n    return 'sudo {}'.format(command.script)", "running_command": null, "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim [enter/â†‘/â†“/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...", "âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim\n[sudo] password for nvbn:\nReading package lists... Done\n...", "brew install thefuck", "pip install thefuck"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:23:01.505754"}
{"repo_name": "comfyanonymous/ComfyUI", "stars": 92345, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒComfyUIå¯¹äºAMDæ˜¾å¡çš„æ”¯æŒ", "task_description": "ä½¿ç”¨ç‰¹å®šçš„ç¯å¢ƒå˜é‡æ¥æ”¯æŒAMDæ˜¾å¡ï¼Œè§£å†³ROCmä¸å…¼å®¹çš„é—®é¢˜ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py", "expected_input": null, "expected_output": null}, {"task_title": "è¿è¡ŒComfyUIå¯¹äºRDNA3æ˜¾å¡çš„æ”¯æŒ", "task_description": "ä½¿ç”¨ç‰¹å®šçš„ç¯å¢ƒå˜é‡æ¥æ”¯æŒAMD RDNA3æ˜¾å¡ï¼Œè§£å†³ROCmä¸å…¼å®¹çš„é—®é¢˜ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["pip install comfy-cli\ncomfy install"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:23:18.937901"}
{"repo_name": "fastapi/fastapi", "stars": 91344, "language": "Python", "tasks": [{"task_title": "Read Root Endpoint", "task_description": "å®šä¹‰æ ¹è·¯å¾„çš„GETè¯·æ±‚ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "Read Item Endpoint", "task_description": "å®šä¹‰å¸¦æœ‰è·¯å¾„å‚æ•°çš„GETè¯·æ±‚ï¼Œè¿”å›æŒ‡å®šitem_idå’Œå¯é€‰æŸ¥è¯¢å‚æ•°qçš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": "item_id=1&q=test", "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "Asynchronous Read Root Endpoint", "task_description": "å®šä¹‰æ ¹è·¯å¾„çš„GETè¯·æ±‚ï¼Œä½¿ç”¨å¼‚æ­¥å‡½æ•°è¿”å›JSONå“åº”ã€‚", "example_code": "@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "Asynchronous Read Item Endpoint", "task_description": "å®šä¹‰å¸¦æœ‰è·¯å¾„å‚æ•°çš„å¼‚æ­¥GETè¯·æ±‚ï¼Œè¿”å›æŒ‡å®šitem_idå’Œå¯é€‰æŸ¥è¯¢å‚æ•°qçš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": "item_id=1&q=test", "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}], "setup": {"setup_commands": ["$ pip install \"fastapi[standard]\"\n\n---> 100%"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:23:32.759649"}
{"repo_name": "openai/whisper", "stars": 90154, "language": "Python", "tasks": [{"task_title": "Transcribe audio to text", "task_description": "ä½¿ç”¨turboæ¨¡å‹å°†éŸ³é¢‘æ–‡ä»¶è½¬å½•ä¸ºæ–‡æœ¬ã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Transcribed text from audio.mp3"}, {"task_title": "Detect language from audio", "task_description": "åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼Œæ£€æµ‹å…¶ä¸­çš„è¯­è¨€ã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Detected language: [detected language]"}, {"task_title": "Translate audio to text", "task_description": "ä½¿ç”¨mediumæ¨¡å‹å°†éŸ³é¢‘æ–‡ä»¶è½¬å½•å¹¶ç¿»è¯‘æˆæŒ‡å®šè¯­è¨€ã€‚", "example_code": null, "running_command": "whisper japanese.wav --model medium --language Japanese --task translate", "expected_input": "japanese.wav", "expected_output": "Translated text from japanese.wav"}, {"task_title": "Specify language for transcription", "task_description": "æŒ‡å®šéŸ³é¢‘æ–‡ä»¶çš„è¯­è¨€è¿›è¡Œè½¬å½•ã€‚", "example_code": null, "running_command": "whisper japanese.wav --language Japanese", "expected_input": "japanese.wav", "expected_output": "Transcribed text from japanese.wav in Japanese"}, {"task_title": "Get help information", "task_description": "æ˜¾ç¤ºwhisperå‘½ä»¤è¡Œå·¥å…·çš„å¸®åŠ©ä¿¡æ¯ã€‚", "example_code": null, "running_command": "whisper --help", "expected_input": null, "expected_output": "Help information for whisper command"}], "setup": {"setup_commands": ["# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg", "pip install setuptools-rust"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T22:23:53.215975"}
{"repo_name": "microsoft/markitdown", "stars": 82291, "language": "Python", "tasks": [{"task_title": "Convert PDF to Markdown", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æ¡£ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf > document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF to Markdown with Output File Option", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF from Standard Input", "task_description": "ä»æ ‡å‡†è¾“å…¥è¯»å–PDFæ–‡ä»¶å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "cat path-to-file.pdf | markitdown", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "List Available Plugins", "task_description": "åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --list-plugins", "expected_input": null, "expected_output": "å¯ç”¨æ’ä»¶åˆ—è¡¨"}, {"task_title": "Use Plugins for Conversion", "task_description": "åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ä½¿ç”¨æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --use-plugins path-to-file.pdf", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownï¼Œå¹¶æŒ‡å®šæ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"", "expected_input": "path-to-file.pdf, <document_intelligence_endpoint>", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert Excel File to Markdown without Plugins", "task_description": "å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œç¦ç”¨æ’ä»¶ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) \nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.xlsx", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownï¼Œå¹¶ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.pdf, <document_intelligence_endpoint>", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert Image File with OpenAI Integration", "task_description": "å°†å›¾åƒæ–‡ä»¶è½¬æ¢ä¸ºMarkdownï¼ŒåŒæ—¶é›†æˆOpenAIå®¢æˆ·ç«¯ã€‚", "example_code": "from markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)", "running_command": null, "expected_input": "example.jpg", "expected_output": "Markdownå†…å®¹"}], "setup": {"setup_commands": ["python -m venv .venv\nsource .venv/bin/activate", "uv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment", "conda create -n markitdown python=3.12\nconda activate markitdown", "git clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'", "pip install 'markitdown[pdf, docx, pptx]'", "  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test"], "docker_commands": ["docker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the `markitdown` project from the `microsoft/markitdown` repository, follow these steps:\n\n### Building the Docker Image\n\n1. **Set Up Your Environment**: Ensure you have Docker installed on your machine. You can verify this by running `docker --version` in your terminal.\n\n2. **Clone the Repository**: Clone the `microsoft/markitdown` repository to your local machine. This will give you access to the `Dockerfile` and other necessary files.\n\n3. **Navigate to the Directory**: Change your working directory to the location of the cloned repository where the `Dockerfile` is located.\n\n4. **Build the Image**: Execute the Docker build command to create the image. This process will:\n   - Start from a lightweight Python 3.13 image based on Debian Bullseye.\n   - Set up a non-interactive environment for package installation.\n   - Install runtime dependencies such as `ffmpeg` and `exiftool`, which are essential for processing media files.\n   - Optionally, if specified, install Git for version control capabilities.\n   - Clean up unnecessary files to reduce the image size.\n   - Copy the application code into the image and install the required Python packages, including `markitdown` and a sample plugin.\n   - Set the user permissions to enhance security by running the application as a non-root user.\n\n5. **Tag the Image**: The image will be tagged as `markitdown:latest` upon successful build.\n\n### Running the Docker Container\n\n1. **Run the Container**: Use the Docker run command to execute the container. This command will:\n   - Remove the container after it exits to keep your environment clean.\n   - Accept input from a PDF file located on your local machine and convert it to Markdown format.\n   - Output the converted Markdown to a specified file.\n\n### Purpose of the Image\n\nThe `markitdown` Docker image is designed to facilitate the conversion of PDF documents into Markdown format. It leverages Python along with essential tools like `ffmpeg` for media processing and `exiftool` for metadata extraction. This makes it a powerful tool for users needing to transform documents for web publishing or further editing.\n\n### Dependencies\n\nThe image relies on:\n- **Python 3.13**: The core language for running the application.\n- **ffmpeg**: A multimedia framework for handling video and audio files.\n- **exiftool**: A tool for reading, writing, and editing metadata in files.\n- **Optional Git**: For version control, if needed.\n\nBy following these steps, you will successfully build and run the `markitdown` Docker image, enabling you to convert PDF files to Markdown format efficiently.", ".dockerignore": "To build and run the Docker image for the `markitdown` project from the `microsoft/markitdown` repository, follow these steps:\n\n### Purpose of the Image\nThe `markitdown` Docker image is designed to convert PDF documents into Markdown format. This is particularly useful for users who need to extract text and formatting from PDF files for easier editing or integration into other applications.\n\n### Dependencies\nThe image is built on a base that includes essential tools and libraries required for processing PDF files and converting them into Markdown. These dependencies ensure that the conversion process is efficient and reliable.\n\n### Building the Image\n1. **Clone the Repository**: Start by cloning the `microsoft/markitdown` repository to your local machine. This will give you access to the necessary files, including the `.dockerignore` file, which specifies which files should be excluded from the build context.\n\n2. **Navigate to the Directory**: Change your working directory to the location of the cloned repository where the Dockerfile is located.\n\n3. **Build the Docker Image**: Use the Docker build command to create the image. The build process will read the Dockerfile and the `.dockerignore` file to determine which files to include or exclude. The resulting image will be tagged as `markitdown:latest`.\n\n### Running the Image\n1. **Execute the Container**: Once the image is built, you can run it using the Docker run command. This command will create a container from the `markitdown` image and execute the conversion process.\n\n2. **Input and Output**: Provide the path to your PDF file as input. The container will read the PDF and output the converted Markdown content to a specified file, which you can define in your command.\n\n### Summary\nBy following these steps, you will successfully build and run the `markitdown` Docker image, allowing you to convert PDF documents into Markdown format efficiently. Ensure that you have Docker installed and running on your machine before proceeding with these steps."}}, "timestamp": "2025-10-30T22:24:41.232524"}
{"repo_name": "Significant-Gravitas/AutoGPT", "stars": 179385, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†", "task_description": "ä½¿ç”¨ agent å‘½ä»¤æ¥åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†ã€‚", "example_code": null, "running_command": "cli.py agent", "expected_input": null, "expected_output": "ç›¸å…³çš„ä»£ç†æ“ä½œè¾“å‡º"}, {"task_title": "åŸºå‡†æµ‹è¯•", "task_description": "ä½¿ç”¨ benchmark å‘½ä»¤æ¥å¯åŠ¨åŸºå‡†æµ‹è¯•å¹¶åˆ—å‡ºæµ‹è¯•å’Œç±»åˆ«ã€‚", "example_code": null, "running_command": "cli.py benchmark", "expected_input": null, "expected_output": "åŸºå‡†æµ‹è¯•ç»“æœåŠæµ‹è¯•ç±»åˆ«çš„åˆ—è¡¨"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image for the Significant-Gravitas/AutoGPT project, follow these actionable steps:\n\n### Purpose of the Image\nThe Docker image is designed to encapsulate the AutoGPT application, which is an advanced AI tool for autonomous task execution. The image includes all necessary dependencies for both the backend and frontend components, ensuring a consistent environment for development and deployment.\n\n### Dependencies\nThe image relies on several key dependencies, including:\n- **Python Libraries**: Managed through Poetry, with dependencies specified in `pyproject.toml` and locked in `poetry.lock` files across various components (platform, classic).\n- **Frontend Frameworks**: JavaScript dependencies defined in `package.json` and managed with a package manager (like pnpm).\n- **Database Migrations**: Necessary files for database schema management.\n- **Environment Configuration**: `.env` files that contain sensitive configuration details required for the application to function correctly.\n\n### Building Steps\n1. **Prepare the Environment**: Ensure Docker is installed and running on your machine.\n2. **Clone the Repository**: Obtain the Significant-Gravitas/AutoGPT repository from GitHub to your local machine.\n3. **Navigate to the Project Directory**: Open a terminal and change to the directory where the repository is located.\n4. **Create the `.dockerignore` File**: This file is crucial as it specifies which files and directories should be excluded from the Docker build context. The provided `.dockerignore` file ensures that only relevant files are included, optimizing the build process.\n5. **Build the Docker Image**: Use Dockerâ€™s build command to create the image. This process will read the Dockerfile (not disclosed here) and the `.dockerignore` file to include only the necessary files and dependencies.\n6. **Verify the Image**: Once the build completes, check that the image has been created successfully by listing your Docker images.\n\n### Running the Image\n1. **Run the Docker Container**: Use Dockerâ€™s run command to start a container from the built image. Ensure to pass any necessary environment variables and port mappings to make the application accessible.\n2. **Access the Application**: Once the container is running, you can access the AutoGPT application through your web browser or API client, depending on its configuration.\n\nBy following these steps, you will successfully build and run the AutoGPT Docker image, enabling you to leverage its capabilities in a controlled and reproducible environment."}}, "timestamp": "2025-10-30T23:28:16.324119"}
{"repo_name": "ytdl-org/youtube-dl", "stars": 138633, "language": "Python", "tasks": [{"task_title": "è·å–è§†é¢‘æ–‡ä»¶å", "task_description": "ä½¿ç”¨youtube-dlè·å–æŒ‡å®šè§†é¢‘çš„æ–‡ä»¶åï¼Œæ”¯æŒç‰¹æ®Šå­—ç¬¦ã€‚", "example_code": null, "running_command": "youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl test video ''_Ã¤â†­ğ•.mp4"}, {"task_title": "è·å–ç®€åŒ–æ–‡ä»¶å", "task_description": "ä½¿ç”¨youtube-dlè·å–æŒ‡å®šè§†é¢‘çš„ç®€åŒ–æ–‡ä»¶åï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦ã€‚", "example_code": null, "running_command": "youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl_test_video_.mp4"}, {"task_title": "ä¸‹è½½YouTubeæ’­æ”¾åˆ—è¡¨", "task_description": "ä¸‹è½½YouTubeæ’­æ”¾åˆ—è¡¨ä¸­çš„è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘ä¿å­˜åœ¨ç‹¬ç«‹ç›®å½•ä¸­ï¼Œå¹¶æŒ‰æ’­æ”¾é¡ºåºç´¢å¼•ã€‚", "example_code": null, "running_command": "youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_input": "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_output": "ä¸‹è½½çš„æ’­æ”¾åˆ—è¡¨è§†é¢‘æ–‡ä»¶"}, {"task_title": "ä¸‹è½½Udemyè¯¾ç¨‹", "task_description": "ä¸‹è½½Udemyè¯¾ç¨‹ï¼Œä¿æŒæ¯ä¸ªç« èŠ‚åœ¨ç‹¬ç«‹ç›®å½•ä¸‹ã€‚", "example_code": null, "running_command": "youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/", "expected_input": "https://www.udemy.com/java-tutorial/", "expected_output": "ä¸‹è½½çš„è¯¾ç¨‹è§†é¢‘æ–‡ä»¶"}, {"task_title": "ä¸‹è½½ç³»åˆ—è§†é¢‘", "task_description": "ä¸‹è½½æ•´ä¸ªç³»åˆ—çš„å­£ï¼Œä¿æŒæ¯ä¸ªç³»åˆ—å’Œæ¯ä¸ªå­£çš„ç‹¬ç«‹ç›®å½•ã€‚", "example_code": null, "running_command": "youtube-dl -o 'C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s' https://videomore.ru/kino_v_detalayah/5_sezon/367617", "expected_input": "https://videomore.ru/kino_v_detalayah/5_sezon/367617", "expected_output": "ä¸‹è½½çš„ç³»åˆ—è§†é¢‘æ–‡ä»¶"}, {"task_title": "æµå¼ä¸‹è½½è§†é¢‘", "task_description": "å°†ä¸‹è½½çš„è§†é¢‘æµå¼è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºã€‚", "example_code": null, "running_command": "youtube-dl -o - BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "è§†é¢‘æµæ•°æ®"}, {"task_title": "ä¸‹è½½æœ€ä½³mp4æ ¼å¼", "task_description": "ä¸‹è½½å¯ç”¨çš„æœ€ä½³mp4æ ¼å¼æˆ–å…¶ä»–æ ¼å¼ï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨çš„mp4ã€‚", "example_code": null, "running_command": "youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³mp4è§†é¢‘æ–‡ä»¶"}, {"task_title": "é™åˆ¶ä¸‹è½½480p", "task_description": "ä¸‹è½½æœ€ä½³æ ¼å¼ï¼Œä½†ä¸è¶…è¿‡480pã€‚", "example_code": null, "running_command": "youtube-dl -f 'bestvideo[height<=480]+bestaudio/best[height<=480]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„480pè§†é¢‘æ–‡ä»¶"}, {"task_title": "é™åˆ¶æ–‡ä»¶å¤§å°ä¸‹è½½", "task_description": "ä¸‹è½½æœ€ä½³è§†é¢‘æ ¼å¼ï¼Œä½†ä¸è¶…è¿‡50 MBã€‚", "example_code": null, "running_command": "youtube-dl -f 'best[filesize<50M]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³è§†é¢‘æ–‡ä»¶ï¼Œå¤§å°å°äº50MB"}, {"task_title": "é€šè¿‡HTTP/HTTPSåè®®ä¸‹è½½", "task_description": "ä¸‹è½½æœ€ä½³æ ¼å¼ï¼Œä½†åªé€šè¿‡HTTP/HTTPSåè®®ã€‚", "example_code": null, "running_command": "youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³è§†é¢‘æ–‡ä»¶"}, {"task_title": "ä¸‹è½½ç‰¹å®šæ—¥æœŸä¹‹åçš„è§†é¢‘", "task_description": "ä¸‹è½½åœ¨è¿‡å»6ä¸ªæœˆå†…ä¸Šä¼ çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "youtube-dl --dateafter now-6months", "expected_input": null, "expected_output": "ä¸‹è½½çš„ç¬¦åˆæ¡ä»¶çš„è§†é¢‘"}, {"task_title": "ä¸‹è½½ç‰¹å®šæ—¥æœŸçš„è§†é¢‘", "task_description": "ä¸‹è½½åœ¨1970å¹´1æœˆ1æ—¥ä¸Šä¼ çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "youtube-dl --date 19700101", "expected_input": null, "expected_output": "ä¸‹è½½çš„ç¬¦åˆæ¡ä»¶çš„è§†é¢‘"}, {"task_title": "ä¸‹è½½ç‰¹å®šåå¹´çš„è§†é¢‘", "task_description": "ä¸‹è½½åœ¨2000å¹´ä»£ä¸Šä¼ çš„è§†é¢‘ã€‚", "example_code": null, "running_command": "youtube-dl --dateafter 20000101 --datebefore 20091231", "expected_input": null, "expected_output": "ä¸‹è½½çš„ç¬¦åˆæ¡ä»¶çš„è§†é¢‘"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:28:55.838623"}
{"repo_name": "langflow-ai/langflow", "stars": 136694, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒLangflow", "task_description": "å¯åŠ¨Langflowåº”ç”¨ç¨‹åºï¼Œæä¾›ä¸€ä¸ªè¿è¡Œç¯å¢ƒä»¥å¤„ç†ç›¸å…³çš„ä»»åŠ¡å’ŒåŠŸèƒ½ã€‚", "example_code": null, "running_command": "uv run langflow run", "expected_input": null, "expected_output": "Langflow application is running..."}], "setup": {"setup_commands": ["uv pip install langflow -U"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image for the `langflow-ai/langflow` repository, follow these steps, taking into account the specified `.dockerignore` file which optimizes the build process by excluding unnecessary files and directories.\n\n### Building the Docker Image\n\n1. **Purpose of the Image**: The Docker image is designed to encapsulate the Langflow application, which is likely a web-based interface for managing language models. It provides a consistent environment for development and deployment, ensuring that all necessary dependencies are included.\n\n2. **Dependencies**: The image will typically include essential components such as:\n   - A web server (e.g., Node.js) for serving the frontend application.\n   - Language model libraries and frameworks required for the application's functionality.\n   - Any additional tools or libraries necessary for building and running the application.\n\n3. **Building Steps**:\n   - **Clone the Repository**: Start by cloning the `langflow-ai/langflow` repository to your local machine.\n   - **Navigate to the Directory**: Change into the directory where the Dockerfile is located.\n   - **Docker Build Command**: Execute the Docker build command, which will read the Dockerfile and create an image. The `.dockerignore` file will ensure that specified directories and files (like `node_modules`, build artifacts, and temporary files) are excluded from the build context, resulting in a smaller and more efficient image.\n\n### Running the Docker Container\n\n1. **Run the Image**: After successfully building the image, you can run it using the Docker run command. This will start a container based on the image, allowing you to interact with the Langflow application.\n\n2. **Accessing the Application**: Typically, the application will be accessible via a web browser at a specified port. Ensure that you map the container's port to a port on your host machine to access the application.\n\n### Summary\n\nBy following these steps, you will have a Docker image that encapsulates the Langflow application, complete with all necessary dependencies while excluding unnecessary files as specified in the `.dockerignore`. This approach ensures a clean and efficient build process, facilitating easy deployment and scaling of the application."}}, "timestamp": "2025-10-30T23:29:12.466045"}
{"repo_name": "yt-dlp/yt-dlp", "stars": 133305, "language": "Python", "tasks": [{"task_title": "Download video with specified filename format", "task_description": "This command downloads a video and saves it with a specified filename format, including the correct extension.", "example_code": null, "running_command": "yt-dlp --print filename -o \"test video.%(ext)s\" BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "test video.webm"}, {"task_title": "Download video with title as filename", "task_description": "This command downloads a video and saves it using the title of the video as the filename.", "example_code": null, "running_command": "yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl test video ''_Ã¤â†­ğ•.webm"}, {"task_title": "Download video with restricted filenames", "task_description": "This command downloads a video and saves it with a restricted filename format, removing any weird characters.", "example_code": null, "running_command": "yt-dlp --print filename -o \"%(title)s.%(ext)s\" BaW_jenozKc --restrict-filenames", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl_test_video_.webm"}, {"task_title": "Download YouTube playlist videos in separate directories by order", "task_description": "This command downloads all videos from a YouTube playlist, saving them in a directory structure indexed by their order in the playlist.", "example_code": null, "running_command": "yt-dlp -o \"%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s\" \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"", "expected_input": "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_output": "Downloaded videos saved in a directory structure."}, {"task_title": "Download Udemy course videos by chapter", "task_description": "This command downloads videos from a Udemy course, organizing them by chapter in a specified directory.", "example_code": null, "running_command": "yt-dlp -u user -p password -P \"~/MyVideos\" -o \"%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s\" \"https://www.udemy.com/java-tutorial\"", "expected_input": "https://www.udemy.com/java-tutorial", "expected_output": "Downloaded Udemy course videos organized by chapter."}, {"task_title": "Download video and subtitles to specified directories", "task_description": "This command downloads a video and its subtitles, saving them to specified directories.", "example_code": null, "running_command": "yt-dlp -P \"C:/MyVideos\" -P \"temp:tmp\" -P \"subtitle:subs\" -o \"%(uploader)s/%(title)s.%(ext)s\" BaW_jenozKc --write-subs", "expected_input": "BaW_jenozKc", "expected_output": "Video and subtitles downloaded to specified directories."}, {"task_title": "Download best audio and video formats", "task_description": "This command downloads the best available video and audio formats, merging them if necessary.", "example_code": null, "running_command": "yt-dlp -f \"bv+ba/b\"", "expected_input": "BaW_jenozKc", "expected_output": "Best video and audio formats downloaded and merged."}, {"task_title": "Download video with specific codec preference", "task_description": "This command downloads the best video available with a specified codec, preferring h264 or h265.", "example_code": null, "running_command": "yt-dlp -f \"(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)\"", "expected_input": "BaW_jenozKc", "expected_output": "Best video with specified codec downloaded."}, {"task_title": "Extract audio from video", "task_description": "This command downloads a video and extracts the audio, saving it in a specified format.", "example_code": null, "running_command": "yt-dlp -x --audio-format mp3 BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "Audio extracted and saved in mp3 format."}, {"task_title": "Download video using a custom logger", "task_description": "This command demonstrates using a custom logger to track download progress.", "example_code": null, "running_command": "yt-dlp -o \"%(title)s.%(ext)s\" --logger MyLogger() BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "Download progress logged using custom logger."}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:29:51.026429"}
{"repo_name": "deepseek-ai/DeepSeek-V3", "stars": 100075, "language": "Python", "tasks": [{"task_title": "FP8æƒé‡è½¬æ¢ä¸ºBF16æ ¼å¼", "task_description": "è¯¥ä»»åŠ¡å°†FP8æ ¼å¼çš„æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºBF16æ ¼å¼ï¼Œä»¥ä¾¿äºåœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨ã€‚", "example_code": null, "running_command": "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights", "expected_input": "/path/to/fp8_weights, /path/to/bf16_weights", "expected_output": "è½¬æ¢å®Œæˆï¼Œç”Ÿæˆçš„BF16æƒé‡æ–‡ä»¶ä½äºæŒ‡å®šè·¯å¾„"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:30:06.054542"}
{"repo_name": "nvbn/thefuck", "stars": 94519, "language": "Python", "tasks": [{"task_title": "Fixing git push error", "task_description": "Automatically suggests the correct command to set the upstream branch for git push.", "example_code": null, "running_command": "fuck", "expected_input": "git push", "expected_output": "git push --set-upstream origin master"}, {"task_title": "Correcting command not found error", "task_description": "Suggests the correct command when a misspelled command is entered.", "example_code": null, "running_command": "fuck", "expected_input": "puthon", "expected_output": "python"}, {"task_title": "Fixing misspelled git command", "task_description": "Suggests the correct git command when a misspelled command is entered.", "example_code": null, "running_command": "fuck", "expected_input": "git brnch", "expected_output": "git branch"}, {"task_title": "Fixing Leiningen task error", "task_description": "Suggests the correct Leiningen task when an invalid task is entered.", "example_code": null, "running_command": "fuck", "expected_input": "lein rpl", "expected_output": "lein repl"}, {"task_title": "Using custom alias for The Fuck", "task_description": "Sets a custom alias for using The Fuck in the shell.", "example_code": null, "running_command": "eval $(thefuck --alias)", "expected_input": null, "expected_output": null}, {"task_title": "Using The Fuck with confirmation", "task_description": "Runs The Fuck with confirmation for the suggested command.", "example_code": null, "running_command": "fuck --yeah", "expected_input": null, "expected_output": null}, {"task_title": "Re-running last command with The Fuck", "task_description": "Re-runs the last command that failed with The Fuck.", "example_code": null, "running_command": "fuck -r", "expected_input": null, "expected_output": null}, {"task_title": "Upgrading The Fuck", "task_description": "Upgrades The Fuck to the latest version using pip.", "example_code": null, "running_command": "pip3 install thefuck --upgrade", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim [enter/â†‘/â†“/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...", "âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim\n[sudo] password for nvbn:\nReading package lists... Done\n...", "brew install thefuck", "pip install thefuck"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:30:23.647065"}
{"repo_name": "comfyanonymous/ComfyUI", "stars": 92351, "language": "Python", "tasks": [{"task_title": "Run ComfyUI with AMD cards", "task_description": "This task demonstrates how to run ComfyUI with AMD cards by setting the appropriate graphics version for different RDNA architectures.", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py", "expected_input": null, "expected_output": "ComfyUI application running with AMD card support"}, {"task_title": "Run ComfyUI for RDNA3 cards", "task_description": "This task shows how to run ComfyUI specifically for AMD RDNA3 cards by setting the correct graphics version.", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py", "expected_input": null, "expected_output": "ComfyUI application running with RDNA3 card support"}, {"task_title": "Enable experimental memory efficient attention", "task_description": "This task explains how to enable experimental memory efficient attention on recent PyTorch in ComfyUI for certain AMD GPUs.", "example_code": null, "running_command": null, "expected_input": null, "expected_output": "Memory efficient attention enabled"}], "setup": {"setup_commands": ["pip install comfy-cli\ncomfy install"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:30:39.768740"}
{"repo_name": "fastapi/fastapi", "stars": 91348, "language": "Python", "tasks": [{"task_title": "è¯»å–æ ¹è·¯å¾„", "task_description": "å®šä¹‰ä¸€ä¸ªGETè¯·æ±‚çš„æ ¹è·¯å¾„ï¼ˆ'/'ï¼‰ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "è¯»å–ç‰¹å®šé¡¹", "task_description": "å®šä¹‰ä¸€ä¸ªGETè¯·æ±‚ï¼Œæ¥å—è·¯å¾„å‚æ•°item_idå¹¶å¯é€‰æŸ¥è¯¢å‚æ•°qï¼Œè¿”å›åŒ…å«item_idå’Œqçš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": "item_id=1&q=test", "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "å¼‚æ­¥è¯»å–æ ¹è·¯å¾„", "task_description": "ä½¿ç”¨å¼‚æ­¥å‡½æ•°å®šä¹‰ä¸€ä¸ªGETè¯·æ±‚çš„æ ¹è·¯å¾„ï¼ˆ'/'ï¼‰ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "å¼‚æ­¥è¯»å–ç‰¹å®šé¡¹", "task_description": "ä½¿ç”¨å¼‚æ­¥å‡½æ•°å®šä¹‰ä¸€ä¸ªGETè¯·æ±‚ï¼Œæ¥å—è·¯å¾„å‚æ•°item_idå¹¶å¯é€‰æŸ¥è¯¢å‚æ•°qï¼Œè¿”å›åŒ…å«item_idå’Œqçš„JSONå“åº”ã€‚", "example_code": "@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": "item_id=1&q=test", "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}], "setup": {"setup_commands": ["$ pip install \"fastapi[standard]\"\n\n---> 100%"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:30:55.813629"}
{"repo_name": "openai/whisper", "stars": 90159, "language": "Python", "tasks": [{"task_title": "Transcribe audio files", "task_description": "This task transcribes audio files into text using the specified model.", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Transcribed text from audio.mp3"}, {"task_title": "Specify language for transcription", "task_description": "This task transcribes an audio file with a specified language.", "example_code": null, "running_command": "whisper japanese.wav --language Japanese", "expected_input": "japanese.wav", "expected_output": "Transcribed text from japanese.wav in Japanese"}, {"task_title": "Translate audio content", "task_description": "This task translates the spoken content of an audio file into another language.", "example_code": null, "running_command": "whisper japanese.wav --model medium --language Japanese --task translate", "expected_input": "japanese.wav", "expected_output": "Translated text from japanese.wav"}, {"task_title": "Detect spoken language", "task_description": "This task detects the language spoken in an audio file.", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Detected language: [language]"}], "setup": {"setup_commands": ["# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg", "pip install setuptools-rust"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:31:14.069037"}
{"repo_name": "microsoft/markitdown", "stars": 82292, "language": "Python", "tasks": [{"task_title": "Convert PDF to Markdown", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œè¾“å‡ºåˆ°æŒ‡å®šæ–‡ä»¶ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf > document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF to Markdown with Output Option", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶åã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF from Standard Input", "task_description": "ä»æ ‡å‡†è¾“å…¥è¯»å–PDFæ–‡ä»¶å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "cat path-to-file.pdf | markitdown", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "List Available Plugins", "task_description": "åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --list-plugins", "expected_input": null, "expected_output": "æ’ä»¶åˆ—è¡¨"}, {"task_title": "Use Plugins for Conversion", "task_description": "ä½¿ç”¨æ’ä»¶å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "markitdown --use-plugins path-to-file.pdf", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶ä½¿ç”¨æŒ‡å®šçš„æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"", "expected_input": "path-to-file.pdf, <document_intelligence_endpoint>", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert Excel File to Markdown", "task_description": "å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œç¦ç”¨æ’ä»¶ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False)\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.xlsx", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint in Code", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶ä½¿ç”¨æŒ‡å®šçš„æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.pdf, <document_intelligence_endpoint>", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert Image File to Markdown using OpenAI", "task_description": "å°†å›¾åƒæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œä½¿ç”¨OpenAIçš„LLMã€‚", "example_code": "from markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)", "running_command": null, "expected_input": "example.jpg", "expected_output": "Markdownå†…å®¹"}], "setup": {"setup_commands": ["python -m venv .venv\nsource .venv/bin/activate", "uv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment", "conda create -n markitdown python=3.12\nconda activate markitdown", "git clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'", "pip install 'markitdown[pdf, docx, pptx]'", "  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test"], "docker_commands": ["docker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the Markitdown project from the Microsoft repository, follow these steps:\n\n### Building the Docker Image\n\n1. **Prepare Your Environment**: Ensure you have Docker installed and running on your machine.\n\n2. **Clone the Repository**: Download the `microsoft/markitdown` repository to your local machine. This repository contains the necessary files, including the Dockerfile.\n\n3. **Navigate to the Directory**: Open a terminal and change to the directory where the Dockerfile is located.\n\n4. **Build the Image**: Execute the Docker build command. This process will create an image named `markitdown:latest`. The image is based on a slim version of Python 3.13 and includes essential runtime dependencies such as `ffmpeg` and `exiftool`, which are used for processing multimedia files and metadata, respectively. The build process also installs the Markitdown package along with a sample plugin.\n\n### Running the Docker Container\n\n1. **Run the Container**: After successfully building the image, you can run it using the Docker run command. This command will execute the Markitdown tool, which converts PDF files into Markdown format.\n\n2. **Input and Output**: When running the container, you will need to specify the input PDF file (e.g., `~/your-file.pdf`) and redirect the output to a Markdown file (e.g., `output.md`). The container will process the PDF and generate the corresponding Markdown content.\n\n### Summary of the Image Purpose and Dependencies\n\nThe Docker image serves the purpose of providing a lightweight environment to convert PDF documents into Markdown format using the Markitdown tool. It includes:\n\n- **Python 3.13**: The programming language used for the Markitdown application.\n- **FFmpeg**: A multimedia framework for handling video, audio, and other multimedia files and streams.\n- **ExifTool**: A tool for reading, writing, and editing metadata in files.\n\nBy following these steps, you will be able to build and run the Markitdown Docker image effectively.", ".dockerignore": "To build and run the Docker image for the Microsoft Markitdown project, follow these steps:\n\n### Building the Docker Image\n\n1. **Setup Your Environment**: Ensure you have Docker installed on your machine. You can verify this by running `docker --version` in your terminal.\n\n2. **Clone the Repository**: Download the `microsoft/markitdown` repository to your local machine. This repository contains the necessary files for building the Docker image.\n\n3. **Create the .dockerignore File**: The `.dockerignore` file is crucial as it specifies which files and directories should be excluded from the Docker build context. In this case, it includes all files and directories except for the `packages/` directory. This helps to reduce the build context size and speeds up the build process by ignoring unnecessary files.\n\n4. **Build the Image**: Navigate to the root directory of the cloned repository where the Dockerfile is located. Use the Docker build command to create the image. The image is designed to convert Markdown files into PDF format, leveraging the dependencies specified in the Dockerfile. These dependencies typically include tools for processing Markdown and generating PDFs, ensuring that the image has everything needed to perform the conversion.\n\n5. **Tag the Image**: During the build process, tag the image as `markitdown:latest` to easily reference it later.\n\n### Running the Docker Image\n\n1. **Run the Container**: After successfully building the image, you can run it to convert a Markdown file to PDF. Use the Docker run command, which allows you to execute the container interactively. The command takes your Markdown file as input and outputs the converted content in Markdown format to a specified output file.\n\n2. **Input and Output**: Make sure to provide the path to your Markdown file when running the container. The output will be directed to a file named `output.md`, which will contain the converted content.\n\n### Summary\n\nThe Docker image built from the Microsoft Markitdown repository is designed to facilitate the conversion of Markdown documents into PDF format. It relies on specific dependencies that are included in the build process, ensuring that all necessary tools are available within the container. By following the outlined steps, you can efficiently build and run the Docker image to convert your Markdown files."}}, "timestamp": "2025-10-30T23:32:02.961533"}
{"repo_name": "3b1b/manim", "stars": 81544, "language": "Python", "tasks": [{"task_title": "Opening Manim Example", "task_description": "å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Manimåº“åˆ›å»ºä¸€ä¸ªç®€å•çš„åŠ¨ç”»ç¤ºä¾‹ã€‚", "example_code": "from manim import *\n\nclass OpeningManimExample(Scene):\n    def construct(self):\n        square = Square()\n        self.play(Create(square))\n        self.wait(1)", "running_command": null, "expected_input": null, "expected_output": "ä¸€ä¸ªåŒ…å«æ–¹å½¢çš„åŠ¨ç”»"}], "setup": {"setup_commands": ["# Install manimgl\npip install manimgl\n\n# Try it out\nmanimgl", "# Install manimgl\npip install -e .\n\n# Try it out\nmanimgl example_scenes.py OpeningManimExample\n# or\nmanim-render example_scenes.py OpeningManimExample", "    git clone https://github.com/3b1b/manim.git\n    cd manim\n    pip install -e .\n    manimgl example_scenes.py OpeningManimExample", "    brew install ffmpeg mactex", "    arch -arm64 brew install pkg-config cairo", "    git clone https://github.com/3b1b/manim.git\n    cd manim\n    pip install -e .\n    manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:32:12.630401"}
{"repo_name": "hacksider/Deep-Live-Cam", "stars": 74359, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨CUDAæ‰§è¡Œ", "task_description": "ä½¿ç”¨CUDAä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider cuda", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨CoreMLæ‰§è¡Œ", "task_description": "ä½¿ç”¨CoreMLä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python3.10 run.py --execution-provider coreml", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨CoreMLæ‰§è¡Œ", "task_description": "ä½¿ç”¨CoreMLä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider coreml", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨DirectMLæ‰§è¡Œ", "task_description": "ä½¿ç”¨DirectMLä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider directml", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨OpenVINOæ‰§è¡Œ", "task_description": "ä½¿ç”¨OpenVINOä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider openvino", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["git clone https://github.com/hacksider/Deep-Live-Cam.git\ncd Deep-Live-Cam", "python -m venv venv\nvenv\\Scripts\\activate\npip install -r requirements.txt", "# Ensure you use the installed Python 3.10\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt", "# Install Python 3.11 (specific version is important)\nbrew install python@3.11\n\n# Install tkinter package (required for the GUI)\nbrew install python-tk@3.10\n\n# Create and activate virtual environment with Python 3.11\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt", "# Deactivate the virtual environment\nrm -rf venv\n\n# Reinstall the virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# install the dependencies again\npip install -r requirements.txt\n\n# gfpgan and basicsrs issue fix\npip install git+https://github.com/xinntao/BasicSR.git@master\npip uninstall gfpgan -y\npip install git+https://github.com/TencentARC/GFPGAN.git@master", "pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\npip uninstall onnxruntime onnxruntime-gpu\npip install onnxruntime-gpu==1.21.0", "pip uninstall onnxruntime onnxruntime-silicon\npip install onnxruntime-silicon==1.13.1", "pip uninstall onnxruntime onnxruntime-coreml\npip install onnxruntime-coreml==1.21.0", "pip uninstall onnxruntime onnxruntime-directml\npip install onnxruntime-directml==1.21.0", "pip uninstall onnxruntime onnxruntime-openvino\npip install onnxruntime-openvino==1.21.0"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:32:39.656971"}
{"repo_name": "Shubhamsaboo/awesome-llm-apps", "stars": 74234, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨AIæ—…è¡ŒåŠ©æ‰‹è¿›è¡Œæ—…è¡Œè§„åˆ’", "task_description": "æ­¤ä»»åŠ¡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨AIæ—…è¡ŒåŠ©æ‰‹è¿›è¡Œæ—…è¡Œè§„åˆ’ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ç”Ÿæˆæ—…è¡Œå»ºè®®ã€‚", "example_code": "from ai_travel_agent import TravelAgent\n\nagent = TravelAgent()\nresponse = agent.plan_trip(destination='Paris', duration=5)\nprint(response)", "running_command": null, "expected_input": {"destination": "Paris", "duration": 5}, "expected_output": "å»ºè®®çš„æ—…è¡Œè¡Œç¨‹å’Œæ´»åŠ¨"}], "setup": {"setup_commands": ["    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git ", "    pip install -r requirements.txt"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:32:46.787284"}
{"repo_name": "browser-use/browser-use", "stars": 71954, "language": "Python", "tasks": [{"task_title": "åˆå§‹åŒ–é¡¹ç›®", "task_description": "ä½¿ç”¨CLIå‘½ä»¤åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„é¡¹ç›®ã€‚", "example_code": null, "running_command": "uv init", "expected_input": null, "expected_output": null}, {"task_title": "æ·»åŠ ä¾èµ–åº“", "task_description": "ä½¿ç”¨CLIå‘½ä»¤å°†browser-useåº“æ·»åŠ åˆ°é¡¹ç›®ä¸­ã€‚", "example_code": null, "running_command": "uv add browser-use", "expected_input": null, "expected_output": null}, {"task_title": "åŒæ­¥ä¾èµ–", "task_description": "ä½¿ç”¨CLIå‘½ä»¤åŒæ­¥é¡¹ç›®ä¸­çš„ä¾èµ–åº“ã€‚", "example_code": null, "running_command": "uv sync", "expected_input": null, "expected_output": null}, {"task_title": "å®‰è£…åº“", "task_description": "ä½¿ç”¨CLIå‘½ä»¤å®‰è£…browser-useåº“ã€‚", "example_code": null, "running_command": "uvx browser-use install", "expected_input": null, "expected_output": null}, {"task_title": "è¿è¡Œç¤ºä¾‹ä»£ç ", "task_description": "åˆ›å»ºä¸€ä¸ªAgentæ¥æŸ¥æ‰¾browser-useåº“çš„æ˜Ÿæ ‡æ•°é‡ï¼Œå¹¶è¿è¡Œè¯¥Agentã€‚", "example_code": "from browser_use import Agent, Browser, ChatBrowserUse\nimport asyncio\n\nasync def example():\n    browser = Browser(\n        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud\n    )\n\n    llm = ChatBrowserUse()\n\n    agent = Agent(\n        task=\"Find the number of stars of the browser-use repo\",\n        llm=llm,\n        browser=browser,\n    )\n\n    history = await agent.run()\n    return history\n\nif __name__ == \"__main__\":\n    history = asyncio.run(example())", "running_command": null, "expected_input": null, "expected_output": "å†å²è®°å½•æ•°æ®ï¼ˆå…·ä½“å†…å®¹å–å†³äºå®é™…è¿è¡Œç»“æœï¼‰"}, {"task_title": "åˆå§‹åŒ–Agentæ¨¡æ¿", "task_description": "ä½¿ç”¨CLIå‘½ä»¤åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„Agentæ¨¡æ¿ã€‚", "example_code": null, "running_command": "uvx browser-use init --template default", "expected_input": null, "expected_output": null}, {"task_title": "åˆå§‹åŒ–Agentæ¨¡æ¿å¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶", "task_description": "ä½¿ç”¨CLIå‘½ä»¤åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„Agentæ¨¡æ¿å¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶åã€‚", "example_code": null, "running_command": "uvx browser-use init --template default --output my_agent.py", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨è‡ªå®šä¹‰å·¥å…·", "task_description": "å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰å·¥å…·å¹¶åœ¨Agentä¸­ä½¿ç”¨ã€‚", "example_code": "from browser_use.tools import Tool\n\n@Tool()\ndef custom_tool(param: str) -> str:\n    \"\"\"Description of what this tool does.\"\"\"\n    return f\"Result: {param}\"\n\nagent = Agent(\n    task=\"Your task\",\n    llm=llm,\n    browser=browser,\n    use_custom_tools=[custom_tool],\n)", "running_command": null, "expected_input": null, "expected_output": "Agentçš„è¿è¡Œç»“æœï¼ˆå…·ä½“å†…å®¹å–å†³äºå®é™…è¿è¡Œç»“æœï¼‰"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the `browser-use` project, follow these steps:\n\n### Purpose of the Image\nThe `browser-use` Docker image is designed to automate web interactions, making websites accessible for AI agents. It bundles essential dependencies including Python 3.12, pip, Playwright, and Chromium, along with the `browser-use` application itself. This setup allows users to run automated tasks in a controlled environment, leveraging the capabilities of a headless browser.\n\n### Building the Docker Image\n1. **Clone the Repository**: Start by cloning the `browser-use` repository from GitHub to your local machine.\n   - Use the command: `git clone https://github.com/browser-use/browser-use.git`\n   - Navigate into the cloned directory: `cd browser-use`\n\n2. **Build the Image**: Execute the Docker build command to create the image.\n   - This command will build the image without using the cache to ensure all dependencies are freshly installed: `docker build . -t browseruse --no-cache`\n   - The build process will install necessary system packages, set up a virtual environment, and install the `browser-use` application along with its dependencies.\n\n### Running the Docker Container\n1. **Run the Container**: After building the image, you can run the container with the following command:\n   - `docker run -v \"$PWD/data\":/data browseruse`\n   - This command mounts a local directory (`data`) to the container, allowing you to persist data generated by the application.\n\n2. **Check the Version**: To verify the installation and check the version of `browser-use`, use:\n   - `docker run -v \"$PWD/data\":/data browseruse --version`\n\n### Multi-Architecture Build (Optional)\nIf you need to support multiple architectures, you can set up a multi-architecture build:\n1. **Create a Buildx Instance**: Use Docker Buildx to create a new build instance.\n   - Command: `docker buildx create --use`\n\n2. **Build for Multiple Platforms**: Build the image for multiple architectures (e.g., `linux/amd64` and `linux/arm64`) and push it to a Docker registry:\n   - Command: `docker buildx build . --platform=linux/amd64,linux/arm64 --push -t browseruse/browseruse:some-tag`\n\n### Additional Resources\nFor more detailed documentation and usage instructions, refer to the official documentation at [https://docs.browser-use.com](https://docs.browser-use.com).", ".dockerignore": "To build and run a Docker image using the repository `browser-use/browser-use`, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image is designed to encapsulate the environment necessary for running the browser-use application. This application likely involves web scraping, data processing, or browser automation, and requires specific dependencies to function correctly. The image ensures that all necessary libraries and tools are included, providing a consistent environment across different systems.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `browser-use` repository to your local machine. This will give you access to the necessary files, including the `.dockerignore` file, which helps optimize the build process by excluding unnecessary files.\n\n2. **Prepare the Docker Environment**: Ensure that Docker is installed and running on your machine. This is crucial for building and running the Docker image.\n\n3. **Create the Docker Image**: Use the Docker CLI to build the image. The build process will read the Dockerfile in the repository, which defines the base image, dependencies, and commands needed to set up the application environment. The `.dockerignore` file will prevent the inclusion of specified files and directories, such as documentation, cache files, virtual environments, and sensitive information, ensuring a cleaner and smaller image.\n\n4. **Dependencies**: The image will include essential dependencies for the application, which may consist of libraries for web scraping (like BeautifulSoup or Scrapy), data manipulation (such as Pandas), and any other tools required for browser automation. These dependencies will be installed during the image build process.\n\n5. **Tagging the Image**: Optionally, tag the image with a meaningful name and version to easily identify it later.\n\n### Running the Docker Image\nOnce the image is built, you can run it using Docker. This will create a container instance of the application, allowing you to execute the browser-use functionalities in an isolated environment.\n\n### Additional Considerations\n- **Environment Variables**: If the application requires any environment variables (e.g., API keys, configuration settings), ensure they are provided at runtime, either through a `.env` file or by passing them directly in the Docker run command.\n- **Volume Mounts**: If the application needs to access data files or output results, consider using Docker volumes to persist data outside the container.\n\nBy following these steps, you will successfully build and run the Docker image for the browser-use application, ensuring a consistent and efficient development and production workflow."}}, "timestamp": "2025-10-30T23:33:27.809633"}
{"repo_name": "pallets/flask", "stars": 70689, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºåŸºæœ¬çš„Flaskåº”ç”¨", "task_description": "åˆ›å»ºä¸€ä¸ªFlaskåº”ç”¨å¹¶å®šä¹‰ä¸€ä¸ªè·¯ç”±ï¼Œè¯¥è·¯ç”±è¿”å›ä¸€ä¸ªç®€å•çš„é—®å€™æ¶ˆæ¯ã€‚", "example_code": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello, World!\"", "running_command": null, "expected_input": null, "expected_output": "Hello, World!"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:33:53.211897"}
{"repo_name": "sherlock-project/sherlock", "stars": 69964, "language": "Python", "tasks": [{"task_title": "Check a single username across social networks", "task_description": "This task checks the availability of a single username 'user123' across various social networks.", "example_code": null, "running_command": "sherlock user123", "expected_input": "user123", "expected_output": "[{ 'username': 'user123', 'links': [...] }]"}, {"task_title": "Check multiple usernames across social networks", "task_description": "This task checks the availability of multiple usernames 'user1', 'user2', and 'user3' across various social networks.", "example_code": null, "running_command": "sherlock user1 user2 user3", "expected_input": "user1 user2 user3", "expected_output": "[{ 'username': 'user1', 'links': [...] }, { 'username': 'user2', 'links': [...] }, { 'username': 'user3', 'links': [...] }]"}, {"task_title": "Display help information", "task_description": "This task displays the help information for the Sherlock CLI tool, including usage and available options.", "example_code": null, "running_command": "sherlock --help", "expected_input": null, "expected_output": "usage: sherlock [-h] [--version] [--verbose] ..."}, {"task_title": "Call Sherlock with JSON input", "task_description": "This task demonstrates how to call the Sherlock tool with a JSON input containing usernames.", "example_code": null, "running_command": "echo '{\"usernames\":[\"user123\"]}' | apify call -so netmilk/sherlock", "expected_input": "{\"usernames\":[\"user123\"]}", "expected_output": "[{ \"username\": \"user123\", \"links\": [...] }]"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the Sherlock project, follow these steps:\n\n### Building the Docker Image\n\n1. **Update Version Information**: Before building the image, ensure you update the version tag in the Dockerfile to match the version specified in the `sherlock/__init__.py` file. This ensures that the Docker image corresponds to the correct version of the application.\n\n2. **Set VCS Reference**: Update the `VCS_REF` argument in the Dockerfile to reflect the full commit hash of the tagged version you are building. This is important for tracking the specific state of the codebase used in the image.\n\n3. **Build the Image**: Use Docker to build the image. The build process will create a lightweight image based on the Python 3.12 slim variant. The image installs the necessary dependencies, including the Sherlock project itself, using the specified version tag. The command will tag the image with both the specific version and the latest tag for easy reference.\n\n### Purpose of the Image\n\nThe resulting Docker image is designed to run the Sherlock application, which is a tool for hunting down social media accounts by username. It encapsulates all necessary dependencies, ensuring a consistent environment for running the application.\n\n### Dependencies\n\nThe image relies on the Python 3.12 slim-bullseye base image, which provides a minimal environment to run Python applications. The primary dependency installed is the Sherlock project itself, specified by the version tag you set during the build process. This ensures that the application has all required libraries and dependencies to function correctly.\n\n### Running the Docker Container\n\nOnce the image is built, you can run it using Docker. The entry point is set to the Sherlock command, allowing you to execute the application directly. You may need to provide additional parameters or configurations depending on your specific use case.\n\nBy following these steps, you will have a fully functional Docker image for the Sherlock project, ready for deployment or local testing.", ".dockerignore": "To build and run a Docker image for the Sherlock project, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image for the Sherlock project is designed to encapsulate the application and its dependencies, allowing for a consistent and isolated environment for running the Sherlock tool. This tool is used for identifying usernames across various social media platforms, making it useful for security researchers and developers.\n\n### Dependencies\nThe image includes essential dependencies specified in the `requirements.txt` file, which is the only text file retained in the build context due to the `.dockerignore` configuration. This ensures that only the necessary components are included in the image, while excluding unnecessary files and directories such as version control files, development environments, and test directories.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `sherlock-project/sherlock` repository to your local machine.\n   \n2. **Navigate to the Project Directory**: Change your working directory to the root of the cloned repository.\n\n3. **Build the Docker Image**: Use the Docker build command to create the image. The build process will utilize the Dockerfile in the repository, which sets up the environment by installing Python and the required libraries listed in `requirements.txt`. The `.dockerignore` file will ensure that unnecessary files are excluded from the build context, optimizing the image size and build time.\n\n4. **Tag the Image**: Optionally, tag the image with a meaningful name for easier reference later.\n\n### Running the Docker Container\nOnce the image is built, you can run a container from it. The container will execute the Sherlock application, allowing you to use it for username searches across various platforms. You may need to pass specific arguments or configurations depending on your use case.\n\n### Summary\nBy following these steps, you will have a Docker image that encapsulates the Sherlock application and its dependencies, ready for use in a controlled environment. This setup enhances portability and ensures that the application runs consistently across different systems."}}, "timestamp": "2025-10-30T23:34:28.002017"}
{"repo_name": "xtekky/gpt4free", "stars": 65456, "language": "Python", "tasks": [{"task_title": "Run GUI", "task_description": "å¯åŠ¨GUIç•Œé¢ï¼Œå…è®¸ç”¨æˆ·ä¸ç¨‹åºè¿›è¡Œäº¤äº’ã€‚", "example_code": "from g4f.gui import run_gui\nrun_gui()", "running_command": null, "expected_input": null, "expected_output": "å¯åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢"}, {"task_title": "Run CLI with GUI", "task_description": "é€šè¿‡å‘½ä»¤è¡Œæ¥å£å¯åŠ¨GUIï¼ŒæŒ‡å®šç«¯å£å’Œè°ƒè¯•æ¨¡å¼ã€‚", "example_code": null, "running_command": "python -m g4f.cli gui --port 8080 --debug", "expected_input": null, "expected_output": "å¯åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼Œç›‘å¬8080ç«¯å£ï¼Œè°ƒè¯•æ¨¡å¼å¼€å¯"}, {"task_title": "Run CLI", "task_description": "é€šè¿‡å‘½ä»¤è¡Œæ¥å£å¯åŠ¨ç¨‹åºï¼ŒæŒ‡å®šç«¯å£å’Œè°ƒè¯•æ¨¡å¼ã€‚", "example_code": null, "running_command": "python -m g4f --port 8080 --debug", "expected_input": null, "expected_output": "ç¨‹åºå¯åŠ¨ï¼Œç›‘å¬8080ç«¯å£ï¼Œè°ƒè¯•æ¨¡å¼å¼€å¯"}, {"task_title": "Chat Completion", "task_description": "ä½¿ç”¨å®¢æˆ·ç«¯ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼Œè·å–æ¨¡å‹çš„å›å¤ã€‚", "example_code": "from g4f.client import Client\n\nclient = Client()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    web_search=False\n)\nprint(response.choices[0].message.content)", "running_command": null, "expected_input": "Hello, how are you?", "expected_output": "æ¨¡å‹çš„å›å¤å†…å®¹"}, {"task_title": "Image Generation", "task_description": "ä½¿ç”¨å®¢æˆ·ç«¯ç”Ÿæˆå›¾åƒï¼Œå¹¶è¿”å›ç”Ÿæˆçš„å›¾åƒURLã€‚", "example_code": "from g4f.client import Client\n\nclient = Client()\nresponse = client.images.generate(\n    model=\"flux\",\n    prompt=\"a white siamese cat\",\n    response_format=\"url\"\n)\nprint(f\"Generated image URL: {response.data[0].url}\")", "running_command": null, "expected_input": "a white siamese cat", "expected_output": "ç”Ÿæˆçš„å›¾åƒURL"}, {"task_title": "Asynchronous Chat Completion", "task_description": "ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼Œè·å–æ¨¡å‹çš„å›å¤ã€‚", "example_code": "from g4f.client import AsyncClient\nimport asyncio\n\nasync def main():\n    client = AsyncClient()\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing briefly\"}],\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())", "running_command": null, "expected_input": "Explain quantum computing briefly", "expected_output": "æ¨¡å‹çš„å›å¤å†…å®¹"}], "setup": {"setup_commands": ["pip install -U g4f[all]", "git clone https://github.com/xtekky/gpt4free.git\ncd gpt4free\npip install -r requirements.txt\npip install -e .", "pip install -U g4f[all]"], "docker_commands": ["   docker pull hlohaus789/g4f", "   docker run -p 8080:8080 -p 7900:7900 \\\n     --shm-size=\"2g\" \\\n     -v ${PWD}/har_and_cookies:/app/har_and_cookies \\\n     -v ${PWD}/generated_media:/app/generated_media \\\n     hlohaus789/g4f:latest", "mkdir -p ${PWD}/har_and_cookies ${PWD}/generated_media\nchown -R 1000:1000 ${PWD}/har_and_cookies ${PWD}/generated_media\n\ndocker run \\\n  -p 1337:8080 -p 8080:8080 \\\n  -v ${PWD}/har_and_cookies:/app/har_and_cookies \\\n  -v ${PWD}/generated_media:/app/generated_media \\\n  hlohaus789/g4f:latest-slim"], "has_docker_files": true, "docker_setup_descriptions": {"docker-compose.yml": "To build and run the Docker container for the `gpt4free` service from the `xtekky/gpt4free` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image `hlohaus789/g4f:latest` is designed to provide a lightweight environment for running the `gpt4free` application, which is likely a service that interfaces with the GPT-4 model. This image encapsulates all necessary dependencies and configurations needed to operate the application efficiently.\n\n### Building the Image\n1. **Set Up Your Environment**: Ensure you have Docker and Docker Compose installed on your machine. You can verify this by running `docker --version` and `docker-compose --version`.\n\n2. **Clone the Repository**: Clone the `xtekky/gpt4free` repository to your local machine using Git:\n   ```bash\n   git clone https://github.com/xtekky/gpt4free.git\n   cd gpt4free\n   ```\n\n3. **Prepare Directories**: Create the necessary directories for storing HAR files and generated media. This can be done with the following commands:\n   ```bash\n   mkdir -p har_and_cookies generated_media\n   ```\n\n4. **Set Permissions**: Ensure that the created directories have the correct permissions:\n   ```bash\n   chown -R 1000:1000 har_and_cookies generated_media\n   ```\n\n5. **Build the Docker Image**: Use Docker Compose to build the image defined in the `docker-compose.yml` file. This will compile the application and its dependencies as specified in the Dockerfile referenced in the compose file:\n   ```bash\n   docker-compose build\n   ```\n\n### Running the Container\n1. **Start the Service**: After the build completes successfully, you can start the service using Docker Compose. This will launch the container with the specified configurations, including memory allocation and port mappings:\n   ```bash\n   docker-compose up\n   ```\n\n2. **Access the Application**: Once the container is running, you can access the `gpt4free` service through your web browser or API client at `http://localhost:8080`. The application will also be available at `http://localhost:1337` and `http://localhost:7900` for additional functionalities.\n\n### Environment Configuration\nThe container is configured to use shared memory size of 2GB, which is essential for handling large data processes efficiently. The environment variable `OLLAMA_HOST` is set to `host.docker.internal`, allowing the container to communicate with services running on the host machine.\n\nThis setup provides a robust environment for running the `gpt4free` application, ensuring that all dependencies are managed within the Docker ecosystem, facilitating easier deployment and scalability."}}, "timestamp": "2025-10-30T23:35:05.855271"}
{"repo_name": "OpenHands/OpenHands", "stars": 64584, "language": "Python", "tasks": [{"task_title": "Launch the GUI server", "task_description": "å¯åŠ¨OpenHandsçš„GUIæœåŠ¡å™¨ï¼Œæä¾›å›¾å½¢ç•Œé¢ä¾›ç”¨æˆ·ä½¿ç”¨ã€‚", "example_code": null, "running_command": "uvx --python 3.12 --from openhands-ai openhands serve", "expected_input": null, "expected_output": "GUI server is running"}, {"task_title": "Launch the CLI", "task_description": "å¯åŠ¨OpenHandsçš„å‘½ä»¤è¡Œç•Œé¢ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡CLIä¸OpenHandsè¿›è¡Œäº¤äº’ã€‚", "example_code": null, "running_command": "uvx --python 3.12 --from openhands-ai openhands", "expected_input": null, "expected_output": "CLI interface is ready"}], "setup": {"setup_commands": [], "docker_commands": ["docker pull docker.openhands.dev/openhands/runtime:0.60-nikolaik\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:0.60-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands:/.openhands \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.openhands.dev/openhands/openhands:0.60"], "has_docker_files": true, "docker_setup_descriptions": {"docker-compose.yml": "To build and run the Docker application using the `docker-compose.yml` file from the OpenHands/OpenHands repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image `openhands:latest` is designed to facilitate the development and execution of the OpenHands application. It encapsulates all necessary dependencies, including the runtime environment and libraries required for the application to function seamlessly. This image is tailored for use in a sandboxed environment, allowing developers to work on the OpenHands project without affecting their local setup.\n\n### Building the Image\n1. **Clone the Repository**: Start by cloning the OpenHands repository to your local machine. This will provide you with access to the `docker-compose.yml` file and the associated Dockerfile.\n\n2. **Navigate to the Directory**: Change your working directory to the location of the `docker-compose.yml` file. This file defines the services, networks, and volumes needed for the application.\n\n3. **Build the Docker Image**: Use Docker Compose to build the image. This process will read the `docker-compose.yml` file and execute the build instructions specified for the `openhands` service. During this step, Docker will create a new image based on the configurations defined in the Dockerfile located in the `./containers/app/` directory. The build process will resolve dependencies, set up the environment, and prepare the application for execution.\n\n### Running the Application\n1. **Set Environment Variables**: Before running the container, ensure that any necessary environment variables are set. For instance, you may want to specify the `SANDBOX_RUNTIME_CONTAINER_IMAGE` to point to the appropriate runtime version. You can also define the `WORKSPACE_BASE` variable to specify where your workspace will be mounted.\n\n2. **Start the Container**: Utilize Docker Compose to start the application. This will create and run the `openhands` container with the configurations specified in the `docker-compose.yml` file. The container will expose port 3000, allowing you to access the application through your browser or API client.\n\n3. **Volume Mounting**: The setup includes volume mounts for the Docker socket and your local OpenHands configuration directory. This allows the container to interact with the Docker daemon on your host and persist user-specific configurations.\n\n4. **Accessing the Application**: Once the container is running, you can access the OpenHands application at `http://localhost:3000`. Ensure that your local environment is configured to allow connections to this port.\n\n### Additional Notes\n- If you require a specific non-root user within the container, you can uncomment the `SANDBOX_USER_ID` variable in the environment section and adjust permissions accordingly.\n- The container is set to run in interactive mode, allowing you to interact with it via the terminal if needed.\n\nBy following these steps, you will successfully build and run the OpenHands application using Docker, enabling a streamlined development experience.", ".dockerignore": "To build and run the Docker image for the OpenHands project, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image for OpenHands is designed to provide a consistent and isolated environment for running the OpenHands application. This application is likely a web-based platform that requires specific dependencies and configurations to function correctly. The image encapsulates all necessary components, ensuring that the application runs smoothly across different environments.\n\n### Building Steps\n1. **Prepare Your Environment**: Ensure you have Docker installed on your machine. You can verify this by running `docker --version` in your terminal.\n\n2. **Clone the Repository**: Start by cloning the OpenHands repository from GitHub to your local machine. This will give you access to the necessary files, including the `.dockerignore` file, which helps optimize the build process by excluding unnecessary files.\n\n3. **Review the .dockerignore File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This includes:\n   - Node.js dependencies located in `frontend/node_modules`, which are not needed for the build.\n   - Configuration files (except `pyproject.toml`) and documentation files (except `README.md`), which are not required for the runtime environment.\n   - Hidden files, cache directories, and unneeded directories such as `/dev_config/`, `/docs/`, `/evaluation/`, and `/tests/`.\n\n4. **Build the Docker Image**: Use the appropriate Docker command to build the image. This command will read the Dockerfile in the repository and create an image that includes all necessary dependencies, such as Node.js and any other libraries specified in the Dockerfile.\n\n5. **Verify the Image**: After the build completes, you can list your Docker images using `docker images` to ensure that the OpenHands image has been created successfully.\n\n### Running the Docker Image\n1. **Run the Container**: To run the OpenHands application, use the provided Docker run command. This command sets up the environment variables necessary for the application, mounts the Docker socket for container management, and maps the local directory for persistent storage.\n\n2. **Access the Application**: Once the container is running, you can access the OpenHands application by navigating to `http://localhost:3000` in your web browser. The application should be fully operational, leveraging the isolated environment provided by Docker.\n\nBy following these steps, you will have successfully built and run the OpenHands Docker image, allowing you to utilize the application in a controlled and efficient manner."}}, "timestamp": "2025-10-30T23:35:41.371296"}
{"repo_name": "PaddlePaddle/PaddleOCR", "stars": 62084, "language": "Python", "tasks": [{"task_title": "Run PP-OCRv5 inference", "task_description": "ä½¿ç”¨PP-OCRv5æ¨¡å‹è¿›è¡ŒOCRæ¨æ–­ï¼Œå¤„ç†å›¾åƒå¹¶æå–æ–‡æœ¬ã€‚", "example_code": null, "running_command": "paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png", "expected_output": "æå–çš„æ–‡æœ¬ç»“æœ"}, {"task_title": "Run PP-StructureV3 inference", "task_description": "ä½¿ç”¨PP-StructureV3æ¨¡å‹è¿›è¡Œç»“æ„åŒ–æ–‡æ¡£æ¨æ–­ï¼Œå¤„ç†å›¾åƒå¹¶æå–ç»“æ„ä¿¡æ¯ã€‚", "example_code": null, "running_command": "paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png", "expected_output": "æå–çš„ç»“æ„åŒ–ä¿¡æ¯"}, {"task_title": "Run PP-ChatOCRv4 inference", "task_description": "ä½¿ç”¨PP-ChatOCRv4æ¨¡å‹è¿›è¡Œæ–‡æ¡£OCRæ¨æ–­ï¼Œæ”¯æŒç‰¹å®šä¿¡æ¯çš„æå–ã€‚", "example_code": null, "running_command": "paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png", "expected_output": "æå–çš„ç‰¹å®šä¿¡æ¯"}, {"task_title": "Run PaddleOCR-VL inference", "task_description": "ä½¿ç”¨PaddleOCR-VLæ¨¡å‹è¿›è¡Œæ–‡æ¡£è§£æï¼Œå¤„ç†å›¾åƒå¹¶æå–ä¿¡æ¯ã€‚", "example_code": null, "running_command": "paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png", "expected_output": "è§£æç»“æœ"}, {"task_title": "Initialize PaddleOCR instance and run inference", "task_description": "åˆå§‹åŒ–PaddleOCRå®ä¾‹å¹¶åœ¨æ ·æœ¬å›¾åƒä¸Šè¿è¡ŒOCRæ¨æ–­ï¼Œè¾“å‡ºç»“æœå¹¶ä¿å­˜ã€‚", "example_code": "from paddleocr import PaddleOCR\nocr = PaddleOCR(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False,\n    use_textline_orientation=False)\nresult = ocr.predict(\n    input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png\")\nfor res in result:\n    res.print()\n    res.save_to_img(\"output\")\n    res.save_to_json(\"output\")", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png", "expected_output": "æå–çš„æ–‡æœ¬ç»“æœï¼Œä¿å­˜ä¸ºå›¾åƒå’ŒJSONæ–‡ä»¶"}, {"task_title": "Run PPStructureV3 inference and save results", "task_description": "åˆå§‹åŒ–PPStructureV3å®ä¾‹å¹¶è¿›è¡Œæ¨æ–­ï¼Œè¾“å‡ºç»“æœå¹¶ä¿å­˜ä¸ºJSONå’ŒMarkdownæ ¼å¼ã€‚", "example_code": "from paddleocr import PPStructureV3\npipeline = PPStructureV3(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False\n)\noutput = pipeline.predict(\n    input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png\",\n)\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"output\")\n    res.save_to_markdown(save_path=\"output\")", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png", "expected_output": "ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¿å­˜ä¸ºJSONå’ŒMarkdownæ–‡ä»¶"}, {"task_title": "Run PPChatOCRv4Doc for visual prediction", "task_description": "ä½¿ç”¨PPChatOCRv4Docè¿›è¡Œè§†è§‰é¢„æµ‹ï¼Œæ”¯æŒå¤šæ¨¡æ€å¤§å‹æ¨¡å‹çš„æ¨æ–­å’ŒèŠå¤©åŠŸèƒ½ã€‚", "example_code": "from paddleocr import PPChatOCRv4Doc\npipeline = PPChatOCRv4Doc(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False\n)\nvisual_predict_res = pipeline.visual_predict(\n    input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png\",\n    use_common_ocr=True,\n    use_seal_recognition=True,\n    use_table_recognition=True,\n)\nfor res in visual_predict_res:\n    visual_info_list.append(res[\"visual_info\"])\n    layout_parsing_result = res[\"layout_parsing_result\"]", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png", "expected_output": "è§†è§‰é¢„æµ‹ç»“æœå’Œå¸ƒå±€è§£æç»“æœ"}, {"task_title": "Run PaddleOCRVL inference", "task_description": "ä½¿ç”¨PaddleOCRVLæ¨¡å‹è¿›è¡Œæ¨æ–­ï¼Œå¤„ç†å›¾åƒå¹¶æå–ä¿¡æ¯ï¼Œè¾“å‡ºç»“æœå¹¶ä¿å­˜ã€‚", "example_code": "from paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png\")\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"output\")\n    res.save_to_markdown(save_path=\"output\")", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png", "expected_output": "æå–çš„ä¿¡æ¯ï¼Œä¿å­˜ä¸ºJSONå’ŒMarkdownæ–‡ä»¶"}], "setup": {"setup_commands": ["# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series\npython -m pip install paddleocr\n# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.\n# python -m pip install \"paddleocr[all]\""], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:36:37.409801"}
{"repo_name": "hiyouga/LLaMA-Factory", "stars": 61297, "language": "Python", "tasks": [{"task_title": "Sync extra components", "task_description": "Synchronizes additional components like torch and metrics with a pre-release option.", "example_code": null, "running_command": "uv sync --extra torch --extra metrics --prerelease=allow", "expected_input": null, "expected_output": null}, {"task_title": "Train model with LoRA pretraining", "task_description": "Runs the training process using a configuration file for LoRA pretraining.", "example_code": null, "running_command": "uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml", "expected_input": null, "expected_output": null}, {"task_title": "Train model with LoRA SFT", "task_description": "Runs the training process using a configuration file for LoRA SFT.", "example_code": null, "running_command": "llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml", "expected_input": null, "expected_output": null}, {"task_title": "Chat with the trained model", "task_description": "Starts a chat session using the trained model with the specified configuration.", "example_code": null, "running_command": "llamafactory-cli chat examples/inference/llama3_lora_sft.yaml", "expected_input": null, "expected_output": null}, {"task_title": "Export trained model", "task_description": "Exports the trained model using the specified configuration.", "example_code": null, "running_command": "llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml", "expected_input": null, "expected_output": null}, {"task_title": "Start web UI", "task_description": "Launches the web user interface for interacting with the model.", "example_code": null, "running_command": "llamafactory-cli webui", "expected_input": null, "expected_output": null}, {"task_title": "Run Docker for CUDA", "task_description": "Starts the Docker container for CUDA support.", "example_code": null, "running_command": "cd docker/docker-cuda/ && docker compose up -d && docker compose exec llamafactory bash", "expected_input": null, "expected_output": null}, {"task_title": "Run Docker for NPU", "task_description": "Starts the Docker container for NPU support.", "example_code": null, "running_command": "cd docker/docker-npu/ && docker compose up -d && docker compose exec llamafactory bash", "expected_input": null, "expected_output": null}, {"task_title": "Run Docker for ROCm", "task_description": "Starts the Docker container for ROCm support.", "example_code": null, "running_command": "cd docker/docker-rocm/ && docker compose up -d && docker compose exec llamafactory bash", "expected_input": null, "expected_output": null}, {"task_title": "Run inference API", "task_description": "Starts an inference API with specified configuration and options.", "example_code": null, "running_command": "API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true", "expected_input": null, "expected_output": null}, {"task_title": "Set ModelScope Hub environment variable", "task_description": "Sets an environment variable to use the ModelScope Hub.", "example_code": null, "running_command": "export USE_MODELSCOPE_HUB=1", "expected_input": null, "expected_output": null}, {"task_title": "Set OpenMind Hub environment variable", "task_description": "Sets an environment variable to use the OpenMind Hub.", "example_code": null, "running_command": "export USE_OPENMIND_HUB=1", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["pip install --upgrade huggingface_hub\nhuggingface-cli login", "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation", "pip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"", "pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl", "# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh", "# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .", "git clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install ."], "docker_commands": ["docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest", "docker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash", "docker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash", "docker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash"], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run the Docker image for the `hiyouga/LLaMA-Factory` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image created from this repository is designed to facilitate the development and deployment of the LLaMA-Factory project, which is likely focused on machine learning or natural language processing tasks. The image includes necessary dependencies for running the application efficiently, particularly leveraging GPU resources for enhanced performance.\n\n### Building the Image\n1. **Prepare Your Environment**: Ensure you have Docker installed on your machine. You should also have access to a terminal or command prompt.\n\n2. **Clone the Repository**: Clone the `hiyouga/LLaMA-Factory` repository to your local machine. This will provide you with the necessary files, including the Dockerfile and the `.dockerignore` file.\n\n3. **Understand the `.dockerignore` File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This helps keep the image lightweight and avoids unnecessary files being included. The entries in this file include common development artifacts like `.git`, `.venv`, and cache directories.\n\n4. **Build the Docker Image**: Navigate to the directory containing the Dockerfile. Use the Docker build command to create the image. The build process will utilize the specified Dockerfile and any build arguments necessary for your environment (e.g., package index or additional metrics).\n\n5. **Tag the Image**: During the build process, tag the image appropriately (e.g., `llamafactory:latest`) to easily reference it later.\n\n### Running the Image\n1. **Run the Container**: Once the image is built, you can run it using Docker. Use the `docker run` command to start a new container from the image. Ensure to allocate the necessary resources, such as GPU access and specific ports for application interaction (e.g., 7860 and 8000).\n\n2. **Interactive Shell Access**: If you need to interact with the container, you can execute a bash shell within the running container using the `docker exec` command. This allows you to run commands directly in the container environment.\n\n3. **Manage Resources**: Depending on your hardware and application requirements, you may need to adjust device mappings or volume mounts to ensure the container has access to necessary resources.\n\n### Conclusion\nBy following these steps, you will successfully build and run the Docker image for the LLaMA-Factory project. This setup allows for efficient development and testing of machine learning models, leveraging Docker's capabilities to manage dependencies and resources effectively."}}, "timestamp": "2025-10-30T23:37:16.905916"}
{"repo_name": "localstack/localstack", "stars": 61024, "language": "Python", "tasks": [{"task_title": "å¯åŠ¨ LocalStack", "task_description": "ä½¿ç”¨ LocalStack CLI å¯åŠ¨ LocalStack æœåŠ¡ï¼Œè¿è¡Œåœ¨ Docker æ¨¡å¼ä¸‹ã€‚", "example_code": null, "running_command": "localstack start -d", "expected_input": null, "expected_output": "starting LocalStack in Docker mode ğŸ³"}, {"task_title": "æ£€æŸ¥æœåŠ¡çŠ¶æ€", "task_description": "ä½¿ç”¨ LocalStack CLI æŸ¥çœ‹å½“å‰å¯ç”¨æœåŠ¡çš„çŠ¶æ€ã€‚", "example_code": null, "running_command": "localstack status services", "expected_input": null, "expected_output": "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Service                  â”ƒ Status      â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ acm                      â”‚ âœ” available â”‚\nâ”‚ apigateway               â”‚ âœ” available â”‚\nâ”‚ cloudformation           â”‚ âœ” available â”‚\nâ”‚ cloudwatch               â”‚ âœ” available â”‚\nâ”‚ config                   â”‚ âœ” available â”‚\nâ”‚ dynamodb                 â”‚ âœ” available â”‚\n..."}, {"task_title": "åˆ›å»º SQS é˜Ÿåˆ—", "task_description": "ä½¿ç”¨ awslocal å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„ SQS é˜Ÿåˆ—ï¼ŒæŒ‡å®šé˜Ÿåˆ—åç§°ã€‚", "example_code": null, "running_command": "awslocal sqs create-queue --queue-name sample-queue", "expected_input": "sample-queue", "expected_output": "{\n    \"QueueUrl\": \"http://sqs.us-east-1.localhost.localstack.cloud:4566/000000000000/sample-queue\"\n}"}], "setup": {"setup_commands": ["brew install localstack/tap/localstack-cli", "python3 -m pip install localstack"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run a Docker image using the `Dockerfile` from the `localstack/localstack` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image created from this `Dockerfile` is designed to run LocalStack, a fully functional local AWS cloud stack. It allows developers to simulate AWS services locally for testing and development purposes without needing to connect to the actual AWS cloud. The image includes all necessary dependencies and configurations to run LocalStack efficiently.\n\n### Building Steps\n1. **Base Image**: The build process starts with a lightweight Python 3.13.9 base image, which provides the necessary runtime environment. This image is optimized for size and includes essential OS packages.\n\n2. **Install Runtime Dependencies**: The build process installs various runtime dependencies, including tools like `curl`, `git`, and `openssl`, as well as libraries required for the AWS CLI and LocalStack functionality. This ensures that the environment is equipped to handle requests and manage AWS services.\n\n3. **Node.js Installation**: The image installs Node.js from the official distribution, ensuring compatibility with LocalStack's requirements. This step also includes verifying the installation through smoke tests.\n\n4. **User and Directory Setup**: A dedicated user (`localstack`) is created to run the application, and necessary directories are established with appropriate permissions. This enhances security and ensures that LocalStack has the required filesystem structure.\n\n5. **Install Python Packages**: The image installs the AWS CLI and `awslocal`, a command-line tool for interacting with LocalStack, using Python's package manager. This step is crucial for enabling AWS service emulation.\n\n6. **Build Stage**: In a separate build stage, additional build dependencies are installed, and the LocalStack runtime dependencies are set up. This includes creating a virtual environment for Python packages, which isolates the LocalStack installation.\n\n7. **Final Assembly**: The final image is constructed by copying the necessary files and dependencies from the previous stages. This includes the LocalStack core code and any additional scripts needed to start LocalStack.\n\n8. **Health Checks and Configuration**: The image includes health checks to monitor the status of LocalStack services and exposes necessary ports for communication. Environment variables are set for versioning and build metadata.\n\n### Running the Image\nAfter building the image, you can run it using Docker. The container will start LocalStack, allowing you to interact with the simulated AWS services. Ensure to map any required ports and volumes for persistent data storage.\n\n### Dependencies\nThe image relies on several key dependencies:\n- **Python**: For running LocalStack and its associated scripts.\n- **Node.js**: Required for certain LocalStack functionalities.\n- **AWS CLI**: For interacting with AWS services.\n- **Various OS Packages**: Essential for network operations and system interactions.\n\nBy following these steps, you can successfully build and run the LocalStack Docker image, enabling local development and testing of AWS services.", "docker-compose.yml": "To build and run Docker using the `docker-compose.yml` file from the `localstack/localstack` repository, follow these steps:\n\n### Purpose of the Image\nThe `localstack/localstack` image is designed to provide a fully functional local AWS cloud stack, enabling developers to simulate AWS services on their local machines. This is particularly useful for testing and development purposes, allowing you to interact with AWS services without incurring costs or requiring internet access.\n\n### Dependencies\nThe image relies on several components:\n- **Docker**: LocalStack utilizes Docker to run various AWS service emulations in isolated containers.\n- **Python**: As LocalStack is built using Python, it requires Python dependencies for its functionality.\n- **AWS SDKs**: LocalStack supports various AWS SDKs for integration, allowing you to interact with the emulated services.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `localstack/localstack` repository to your local machine. This will provide you with access to the `docker-compose.yml` file.\n\n2. **Navigate to the Directory**: Change your working directory to the location where the `docker-compose.yml` file is located.\n\n3. **Configure Environment Variables**: Before building, you may want to configure environment variables such as `LOCALSTACK_DOCKER_NAME`, `DEBUG`, and `LOCALSTACK_VOLUME_DIR` to customize the behavior of LocalStack. These can be set in your shell or included in a `.env` file.\n\n4. **Build the Image**: Use Docker Compose to build the LocalStack image. This process will pull the necessary base images and dependencies, setting up the environment for LocalStack to run.\n\n5. **Run the Container**: Once the image is built, you can start the LocalStack container. The container will expose the LocalStack Gateway on port 4566 and a range of ports (4510-4559) for various AWS services.\n\n6. **Data Persistence**: LocalStack will store its data in a volume mapped to your local directory (as specified in the `docker-compose.yml`), ensuring that your configurations and state persist across restarts.\n\n### Running Commands\nTo run LocalStack, execute the appropriate Docker Compose command to start the services defined in the `docker-compose.yml`. This will initialize the LocalStack environment, allowing you to interact with the emulated AWS services.\n\n### Summary\nBy following these steps, you will successfully build and run the LocalStack Docker image, enabling you to develop and test AWS applications locally without the need for actual AWS resources.", ".dockerignore": "To build and run a Docker image using the `.dockerignore` file from the `localstack/localstack` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image for LocalStack is designed to provide a fully functional local AWS cloud stack for testing and development purposes. It allows developers to simulate AWS services locally, enabling them to build and test applications without incurring costs or needing access to the actual AWS environment. The image typically includes dependencies such as Python, various AWS SDKs, and tools necessary to run and manage the simulated services.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `localstack/localstack` repository to your local machine. This will give you access to the necessary files, including the `.dockerignore` and Dockerfile.\n\n2. **Navigate to the Directory**: Change your working directory to the cloned repository where the Dockerfile is located.\n\n3. **Review the .dockerignore File**: The `.dockerignore` file is crucial as it specifies which files and directories should be excluded from the Docker build context. This helps to reduce the image size and build time by ignoring unnecessary files such as:\n   - Virtual environment directories (e.g., `.venv*`)\n   - Temporary files and directories (e.g., `target/`, `htmlcov/`)\n   - CI-generated files (e.g., `tests/aws/**/node_modules`, `tests/aws/**/.terraform`)\n   - Python cache files (e.g., `**/__pycache__`)\n\n4. **Build the Image**: Use the Docker build command to create the image. This process will read the Dockerfile and build the image according to the specified instructions, while respecting the exclusions defined in the `.dockerignore` file. The build process may take some time as it pulls the necessary base images and installs the required dependencies.\n\n5. **Run the Image**: After successfully building the image, you can run it using Docker. This will start a container based on the image, allowing you to interact with the LocalStack services. Ensure that you configure the container with the appropriate environment variables and port mappings to access the simulated AWS services.\n\n### Summary\nBy following these steps, you will have a Docker image that encapsulates the LocalStack environment, ready for local development and testing of AWS applications. The use of the `.dockerignore` file ensures that your image remains lightweight and free from unnecessary files, optimizing both build time and performance."}}, "timestamp": "2025-10-30T23:38:01.206150"}
{"repo_name": "openinterpreter/open-interpreter", "stars": 60743, "language": "Python", "tasks": [{"task_title": "Chat with interpreter", "task_description": "Executes a single command to chat with the interpreter.", "example_code": "interpreter.chat(\"Plot AAPL and META's normalized stock prices\")", "running_command": null, "expected_input": "Plot AAPL and META's normalized stock prices", "expected_output": "Plots of normalized stock prices for AAPL and META."}, {"task_title": "Interactive chat session", "task_description": "Starts an interactive chat session with the interpreter.", "example_code": "interpreter.chat()", "running_command": null, "expected_input": null, "expected_output": "Interactive chat interface."}, {"task_title": "Stream chat response", "task_description": "Streams the response for a given message without displaying it immediately.", "example_code": "for chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)", "running_command": null, "expected_input": "What operating system are we on?", "expected_output": "Chunks of responses from the interpreter."}, {"task_title": "Reset messages", "task_description": "Resets the interpreter's memory of previous messages.", "example_code": "interpreter.messages = []", "running_command": null, "expected_input": null, "expected_output": "Interpreter memory reset."}, {"task_title": "Resume chat from messages", "task_description": "Resumes chat from saved messages.", "example_code": "messages = interpreter.chat(\"My name is Killian.\")\ninterpreter.messages = messages", "running_command": null, "expected_input": "My name is Killian.", "expected_output": "Continued conversation remembering 'Killian'."}, {"task_title": "Add subtitles to videos", "task_description": "Adds subtitles to all videos in a specified directory.", "example_code": "interpreter.chat(\"Add subtitles to all videos in /videos.\")", "running_command": null, "expected_input": "Add subtitles to all videos in /videos.", "expected_output": "Subtitles added to all videos in the specified directory."}, {"task_title": "Modify subtitles size", "task_description": "Requests to modify the size of subtitles that were just added.", "example_code": "interpreter.chat(\"These look great but can you make the subtitles bigger?\")", "running_command": null, "expected_input": "These look great but can you make the subtitles bigger?", "expected_output": "Subtitles size modified."}, {"task_title": "Run shell commands", "task_description": "Allows the interpreter to run shell commands without user confirmation.", "example_code": "interpreter.system_message += \"\"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"\"", "running_command": null, "expected_input": null, "expected_output": "System message updated to allow shell commands."}, {"task_title": "Set model for interpreter", "task_description": "Sets the language model to be used by the interpreter.", "example_code": "interpreter.llm.model = \"gpt-3.5-turbo\"", "running_command": null, "expected_input": null, "expected_output": "Model set to gpt-3.5-turbo."}, {"task_title": "Chat endpoint in FastAPI", "task_description": "Creates an endpoint to handle chat requests in a FastAPI application.", "example_code": "@app.get(\"/chat\")\ndef chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")", "running_command": null, "expected_input": "message", "expected_output": "Streaming response of chat results."}, {"task_title": "History endpoint in FastAPI", "task_description": "Creates an endpoint to retrieve the history of messages.", "example_code": "@app.get(\"/history\")\ndef history_endpoint():\n    return interpreter.messages", "running_command": null, "expected_input": null, "expected_output": "List of previous messages."}], "setup": {"setup_commands": ["pip install git+https://github.com/OpenInterpreter/open-interpreter.git", "pip install open-interpreter", "pip install fastapi uvicorn\nuvicorn server:app --reload"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the openinterpreter/open-interpreter repository, follow these steps:\n\n### Building the Docker Image\n\n1. **Set Up Your Environment**: Ensure you have Docker installed on your machine. You can download it from the official Docker website.\n\n2. **Clone the Repository**: Use Git to clone the openinterpreter/open-interpreter repository to your local machine. This will give you access to the Dockerfile and necessary files.\n\n3. **Navigate to the Directory**: Open a terminal and change to the directory where the Dockerfile is located.\n\n4. **Build the Image**: Execute the Docker build command. This process will:\n   - Use the official Python 3.11.8 image as the base, ensuring you have a compatible environment for running Python applications.\n   - Set environment variables, including the server host to `0.0.0.0`, allowing the server to be accessible from outside the container.\n   - Create necessary directories and copy required files (such as the interpreter code, scripts, and dependency files) into the container.\n   - Install the server dependencies specified in the `pyproject.toml` and `poetry.lock` files, ensuring all necessary libraries and packages are available for the application to run.\n   - Expose port 8000, which is the designated port for the websocket server.\n\n### Running the Docker Container\n\n1. **Run the Container**: After successfully building the image, you can run the container. This will start the LMC-compatible websocket server, which listens on port 8000.\n\n2. **Access the Server**: Once the container is running, you can access the websocket server from your browser or any websocket client by connecting to `http://<your-docker-host>:8000`.\n\n### Purpose of the Image\n\nThe purpose of this Docker image is to provide a lightweight, isolated environment for running an LMC-compatible websocket server. This server facilitates communication based on the LMC protocol, allowing for real-time interactions and data exchanges. The image includes all necessary dependencies to ensure the server operates correctly and efficiently.\n\nBy following these steps, you will have a fully functional Docker container running the websocket server from the openinterpreter/open-interpreter repository."}}, "timestamp": "2025-10-30T23:38:31.254781"}
{"repo_name": "FoundationAgents/MetaGPT", "stars": 59151, "language": "Python", "tasks": [{"task_title": "åˆå§‹åŒ–é…ç½®", "task_description": "æ­¤ä»»åŠ¡ç”¨äºåˆå§‹åŒ–MetaGPTçš„é…ç½®æ–‡ä»¶ï¼Œåˆ›å»ºé»˜è®¤çš„é…ç½®æ–‡ä»¶ä»¥ä¾›ç”¨æˆ·ä¿®æ”¹ã€‚", "example_code": null, "running_command": "metagpt --init-config", "expected_input": null, "expected_output": "åˆ›å»º ~/.metagpt/config2.yaml æ–‡ä»¶"}], "setup": {"setup_commands": ["pip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the FoundationAgents/MetaGPT repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to provide an environment for running MetaGPT, a project that likely involves machine learning or natural language processing tasks. The image includes essential dependencies such as Python 3.9 and Node.js 20, along with libraries and tools necessary for the project to function correctly.\n\n### Building the Image\n1. **Base Image**: The build process starts with a lightweight base image that includes both Python 3.9 and Node.js 20. This ensures that the environment is suitable for running applications that require both programming languages.\n\n2. **Install System Dependencies**: The image installs several Debian packages that are critical for the operation of MetaGPT. This includes libraries for handling fonts, Chromium for rendering, and Git for version control. The installation is performed in a single command to minimize the final image size by cleaning up unnecessary files afterward.\n\n3. **Install Mermaid CLI**: The Mermaid CLI is installed globally using npm. This tool is likely used for generating diagrams and visualizations, which can be an integral part of the MetaGPT project. The installation process also includes cleaning the npm cache to further reduce the image size.\n\n4. **Copy Project Files**: The contents of the repository are copied into the image, specifically into the `/app/metagpt` directory. This step ensures that all necessary project files are available within the container.\n\n5. **Install Python Dependencies**: The image sets the working directory to where the project files are located and installs the required Python packages listed in the `requirements.txt` file. Additionally, the project itself is installed in editable mode, allowing for easy modifications during development.\n\n6. **Final Command**: The image is configured to run indefinitely using a command that keeps the container alive. This is useful for debugging or for running the application in a controlled environment.\n\n### Running the Image\nAfter building the image, you can run it using Docker. The container will start and remain active, allowing you to interact with it or execute further commands as needed.\n\n### Summary\nIn summary, this Docker image provides a robust environment for the MetaGPT project, bundling together necessary programming languages, libraries, and tools while maintaining a small footprint. Follow the standard Docker commands to build and run the image, ensuring you have Docker installed and configured on your machine.", ".dockerignore": "To build and run a Docker image for the FoundationAgents/MetaGPT repository, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image is designed to encapsulate the MetaGPT application, which serves as a foundation for building and deploying generative pre-trained transformers. The image includes all necessary dependencies, configurations, and runtime environments required to ensure the application operates seamlessly in a containerized environment.\n\n### Building Steps\n\n1. **Prepare the Environment**: Ensure that Docker is installed on your machine. You should also have access to the FoundationAgents/MetaGPT repository.\n\n2. **Create a .dockerignore File**: This file is crucial as it specifies which files and directories should be excluded from the Docker build context. For this repository, the .dockerignore file includes entries such as `workspace`, `tmp`, `build`, `dist`, `data`, and `geckodriver.log`. This helps to reduce the image size and build time by omitting unnecessary files.\n\n3. **Build the Docker Image**: Navigate to the root directory of the repository in your terminal. Use the Docker build command to create the image. The build process will read the Dockerfile and .dockerignore file, incorporating only the relevant files and dependencies into the image.\n\n4. **Dependencies**: The image will typically include essential libraries and tools required for running the MetaGPT application, such as Python, necessary Python packages (like TensorFlow or PyTorch), and any other dependencies specified in the project documentation.\n\n5. **Run the Docker Container**: Once the image is built, you can run it as a container. This step involves specifying the necessary environment variables, port mappings, and any volume mounts required for persistent data storage or configuration.\n\n### Final Notes\nAfter successfully running the container, you should be able to access the MetaGPT application via the specified ports. Ensure to consult the repository's README for any specific commands or configurations that may be necessary for optimal operation."}}, "timestamp": "2025-10-30T23:38:52.793292"}
{"repo_name": "meta-llama/llama", "stars": 58885, "language": "Python", "tasks": [{"task_title": "èŠå¤©å®Œæˆä»»åŠ¡", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨é¢„è®­ç»ƒçš„Llamaæ¨¡å‹è¿›è¡ŒèŠå¤©å®Œæˆã€‚é€šè¿‡æŒ‡å®šæ£€æŸ¥ç‚¹ç›®å½•ã€åˆ†è¯å™¨è·¯å¾„ã€æœ€å¤§åºåˆ—é•¿åº¦å’Œæ‰¹é‡å¤§å°æ¥è¿›è¡Œé…ç½®ã€‚", "example_code": null, "running_command": "torchrun --nproc_per_node 1 example_chat_completion.py --ckpt_dir llama-2-7b-chat/ --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 6", "expected_input": "ç”¨æˆ·è¾“å…¥çš„èŠå¤©æ–‡æœ¬", "expected_output": "æ¨¡å‹ç”Ÿæˆçš„èŠå¤©å›å¤"}], "setup": {"setup_commands": ["    pip install -e ."], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:39:02.878921"}
{"repo_name": "soimort/you-get", "stars": 56522, "language": "Python", "tasks": [{"task_title": "ä¸‹è½½è§†é¢‘", "task_description": "ä½¿ç”¨you-getåº“ä»YouTubeä¸‹è½½è§†é¢‘ï¼ŒæŒ‡å®šè§†é¢‘URLã€‚", "example_code": null, "running_command": "you-get", "expected_input": "https://www.youtube.com/watch?v=jNQXAC9IVRw", "expected_output": "Downloading Me at the zoo.webm ...\n 100% (  0.5/  0.5MB) â”œâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”¤[1/1]    6 MB/s\n\nSaving Me at the zoo.en.srt ... Done."}], "setup": {"setup_commands": ["pip install you-get", "[sudo] python -m pip install .", "python -m pip install . --user", "git clone git://github.com/soimort/you-get.git", "brew install you-get", "pip install --upgrade you-get", "pip install --upgrade --force-reinstall git+https://github.com/soimort/you-get@develop"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:39:21.796529"}
{"repo_name": "ultralytics/yolov5", "stars": 55874, "language": "Python", "tasks": [{"task_title": "Load YOLOv5 model", "task_description": "åŠ è½½YOLOv5æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒå¤šç§æ¨¡å‹é€‰é¡¹ã€‚", "example_code": "import torch\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")", "running_command": null, "expected_input": null, "expected_output": "YOLOv5 model loaded"}, {"task_title": "Perform inference on an image", "task_description": "å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ¨ç†ï¼Œè‡ªåŠ¨å¤„ç†æ‰¹é‡ã€è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–ã€‚", "example_code": "img = \"https://ultralytics.com/images/zidane.jpg\"\nresults = model(img)", "running_command": null, "expected_input": "https://ultralytics.com/images/zidane.jpg", "expected_output": "Inference results"}, {"task_title": "Print inference results", "task_description": "å°†æ¨ç†ç»“æœæ‰“å°åˆ°æ§åˆ¶å°ã€‚", "example_code": "results.print()", "running_command": null, "expected_input": null, "expected_output": "Inference results printed"}, {"task_title": "Show inference results", "task_description": "åœ¨çª—å£ä¸­æ˜¾ç¤ºæ¨ç†ç»“æœã€‚", "example_code": "results.show()", "running_command": null, "expected_input": null, "expected_output": "Inference results displayed in a window"}, {"task_title": "Save inference results", "task_description": "å°†æ¨ç†ç»“æœä¿å­˜åˆ°æŒ‡å®šç›®å½•ã€‚", "example_code": "results.save()", "running_command": null, "expected_input": null, "expected_output": "Inference results saved to runs/detect/exp"}, {"task_title": "Run inference using a webcam", "task_description": "ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œå®æ—¶æ¨ç†ã€‚", "example_code": null, "running_command": "python detect.py --weights yolov5s.pt --source 0", "expected_input": null, "expected_output": "Inference results displayed from webcam"}, {"task_title": "Run inference on a local image file", "task_description": "å¯¹æœ¬åœ°å›¾åƒæ–‡ä»¶è¿›è¡Œæ¨ç†ã€‚", "example_code": null, "running_command": "python detect.py --weights yolov5s.pt --source img.jpg", "expected_input": "img.jpg", "expected_output": "Inference results for img.jpg"}, {"task_title": "Train YOLOv5 model", "task_description": "åœ¨COCOæ•°æ®é›†ä¸Šè®­ç»ƒYOLOv5æ¨¡å‹ã€‚", "example_code": null, "running_command": "python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64", "expected_input": null, "expected_output": "Model trained on COCO dataset"}, {"task_title": "Validate the model", "task_description": "éªŒè¯è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ€§èƒ½ã€‚", "example_code": null, "running_command": "python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640", "expected_input": null, "expected_output": "Validation results"}, {"task_title": "Run prediction with a trained model", "task_description": "ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„æµ‹ã€‚", "example_code": null, "running_command": "python segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg", "expected_input": "data/images/bus.jpg", "expected_output": "Prediction results for bus.jpg"}, {"task_title": "Export model", "task_description": "å°†è®­ç»ƒå¥½çš„æ¨¡å‹å¯¼å‡ºä¸ºä¸åŒæ ¼å¼ã€‚", "example_code": null, "running_command": "python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0", "expected_input": null, "expected_output": "Model exported to specified formats"}], "setup": {"setup_commands": ["# Install the ultralytics package\npip install ultralytics", "# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image for the Ultralytics YOLOv5 repository, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image is designed to encapsulate the YOLOv5 environment, which is a popular framework for object detection tasks. It simplifies the deployment of the YOLOv5 model by ensuring that all necessary dependencies are included and that the environment is consistent across different systems. This image typically includes Python, relevant libraries (such as PyTorch, OpenCV, and others), and any additional tools required for training and inference.\n\n### Dependencies\nThe image relies on several key dependencies:\n- **Python**: The primary programming language used for YOLOv5.\n- **PyTorch**: A deep learning framework essential for running the YOLOv5 model.\n- **OpenCV**: A library for computer vision tasks, often used for image processing.\n- **Other Libraries**: Additional Python packages as specified in the requirements, which may include libraries for data handling, visualization, and model evaluation.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the YOLOv5 repository from GitHub to your local machine. This will provide access to the necessary files, including the Docker configuration.\n\n2. **Prepare the Environment**: Ensure that Docker is installed and running on your machine. This is crucial for building and running the Docker image.\n\n3. **Create a .dockerignore File**: The `.dockerignore` file is essential for excluding unnecessary files and directories from the Docker context. This helps in reducing the image size and speeding up the build process. The provided `.dockerignore` file includes patterns to ignore files such as:\n   - Git-related files and directories (e.g., `.git`, `.idea`)\n   - Cache files and temporary outputs (e.g., `.cache`, `runs`, `output`)\n   - Model weights and checkpoints (e.g., `*.pt`, `*.pth`)\n   - Python bytecode and virtual environments (e.g., `__pycache__`, `*.egg-info`, `.venv`)\n\n4. **Build the Docker Image**: Use Docker commands to build the image from the repository. This process will read the Dockerfile and the `.dockerignore` file, ensuring that only the necessary files are included in the image.\n\n5. **Run the Docker Container**: Once the image is built, you can run a container from this image. The container will have all the dependencies installed and will be ready to execute YOLOv5 tasks, such as training a model or performing inference on images.\n\n6. **Access the Container**: You can access the running container to execute commands or scripts related to YOLOv5, allowing for interactive use or batch processing of images.\n\nBy following these steps, you will have a fully functional Docker environment for working with YOLOv5, enabling efficient development and deployment of object detection models."}}, "timestamp": "2025-10-30T23:40:13.902037"}
{"repo_name": "ageitgey/face_recognition", "stars": 55660, "language": "Python", "tasks": [{"task_title": "è·å–äººè„¸ä½ç½®", "task_description": "åŠ è½½å›¾åƒå¹¶è·å–å…¶ä¸­æ‰€æœ‰äººè„¸çš„ä½ç½®åæ ‡ã€‚", "example_code": "import face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image)", "running_command": null, "expected_input": "my_picture.jpg", "expected_output": "äººè„¸ä½ç½®çš„åæ ‡æ•°ç»„"}, {"task_title": "è·å–äººè„¸ç‰¹å¾ç‚¹", "task_description": "åŠ è½½å›¾åƒå¹¶è·å–å…¶ä¸­æ‰€æœ‰äººè„¸çš„ç‰¹å¾ç‚¹ä½ç½®ã€‚", "example_code": "import face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)", "running_command": null, "expected_input": "my_picture.jpg", "expected_output": "äººè„¸ç‰¹å¾ç‚¹çš„æ•°ç»„"}, {"task_title": "äººè„¸ç¼–ç æ¯”è¾ƒ", "task_description": "åŠ è½½å·²çŸ¥å’ŒæœªçŸ¥å›¾åƒï¼Œè·å–äººè„¸ç¼–ç å¹¶æ¯”è¾ƒäºŒè€…æ˜¯å¦ä¸ºåŒä¸€äººã€‚", "example_code": "import face_recognition\nknown_image = face_recognition.load_image_file(\"biden.jpg\")\nunknown_image = face_recognition.load_image_file(\"unknown.jpg\")\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)", "running_command": null, "expected_input": "biden.jpg, unknown.jpg", "expected_output": "[True/False]"}, {"task_title": "æ‰¹é‡äººè„¸è¯†åˆ«", "task_description": "è¯†åˆ«æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æœªçŸ¥äººè„¸ï¼Œå¹¶ä¸å·²çŸ¥äººè„¸è¿›è¡Œæ¯”è¾ƒã€‚", "example_code": null, "running_command": "face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person"}, {"task_title": "äººè„¸æ£€æµ‹", "task_description": "æ£€æµ‹æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒä¸­çš„äººè„¸ä½ç½®ã€‚", "example_code": null, "running_command": "face_detection  ./folder_with_pictures/", "expected_input": "./folder_with_pictures/", "expected_output": "examples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792"}, {"task_title": "è®¾ç½®å®¹å·®è¿›è¡Œäººè„¸æ¯”è¾ƒ", "task_description": "è®¾ç½®å®¹å·®å€¼æ¥è¿›è¡Œäººè„¸æ¯”è¾ƒè¯†åˆ«ã€‚", "example_code": null, "running_command": "face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person"}, {"task_title": "æ˜¾ç¤ºè·ç¦»è¿›è¡Œäººè„¸æ¯”è¾ƒ", "task_description": "åœ¨è¯†åˆ«ç»“æœä¸­æ˜¾ç¤ºäººè„¸ç¼–ç ä¹‹é—´çš„è·ç¦»ã€‚", "example_code": null, "running_command": "face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None"}, {"task_title": "å¹¶è¡Œå¤„ç†äººè„¸è¯†åˆ«", "task_description": "ä½¿ç”¨å¤šä¸ªCPUè¿›è¡Œäººè„¸è¯†åˆ«ä»¥æé«˜æ•ˆç‡ã€‚", "example_code": null, "running_command": "face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "è¯†åˆ«ç»“æœ"}, {"task_title": "åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬äºº", "task_description": "æ¯”è¾ƒè‡ªå·±å’ŒæœªçŸ¥å›¾åƒçš„äººè„¸ç¼–ç ï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºåŒä¸€äººã€‚", "example_code": "import face_recognition\npicture_of_me = face_recognition.load_image_file(\"me.jpg\")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\nunknown_picture = face_recognition.load_image_file(\"unknown.jpg\")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\nif results[0] == True:\n    print(\"It's a picture of me!\")\nelse:\n    print(\"It's not a picture of me!\")", "running_command": null, "expected_input": "me.jpg, unknown.jpg", "expected_output": "It's a picture of me!\næˆ–\nIt's not a picture of me!"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run a Docker image using the \"Dockerfile\" from the `ageitgey/face_recognition` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to facilitate the deployment of applications that utilize the `face_recognition` library, which is built on top of the `dlib` library. This image provides a pre-configured environment with all necessary dependencies for face recognition tasks, including image processing capabilities.\n\n### Dependencies\nThe image is based on a slim version of Python 3.10.3 and includes essential packages and libraries required for building and running the `face_recognition` library. Key dependencies include:\n\n- **System Libraries**: `build-essential`, `cmake`, `gfortran`, `git`, `wget`, `curl`, `graphicsmagick`, and various development libraries for image processing and numerical computations (e.g., `libjpeg-dev`, `libatlas-base-dev`, `libavcodec-dev`, etc.).\n- **Python Libraries**: The image installs Python packages like `numpy` and any additional libraries specified in a `requirements.txt` file located in the application directory.\n\n### Building Steps\n1. **Pull the Base Image**: Start with the official Python 3.10.3 slim image as the base.\n2. **Update Package List**: Update the package list to ensure you have the latest versions of the dependencies.\n3. **Install System Dependencies**: Install all required system packages and libraries necessary for compiling and running the `face_recognition` library and its dependencies.\n4. **Clone and Install dlib**: Clone the `dlib` repository from GitHub, check out a specific version, and install it using Python.\n5. **Copy Application Files**: Copy your application files into the image, specifically into the `/root/face_recognition` directory.\n6. **Install Python Dependencies**: Install any additional Python dependencies listed in your `requirements.txt` file and set up the application.\n7. **Set the Command**: Define the default command to run an example script that demonstrates face recognition functionality.\n\n### Running the Image\nAfter building the image, you can run it using Docker. The default command will execute a script that recognizes faces in pictures, demonstrating the capabilities of the `face_recognition` library.\n\n### Summary\nThis Docker image provides a streamlined environment for face recognition applications, ensuring all necessary dependencies are installed and configured correctly. By following the outlined steps, you can easily build and run your own applications based on the `face_recognition` library.", "docker-compose.yml": "To build and run the Docker container for the `ageitgey/face_recognition` repository using the provided `docker-compose.yml` file, follow these steps:\n\n### Purpose of the Image\nThe Docker image created from this configuration is designed to facilitate face recognition tasks using Python. It leverages the `face_recognition` library, which is built on top of the `dlib` library, providing capabilities for detecting and recognizing faces in images. This image is particularly useful for developers and researchers who want to experiment with or deploy face recognition applications without worrying about the underlying dependencies and environment setup.\n\n### Dependencies\nThe image includes essential dependencies such as Python, the `face_recognition` library, and any other libraries required for image processing and manipulation. The environment is pre-configured to run Python scripts that utilize these libraries effectively.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `ageitgey/face_recognition` repository to your local machine. This will provide you access to the `docker-compose.yml` file and the necessary codebase.\n\n2. **Navigate to the Directory**: Change your working directory to the root of the cloned repository where the `docker-compose.yml` file is located.\n\n3. **Build the Image**: Use Docker Compose to build the image. This process will read the `docker-compose.yml` file, set up the necessary environment, and install all dependencies defined in the context. The build context is set to the current directory, ensuring that all files are accessible during the build process.\n\n### Running the Container\n1. **Start the Service**: Once the image is built, you can start the face recognition service using Docker Compose. This will create a container named `face_recognition` and set the working directory inside the container to `/face_recognition/examples`.\n\n2. **Execute the Command**: The container will run the Python script `find_faces_in_picture_cnn.py`, which is designed to find faces in a given picture using a convolutional neural network (CNN). \n\n3. **Volume Mapping**: The current directory is mapped to the `/face_recognition` directory inside the container, allowing you to access and modify files easily. This setup is ideal for testing and development, as any changes made locally will be reflected inside the container.\n\n### Optional GPU Support\nIf you have a compatible NVIDIA GPU and want to leverage GPU acceleration, you can uncomment the relevant lines in the `docker-compose.yml` file. This will enable the use of NVIDIA Docker, allowing the container to utilize GPU resources for faster processing.\n\n### Conclusion\nBy following these steps, you will successfully build and run the face recognition Docker container, enabling you to experiment with face detection and recognition tasks efficiently."}}, "timestamp": "2025-10-30T23:41:06.057232"}
{"repo_name": "unclecode/crawl4ai", "stars": 55247, "language": "Python", "tasks": [{"task_title": "Basic crawl with markdown output", "task_description": "ä½¿ç”¨crawl4aiåº“è¿›è¡ŒåŸºæœ¬çš„ç½‘é¡µæŠ“å–ï¼Œå¹¶ä»¥markdownæ ¼å¼è¾“å‡ºç»“æœã€‚", "example_code": null, "running_command": "crwl https://www.nbcnews.com/business -o markdown", "expected_input": "https://www.nbcnews.com/business", "expected_output": "æŠ“å–çš„å†…å®¹ä»¥markdownæ ¼å¼è¾“å‡º"}, {"task_title": "Deep crawl with BFS strategy", "task_description": "ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ç­–ç•¥è¿›è¡Œæ·±åº¦æŠ“å–ï¼Œé™åˆ¶æœ€å¤§é¡µé¢æ•°ä¸º10ã€‚", "example_code": null, "running_command": "crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10", "expected_input": "https://docs.crawl4ai.com", "expected_output": "æŠ“å–çš„å†…å®¹ï¼Œæœ€å¤š10ä¸ªé¡µé¢"}, {"task_title": "Use LLM extraction with a specific question", "task_description": "ä½¿ç”¨LLMæå–åŠŸèƒ½ï¼Œé’ˆå¯¹ç‰¹å®šé—®é¢˜ä»ç½‘é¡µä¸­æå–ä¿¡æ¯ã€‚", "example_code": null, "running_command": "crwl https://www.example.com/products -q \"Extract all product prices\"", "expected_input": "https://www.example.com/products", "expected_output": "æå–çš„æ‰€æœ‰äº§å“ä»·æ ¼"}, {"task_title": "Submit a crawl job via API", "task_description": "é€šè¿‡HTTP POSTè¯·æ±‚æäº¤æŠ“å–ä»»åŠ¡ï¼Œå¹¶è·å–ç»“æœã€‚", "example_code": "response = requests.post(\n    \"http://localhost:11235/crawl\",\n    json={\"urls\": [\"https://example.com\"], \"priority\": 10}\n)", "running_command": null, "expected_input": "{\"urls\": [\"https://example.com\"], \"priority\": 10}", "expected_output": "Crawl job submitted successfully."}, {"task_title": "Crawl with custom extraction strategy", "task_description": "ä½¿ç”¨è‡ªå®šä¹‰çš„æå–ç­–ç•¥æŠ“å–ç½‘é¡µå†…å®¹ï¼Œå¹¶è¾“å‡ºæå–çš„å†…å®¹ã€‚", "example_code": "async def main():\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n    result = await crawler.arun(url=\"https://www.kidocode.com/degrees/technology\", config=run_config)", "running_command": null, "expected_input": "https://www.kidocode.com/degrees/technology", "expected_output": "Successfully extracted companies"}, {"task_title": "Extract model fees using LLM", "task_description": "ä½¿ç”¨LLMæå–åŠŸèƒ½ä»ç½‘é¡µä¸­æå–æ¨¡å‹åç§°åŠå…¶è´¹ç”¨ã€‚", "example_code": "result = await crawler.arun(url='https://openai.com/api/pricing/', config=run_config)", "running_command": null, "expected_input": "https://openai.com/api/pricing/", "expected_output": "æå–çš„æ¨¡å‹åŠè´¹ç”¨ä¿¡æ¯"}, {"task_title": "Crawl with persistent user data", "task_description": "ä½¿ç”¨æŒä¹…ç”¨æˆ·æ•°æ®ç›®å½•è¿›è¡ŒæŠ“å–ï¼Œä»¥é¿å…ç™»å½•ç­‰æŒ‘æˆ˜ã€‚", "example_code": "async with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url, config=run_config, magic=True)", "running_command": null, "expected_input": "ADDRESS_OF_A_CHALLENGING_WEBSITE", "expected_output": "Successfully crawled {url}"}, {"task_title": "Custom hooks for crawling", "task_description": "å®šä¹‰è‡ªå®šä¹‰é’©å­ä»¥ä¼˜åŒ–æŠ“å–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨Dockerå®¢æˆ·ç«¯è¿›è¡ŒæŠ“å–ã€‚", "example_code": "results = await client.crawl(\n    urls=[\"https://httpbin.org/html\"],\n    hooks={\n        \"on_page_context_created\": on_page_context_created,\n        \"before_goto\": before_goto\n    }\n)", "running_command": null, "expected_input": "https://httpbin.org/html", "expected_output": "æŠ“å–ç»“æœ"}, {"task_title": "Intelligent table extraction", "task_description": "ä½¿ç”¨LLMæå–ç­–ç•¥æ™ºèƒ½æå–ç½‘é¡µä¸­çš„è¡¨æ ¼æ•°æ®ã€‚", "example_code": "result = await crawler.arun(\"https://complex-tables-site.com\", config=config)", "running_command": null, "expected_input": "https://complex-tables-site.com", "expected_output": "Extracted table: {len(table['data'])} rows"}, {"task_title": "Bypass bot detection", "task_description": "ä½¿ç”¨undetected Chromeé…ç½®æŠ“å–å—ä¿æŠ¤çš„ç½‘ç«™ï¼Œç»•è¿‡Cloudflareç­‰æ£€æµ‹ã€‚", "example_code": "result = await crawler.arun(\"https://protected-site.com\")", "running_command": null, "expected_input": "https://protected-site.com", "expected_output": "æˆåŠŸæŠ“å–å†…å®¹"}, {"task_title": "Adaptive crawling", "task_description": "ä½¿ç”¨è‡ªé€‚åº”é…ç½®è¿›è¡ŒæŠ“å–ï¼Œå­¦ä¹ æ¨¡å¼ä»¥æé«˜æå–æ•ˆæœã€‚", "example_code": "state = await adaptive_crawler.digest(start_url=\"https://news.example.com\", query=\"latest news content\")", "running_command": null, "expected_input": "https://news.example.com", "expected_output": "æŠ“å–çš„æœ€æ–°æ–°é—»å†…å®¹"}, {"task_title": "Virtual scrolling", "task_description": "å®ç°è™šæ‹Ÿæ»šåŠ¨ä»¥æŠ“å–åŠ¨æ€åŠ è½½çš„å†…å®¹ã€‚", "example_code": "result = await crawler.arun(url, config=CrawlerRunConfig(virtual_scroll_config=scroll_config))", "running_command": null, "expected_input": "åŠ¨æ€åŠ è½½çš„ç½‘é¡µURL", "expected_output": "æŠ“å–çš„åŠ¨æ€å†…å®¹"}, {"task_title": "Link preview configuration", "task_description": "é…ç½®é“¾æ¥é¢„è§ˆï¼ŒæŒ‰ç›¸å…³æ€§å’Œè´¨é‡å¯¹é“¾æ¥è¿›è¡Œè¯„åˆ†ã€‚", "example_code": "result = await crawler.arun(url, config=CrawlerRunConfig(link_preview_config=link_config, score_links=True))", "running_command": null, "expected_input": "åŒ…å«é“¾æ¥çš„ç½‘é¡µURL", "expected_output": "é“¾æ¥çš„ç›¸å…³æ€§è¯„åˆ†"}], "setup": {"setup_commands": ["# Install the package\npip install -U crawl4ai\n\n# For pre release versions\npip install crawl4ai --pre\n\n# Run post-installation setup\ncrawl4ai-setup\n\n# Verify your installation\ncrawl4ai-doctor", "pip install crawl4ai\ncrawl4ai-setup # Setup the browser", "pip install crawl4ai[sync]", "git clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\npip install -e .                    # Basic installation in editable mode", "pip install -e \".[torch]\"           # With PyTorch features\npip install -e \".[transformer]\"     # With Transformer features\npip install -e \".[cosine]\"          # With cosine similarity features\npip install -e \".[sync]\"            # With synchronous crawling (Selenium)\npip install -e \".[all]\"             # Install all optional features", "  seeder = AsyncUrlSeeder(SeedingConfig(\n      source=\"sitemap+cc\",\n      pattern=\"*/blog/*\",\n      query=\"python tutorials\",\n      score_threshold=0.4\n  ))\n  \n  urls = await seeder.discover(\"https://example.com\")", "  pip install -U crawl4ai", "  pip install crawl4ai --pre", "  pip install crawl4ai==0.4.3b1"], "docker_commands": ["# Pull and run the latest release\ndocker pull unclecode/crawl4ai:latest\ndocker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:latest\n\n# Visit the playground at http://localhost:11235/playground"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the \"Crawl4AI\" project from the repository `unclecode/crawl4ai`, follow these steps:\n\n### Building the Docker Image\n\n1. **Base Image**: The Docker image is built on top of a slim version of Python 3.12, ensuring a lightweight environment suitable for running Python applications.\n\n2. **Dependencies**: The image installs essential system packages and libraries required for the application, including:\n   - Development tools (e.g., `build-essential`, `git`, `cmake`)\n   - Libraries for image processing and web scraping (e.g., `libjpeg-dev`, `libglib2.0-0`, `libnss3`)\n   - Redis server for caching and data storage.\n   - Supervisor for process management.\n\n3. **Environment Variables**: Several environment variables are set to optimize the Python environment and configure the application, including settings for Redis connection and Python execution behavior.\n\n4. **User Creation**: A non-root user (`appuser`) is created for enhanced security, and the application is configured to run under this user.\n\n5. **Application Setup**: The image includes logic to install the application either from a local source or directly from the GitHub repository. It also installs necessary Python packages defined in a `requirements.txt` file, along with optional packages based on the installation type specified.\n\n6. **Health Check**: A health check is defined to ensure that the application has sufficient memory and that the Redis service is running.\n\n7. **Final Configuration**: The application is set to run using Supervisor, which manages the application processes and ensures they are restarted if they fail.\n\n### Running the Docker Container\n\n1. **Pull the Image**: Use the following command to pull the latest release of the Crawl4AI image from Docker Hub:\n   ```bash\n   docker pull unclecode/crawl4ai:latest\n   ```\n\n2. **Run the Container**: Start a new container from the pulled image, mapping the necessary ports and allocating shared memory:\n   ```bash\n   docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:latest\n   ```\n\n3. **Access the Application**: Once the container is running, you can access the application playground by navigating to `http://localhost:11235/playground` in your web browser.\n\n### Summary\n\nThe Crawl4AI Docker image is designed to provide a robust environment for running a web crawler and scraper that is compatible with large language models (LLMs). It includes all necessary dependencies, is optimized for security and performance, and is easy to deploy using Docker.", "docker-compose.yml": "To build and run the Docker image for the `crawl4ai` project from the `unclecode/crawl4ai` repository using the provided `docker-compose.yml` file, follow these steps:\n\n### Building the Docker Image\n\n1. **Clone the Repository**: Start by cloning the `unclecode/crawl4ai` repository to your local machine. This will give you access to the `docker-compose.yml` file and any other necessary resources.\n\n2. **Prepare Environment Variables**: Create a `.llm.env` file in the root of the cloned repository. This file should contain your API keys for various services (e.g., OpenAI, DeepSeek, Anthropic, etc.). You can use the `.llm.env.example` file as a template to set up your environment variables. Ensure that the keys are correctly populated.\n\n3. **Build the Image**: Navigate to the directory containing the `docker-compose.yml` file in your terminal. Use the Docker Compose command to build the image. The build process will pull the necessary base images and install dependencies required for the application, such as libraries for web serving and any specific tools needed for crawling AI-related tasks. The resulting image will be tagged as `unclecode/crawl4ai` with the specified version.\n\n### Running the Docker Container\n\n4. **Run the Container**: After successfully building the image, you can run the container using Docker Compose. The command will start the `crawl4ai` service, mapping the internal port 11235 to the same port on your host machine. This allows you to access the application via your web browser.\n\n5. **Access the Application**: Once the container is running, open your web browser and navigate to `http://localhost:11235/playground`. This will take you to the application's playground interface, where you can interact with the features provided by the `crawl4ai` service.\n\n### Additional Notes\n\n- **Resource Management**: The Docker Compose configuration includes resource limits to ensure the container does not exceed 4GB of memory, with a reservation of 1GB. This is important for maintaining performance, especially when handling large datasets or multiple requests.\n\n- **Health Checks**: The configuration includes a health check that periodically verifies if the service is running correctly. If the service fails, Docker will attempt to restart it automatically.\n\n- **User Permissions**: The container runs under a non-root user (`appuser`) for improved security.\n\nBy following these steps, you will have a fully operational instance of the `crawl4ai` application running in a Docker container, ready for use and experimentation."}}, "timestamp": "2025-10-30T23:42:08.451764"}
{"repo_name": "Textualize/rich", "stars": 54232, "language": "Python", "tasks": [{"task_title": "Print Styled Text", "task_description": "ä½¿ç”¨Richåº“æ‰“å°å¸¦æœ‰æ ·å¼çš„æ–‡æœ¬ï¼Œæ”¯æŒé¢œè‰²å’Œæ ¼å¼ã€‚", "example_code": "from rich import print\n\nprint(\"Hello, [bold magenta]World[/bold magenta]!\")", "running_command": null, "expected_input": null, "expected_output": "Hello, World! (with 'World' in bold magenta)"}, {"task_title": "Pretty Print Installation", "task_description": "å®‰è£…Pretty PrintåŠŸèƒ½ä»¥å¢å¼ºè¾“å‡ºçš„å¯è¯»æ€§ã€‚", "example_code": "from rich import pretty\npretty.install()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "Create Console Object", "task_description": "åˆ›å»ºä¸€ä¸ªConsoleå¯¹è±¡ä»¥ç”¨äºåç»­çš„æ‰“å°å’Œæ—¥å¿—åŠŸèƒ½ã€‚", "example_code": "from rich.console import Console\n\nconsole = Console()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "Print Multiple Items", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°å¤šä¸ªé¡¹ç›®ã€‚", "example_code": "console.print(\"Hello\", \"World!\")", "running_command": null, "expected_input": null, "expected_output": "Hello World!"}, {"task_title": "Print Styled Text with Console", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°å¸¦æ ·å¼çš„æ–‡æœ¬ã€‚", "example_code": "console.print(\"Hello\", \"World!\", style=\"bold red\")", "running_command": null, "expected_input": null, "expected_output": "Hello World! (with 'Hello' in bold red)"}, {"task_title": "Print Styled Sentence", "task_description": "æ‰“å°ä¸€ä¸ªåŒ…å«å¤šç§æ ·å¼çš„å¥å­ã€‚", "example_code": "console.print(\"Where there is a [bold cyan]Will[/bold cyan] there [u]is[/u] a [i]way[/i].\")", "running_command": null, "expected_input": null, "expected_output": "Where there is a Will there is a way. (with 'Will' in bold cyan, 'is' underlined, and 'way' italicized)"}, {"task_title": "Inspect Python Object", "task_description": "ä½¿ç”¨inspectåŠŸèƒ½æŸ¥çœ‹Pythonå¯¹è±¡çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¶æ–¹æ³•ã€‚", "example_code": "my_list = [\"foo\", \"bar\"]\nfrom rich import inspect\ninspect(my_list, methods=True)", "running_command": null, "expected_input": null, "expected_output": "Detailed inspection of my_list with methods"}, {"task_title": "Log with Console", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡è®°å½•ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœ¬åœ°å˜é‡ã€‚", "example_code": "console.log(test_data, log_locals=True)", "running_command": null, "expected_input": null, "expected_output": "Log output of test_data with local variables"}, {"task_title": "Print Emoji", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°å¤šä¸ªEmojiã€‚", "example_code": "console.print(\":smiley: :vampire: :pile_of_poo: :thumbs_up: :raccoon:\")", "running_command": null, "expected_input": null, "expected_output": "ğŸ˜ƒ ğŸ§› ğŸ’© ğŸ‘ ğŸ¦"}, {"task_title": "Create and Print Table", "task_description": "åˆ›å»ºä¸€ä¸ªè¡¨æ ¼å¹¶æ‰“å°ï¼Œæ”¯æŒæ ·å¼å’Œå¯¹é½ã€‚", "example_code": "table = Table(show_header=True, header_style=\"bold magenta\")\ntable.add_row(...)\nconsole.print(table)", "running_command": null, "expected_input": null, "expected_output": "Formatted table output"}, {"task_title": "Track Progress", "task_description": "ä½¿ç”¨trackåŠŸèƒ½æ˜¾ç¤ºè¿›åº¦æ¡ã€‚", "example_code": "for step in track(range(100)):\n    do_step(step)", "running_command": null, "expected_input": null, "expected_output": "Progress bar indicating completion of steps"}, {"task_title": "Show Status with Console", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ˜¾ç¤ºä»»åŠ¡çŠ¶æ€ã€‚", "example_code": "with console.status(\"[bold green]Working on tasks...\") as status:\n    while tasks:\n        console.log(f\"{task} complete\")", "running_command": null, "expected_input": null, "expected_output": "Status message and task completion logs"}, {"task_title": "Print Directory Contents in Columns", "task_description": "æ‰“å°æŒ‡å®šç›®å½•çš„å†…å®¹ï¼Œä½¿ç”¨åˆ—æ ¼å¼ã€‚", "example_code": "directory = os.listdir(sys.argv[1])\nprint(Columns(directory))", "running_command": null, "expected_input": "Directory path", "expected_output": "Formatted list of directory contents in columns"}, {"task_title": "Print Markdown Content", "task_description": "ä»æ–‡ä»¶ä¸­è¯»å–Markdownå†…å®¹å¹¶æ‰“å°ã€‚", "example_code": "with open(\"README.md\") as readme:\n    markdown = Markdown(readme.read())\nconsole.print(markdown)", "running_command": null, "expected_input": null, "expected_output": "Formatted Markdown output"}, {"task_title": "Syntax Highlighting", "task_description": "æ‰“å°å¸¦æœ‰è¯­æ³•é«˜äº®çš„ä»£ç ç‰‡æ®µã€‚", "example_code": "syntax = Syntax(my_code, \"python\", theme=\"monokai\", line_numbers=True)\nconsole.print(syntax)", "running_command": null, "expected_input": null, "expected_output": "Formatted code output with syntax highlighting"}], "setup": {"setup_commands": ["python -m pip install rich"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:42:45.776661"}
{"repo_name": "OpenBB-finance/OpenBB", "stars": 54063, "language": "Python", "tasks": [{"task_title": "è·å–å†å²è‚¡ä»·", "task_description": "è¿™ä¸ªä»»åŠ¡ç”¨äºè·å–ç‰¹å®šè‚¡ç¥¨ï¼ˆå¦‚AAPLï¼‰çš„å†å²ä»·æ ¼æ•°æ®ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ•°æ®æ¡†æ ¼å¼ã€‚", "example_code": "from openbb import obb\noutput = obb.equity.price.historical(\"AAPL\")\ndf = output.to_dataframe()", "running_command": null, "expected_input": "AAPL", "expected_output": "DataFrame containing historical price data for AAPL"}], "setup": {"setup_commands": ["pip install \"openbb[all]\""], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:42:53.128038"}
{"repo_name": "Z4nzu/hackingtool", "stars": 53904, "language": "Python", "tasks": [{"task_title": "æ„å»ºDockeré•œåƒ", "task_description": "ä½¿ç”¨Dockeræ„å»ºä¸€ä¸ªåä¸ºvgpastor/hackingtoolçš„é•œåƒã€‚", "example_code": null, "running_command": "docker build -t vgpastor/hackingtool .", "expected_input": null, "expected_output": "æ„å»ºæˆåŠŸçš„Dockeré•œåƒ"}, {"task_title": "é€‰æ‹©æ“ä½œç³»ç»Ÿ", "task_description": "åœ¨CLIä¸­é€‰æ‹©è¦ä½¿ç”¨çš„æ“ä½œç³»ç»Ÿé€‰é¡¹ã€‚", "example_code": null, "running_command": null, "expected_input": "1, 2, or 0", "expected_output": "é€‰æ‹©çš„æ“ä½œç³»ç»Ÿæˆ–é€€å‡º"}], "setup": {"setup_commands": [], "docker_commands": ["docker-compose up -d", "docker exec -it hackingtool bash"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image from the Z4nzu/hackingtool repository, follow these steps:\n\n### Building the Docker Image\n\n1. **Base Image**: The Docker image is based on the latest version of Kali Linux, a distribution known for its penetration testing tools. This provides a robust environment for security-related tasks.\n\n2. **Dependencies**: During the build process, several essential packages are installed:\n   - **Git**: For version control and managing code repositories.\n   - **Python3 and pip**: To run Python scripts and manage Python packages.\n   - **Figlet**: For creating large text banners in the terminal.\n   - **Sudo**: To allow users to run commands with elevated privileges.\n   - **Boxes**: For creating text-based boxes in the terminal.\n   - **PHP, Curl, xdotool, and wget**: Various utilities for web interaction, automation, and downloading files.\n\n3. **Working Directory**: The working directory is set to `/root/hackingtool`, where the application code will reside.\n\n4. **Python Dependencies**: A `requirements.txt` file is copied into the image, and the necessary Python packages (including Flask for web applications and Requests for HTTP requests) are installed without caching to save space.\n\n5. **Application Code**: The entire application code is copied into the image.\n\n6. **Path Storage**: A text file is created to store the path to the hacking tool, which can be useful for reference or configuration.\n\n7. **Port Exposure**: The image exposes all ports from 1 to 65535, allowing for a wide range of network interactions and services.\n\n8. **Entrypoint**: The container is configured to run a Python script (`hackingtool.py`) when started, which is the main application logic.\n\n### Running the Docker Container\n\n1. **Build the Image**: Use the Docker build command to create the image from the Dockerfile. This will compile all the dependencies and set up the environment as described.\n\n2. **Start the Container**: You can use Docker Compose to start the container in detached mode with the command `docker-compose up -d`. This will ensure that the application runs in the background.\n\n3. **Access the Container**: To interact with the container, use the command `docker exec -it hackingtool bash`. This will open a shell session inside the running container, allowing you to execute commands and interact with the application directly.\n\n### Purpose of the Image\n\nThe primary purpose of this Docker image is to provide a comprehensive environment for running a hacking tool that likely includes various security testing functionalities. It simplifies the setup process by bundling all necessary dependencies and configurations into a single container, making it easy to deploy and use in different environments.", "docker-compose.yml": "To build and run the Docker image using the `docker-compose.yml` file from the Z4nzu/hackingtool repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image, named `vgpastor/hackingtool`, is designed to provide a comprehensive environment for penetration testing and ethical hacking. It includes a variety of pre-installed tools and utilities that facilitate security assessments and vulnerability analysis. The image is built to support various hacking methodologies, making it suitable for both beginners and experienced security professionals.\n\n### Dependencies\nThe image relies on several underlying packages and libraries that are essential for running the included hacking tools effectively. These dependencies may include networking utilities, programming languages, and specific libraries tailored for security testing. The container is configured to allow interactive sessions, enabling users to execute commands and scripts directly within the environment.\n\n### Building and Running Steps\n1. **Clone the Repository**: Start by cloning the Z4nzu/hackingtool repository to your local machine. This will give you access to the `docker-compose.yml` file along with any other necessary resources.\n\n2. **Navigate to the Directory**: Change your working directory to the location where the `docker-compose.yml` file is stored.\n\n3. **Build and Run the Container**: Use Docker Compose to build and run the container in detached mode. This command initializes the container based on the specifications in the `docker-compose.yml` file, setting up the necessary environment and dependencies.\n\n4. **Access the Container**: Once the container is running, you can access it interactively. This allows you to use the tools and utilities installed within the container. You will be able to execute commands as if you were using a standard terminal.\n\n5. **Volume Mapping**: The configuration includes volume mapping that links your local directory to the container's `/root/hackingtool` directory. This setup enables you to easily manage files and scripts between your host machine and the container.\n\n6. **Port Configuration**: The container is set to expose port 22, which allows for SSH access. This can be useful for remote management or for running specific tools that require SSH connectivity.\n\nBy following these steps, you will successfully build and run the Docker container, providing you with a powerful environment for ethical hacking and security testing."}}, "timestamp": "2025-10-30T23:43:24.606543"}
{"repo_name": "psf/requests", "stars": 53426, "language": "Python", "tasks": [{"task_title": "å‘é€GETè¯·æ±‚å¹¶è¿›è¡ŒåŸºæœ¬è®¤è¯", "task_description": "ä½¿ç”¨requestsåº“å‘é€ä¸€ä¸ªGETè¯·æ±‚åˆ°æŒ‡å®šURLï¼Œå¹¶ä½¿ç”¨åŸºæœ¬è®¤è¯è¿›è¡Œèº«ä»½éªŒè¯ã€‚", "example_code": "import requests\n\nr = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))", "running_command": null, "expected_input": "https://httpbin.org/basic-auth/user/pass, ('user', 'pass')", "expected_output": "200"}, {"task_title": "è·å–å“åº”çŠ¶æ€ç ", "task_description": "ä»HTTPå“åº”ä¸­æå–çŠ¶æ€ç ä»¥ç¡®è®¤è¯·æ±‚æ˜¯å¦æˆåŠŸã€‚", "example_code": "r.status_code", "running_command": null, "expected_input": null, "expected_output": "200"}, {"task_title": "è·å–å“åº”å¤´éƒ¨ä¿¡æ¯", "task_description": "ä»HTTPå“åº”ä¸­è·å–ç‰¹å®šçš„å¤´éƒ¨ä¿¡æ¯ï¼Œä¾‹å¦‚å†…å®¹ç±»å‹ã€‚", "example_code": "r.headers['content-type']", "running_command": null, "expected_input": null, "expected_output": "'application/json; charset=utf8'"}, {"task_title": "è·å–å“åº”ç¼–ç ", "task_description": "è·å–HTTPå“åº”çš„ç¼–ç æ ¼å¼ã€‚", "example_code": "r.encoding", "running_command": null, "expected_input": null, "expected_output": "'utf-8'"}, {"task_title": "è·å–å“åº”æ–‡æœ¬", "task_description": "è·å–HTTPå“åº”çš„æ–‡æœ¬å†…å®¹ã€‚", "example_code": "r.text", "running_command": null, "expected_input": null, "expected_output": "'{\"authenticated\": true, ...'"}, {"task_title": "è§£æJSONå“åº”", "task_description": "å°†HTTPå“åº”çš„æ–‡æœ¬å†…å®¹è§£æä¸ºJSONæ ¼å¼çš„Pythonå­—å…¸ã€‚", "example_code": "r.json()", "running_command": null, "expected_input": null, "expected_output": "{'authenticated': True, ...}"}], "setup": {"setup_commands": ["$ python -m pip install requests", "git clone -c fetch.fsck.badTimezone=ignore https://github.com/psf/requests.git"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:43:37.373162"}
{"repo_name": "RVC-Boss/GPT-SoVITS", "stars": 51941, "language": "Python", "tasks": [{"task_title": "éŸ³é¢‘åˆ‡ç‰‡", "task_description": "è¯¥ä»»åŠ¡ç”¨äºå°†åŸå§‹éŸ³é¢‘æ–‡ä»¶æˆ–ç›®å½•ä¸­çš„éŸ³é¢‘åˆ†å‰²æˆå¤šä¸ªå­ç‰‡æ®µï¼ŒåŸºäºéŸ³é‡é˜ˆå€¼ã€æœ€å°é•¿åº¦å’Œé—´éš”ç­‰å‚æ•°è¿›è¡Œåˆ‡å‰²ã€‚", "example_code": null, "running_command": "python audio_slicer.py", "expected_input": "<path_to_original_audio_file_or_directory>, <directory_where_subdivided_audio_clips_will_be_saved>, <volume_threshold>, <minimum_duration_of_each_subclip>, <shortest_time_gap_between_adjacent_subclips>, <step_size_for_computing_volume_curve>", "expected_output": "åˆ‡å‰²åçš„éŸ³é¢‘ç‰‡æ®µå°†ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­"}, {"task_title": "è¯­éŸ³è¯†åˆ«ï¼ˆFunASRï¼‰", "task_description": "è¯¥ä»»åŠ¡ç”¨äºå¯¹è¾“å…¥éŸ³é¢‘è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œè¾“å‡ºè¯†åˆ«ç»“æœã€‚", "example_code": null, "running_command": "python tools/asr/funasr_asr.py", "expected_input": "<input>, <output>", "expected_output": "è¯†åˆ«çš„æ–‡æœ¬ç»“æœ"}, {"task_title": "è¯­éŸ³è¯†åˆ«ï¼ˆFasterWhisperï¼‰", "task_description": "è¯¥ä»»åŠ¡ç”¨äºå¯¹è¾“å…¥éŸ³é¢‘è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œæ”¯æŒæŒ‡å®šè¯­è¨€å’Œç²¾åº¦è®¾ç½®ï¼Œè¾“å‡ºè¯†åˆ«ç»“æœã€‚", "example_code": null, "running_command": "python ./tools/asr/fasterwhisper_asr.py", "expected_input": "<input>, <output>, <language>, <precision>", "expected_output": "è¯†åˆ«çš„æ–‡æœ¬ç»“æœ"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the GPT-SoVITS project from the RVC-Boss repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to provide a consistent and isolated environment for running the GPT-SoVITS application, which is likely focused on voice synthesis or similar tasks. It includes necessary dependencies and configurations to ensure that the application runs smoothly.\n\n### Building Steps\n\n1. **Set Up Docker Environment**: Ensure that Docker is installed and running on your machine. You can verify this by running `docker --version` in your terminal.\n\n2. **Clone the Repository**: Clone the RVC-Boss/GPT-SoVITS repository to your local machine using Git:\n   ```bash\n   git clone https://github.com/RVC-Boss/GPT-SoVITS.git\n   cd GPT-SoVITS\n   ```\n\n3. **Build the Docker Image**:\n   - Use the Docker command to build the image, specifying the Dockerfile located in the cloned repository. The image will be built with the necessary dependencies, including a specific version of CUDA and a base PyTorch environment.\n   - The image will also install Miniconda and additional Python packages required for the application, as specified in the `requirements.txt` and `extra-req.txt` files.\n\n4. **Image Dependencies**: The image relies on:\n   - A CUDA-enabled base image to leverage GPU acceleration.\n   - Miniconda for managing Python environments and dependencies.\n   - Various Python libraries necessary for the GPT-SoVITS application, which are defined in the `requirements.txt` file.\n\n5. **Expose Ports**: The image exposes several ports (9871, 9872, 9873, 9874, 9880) for communication, likely for serving the application or accessing its API.\n\n### Running the Docker Container\n\n1. **Run the Container**: After building the image, you can run it using the following command:\n   ```bash\n   docker run -it --rm -p 9871:9871 -p 9872:9872 -p 9873:9873 -p 9874:9874 -p 9880:9880 <image_name>\n   ```\n   Replace `<image_name>` with the name you assigned to the built image.\n\n2. **Environment Variables**: The container can be customized at runtime using environment variables such as `LITE` and `WORKFLOW`, which can be set to `true` or `false` depending on your needs.\n\n3. **Accessing the Application**: Once the container is running, you can access the application through the exposed ports using a web browser or API client.\n\n### Cleanup\nAfter you are done using the container, it will automatically be removed due to the `--rm` flag, keeping your environment clean.\n\nBy following these steps, you will successfully build and run the Docker image for the GPT-SoVITS project, enabling you to utilize its features in a controlled environment.", "docker-compose.yaml": "To build and run Docker containers using the `docker-compose.yaml` file from the RVC-Boss/GPT-SoVITS repository, follow these steps:\n\n### Purpose of the Docker Images\nThe Docker images defined in the `docker-compose.yaml` file are designed to facilitate the deployment of the GPT-SoVITS models, which are used for voice synthesis and transformation tasks. These images come in different versions optimized for various CUDA environments (CU126 and CU128) and include both standard and lightweight variants. The lightweight versions are tailored for lower resource usage while still providing essential functionalities.\n\n### Dependencies\nThe images rely on NVIDIA GPU support, which means they require the NVIDIA Container Toolkit to be installed on your host machine. Additionally, they utilize shared memory configurations and specific environment variables to optimize performance during model inference.\n\n### Building and Running Steps\n\n1. **Install Docker and Docker Compose**: Ensure that Docker and Docker Compose are installed on your machine. You can download them from the official Docker website.\n\n2. **Set Up NVIDIA Support**: If you haven't already, install the NVIDIA Container Toolkit to enable GPU support in your Docker containers. This is crucial for running the GPT-SoVITS models efficiently.\n\n3. **Clone the Repository**: Clone the RVC-Boss/GPT-SoVITS repository to your local machine using Git:\n   ```bash\n   git clone https://github.com/RVC-Boss/GPT-SoVITS.git\n   cd GPT-SoVITS\n   ```\n\n4. **Configure Environment Variables**: If needed, adjust any environment variables in the `docker-compose.yaml` file to suit your specific requirements. The `is_half=true` setting indicates that the models will use half-precision for improved performance.\n\n5. **Build the Docker Images**: Navigate to the directory containing the `docker-compose.yaml` file and run the following command to build the images:\n   ```bash\n   docker-compose build\n   ```\n   This command will create the necessary Docker images based on the configurations specified in the `docker-compose.yaml` file.\n\n6. **Run the Docker Containers**: After the build process completes, start the containers using:\n   ```bash\n   docker-compose up -d\n   ```\n   The `-d` flag runs the containers in detached mode, allowing them to run in the background.\n\n7. **Access the Services**: The services will be accessible through the specified ports (9871 to 9880). You can interact with the models via these ports as per your application needs.\n\n8. **Monitor and Manage Containers**: Use Docker commands like `docker ps` to monitor running containers and `docker-compose logs` to view logs for troubleshooting.\n\n9. **Stopping the Containers**: When you are done, you can stop the containers with:\n   ```bash\n   docker-compose down\n   ```\n\nBy following these steps, you will successfully build and run the Docker containers for the GPT-SoVITS models, enabling you to leverage their capabilities for voice synthesis tasks.", ".dockerignore": "To build and run a Docker image for the RVC-Boss/GPT-SoVITS repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to encapsulate the environment needed for running the GPT-SoVITS model, which is used for voice synthesis and transformation tasks. This image includes all necessary dependencies, libraries, and tools required to operate the model effectively.\n\n### Dependencies\nThe image typically includes:\n- Python and relevant libraries for machine learning and audio processing.\n- Tools for handling audio files, such as FFmpeg.\n- Pre-trained models and weights necessary for the voice synthesis tasks.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the RVC-Boss/GPT-SoVITS repository to your local machine.\n\n2. **Create a `.dockerignore` File**: Ensure that the `.dockerignore` file is present in the root of your repository. This file is crucial as it specifies which files and directories should be excluded from the Docker build context, helping to keep the image lightweight and free from unnecessary files.\n\n3. **Prepare the Dockerfile**: While the exact content of the Dockerfile is not disclosed, ensure it is set up to:\n   - Use an appropriate base image (e.g., a Python image).\n   - Install required system packages and Python dependencies as defined in your project.\n   - Copy only the necessary files from your project into the image, excluding those specified in the `.dockerignore`.\n\n4. **Build the Docker Image**: Use the Docker command to build the image. This process will read the Dockerfile, install dependencies, and package your application into a single image.\n\n5. **Run the Docker Container**: Once the image is built, you can run a container from it. Ensure you pass any necessary environment variables or volume mounts to access data or configuration files required by the application.\n\n### Running the Image\nAfter building the image, you can run it using Docker commands. Make sure to specify any ports that need to be exposed and any configurations that the application requires to function correctly.\n\n### Conclusion\nBy following these steps, you will successfully build and run the Docker image for the RVC-Boss/GPT-SoVITS project, allowing you to leverage the capabilities of the GPT-SoVITS model in a contained environment."}}, "timestamp": "2025-10-30T23:44:35.154483"}
{"repo_name": "microsoft/autogen", "stars": 51254, "language": "Python", "tasks": [{"task_title": "åŸºæœ¬åŠ©æ‰‹åŠŸèƒ½", "task_description": "åˆ›å»ºä¸€ä¸ªåŠ©æ‰‹ä»£ç†ï¼Œä½¿ç”¨OpenAIçš„GPT-4.1æ¨¡å‹ï¼Œæ‰§è¡Œç®€å•çš„ä»»åŠ¡ä»¥è¿”å›æ–‡æœ¬ã€‚", "example_code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    agent = AssistantAgent(\"assistant\", model_client=model_client)\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n    await model_client.close()\n\nasyncio.run(main())", "running_command": null, "expected_input": "Say 'Hello World!'", "expected_output": "Hello World!"}, {"task_title": "æ•°å­¦å’ŒåŒ–å­¦ä¸“å®¶åŠ©æ‰‹", "task_description": "åˆ›å»ºå¤šä¸ªåŠ©æ‰‹ä»£ç†ï¼ŒåŒ…æ‹¬æ•°å­¦å’ŒåŒ–å­¦ä¸“å®¶ï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¸“å®¶å·¥å…·å¤„ç†ç‰¹å®šä»»åŠ¡ã€‚", "example_code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.tools import AgentTool\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n\n    math_agent = AssistantAgent(\n        \"math_expert\",\n        model_client=model_client,\n        system_message=\"You are a math expert.\",\n        description=\"A math expert assistant.\",\n        model_client_stream=True,\n    )\n    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)\n\n    chemistry_agent = AssistantAgent(\n        \"chemistry_expert\",\n        model_client=model_client,\n        system_message=\"You are a chemistry expert.\",\n        description=\"A chemistry expert assistant.\",\n        model_client_stream=True,\n    )\n    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)\n\n    agent = AssistantAgent(\n        \"assistant\",\n        system_message=\"You are a general assistant. Use expert tools when needed.\",\n        model_client=model_client,\n        model_client_stream=True,\n        tools=[math_agent_tool, chemistry_agent_tool],\n        max_tool_iterations=10,\n    )\n    await Console(agent.run_stream(task=\"What is the integral of x^2?\"))\n    await Console(agent.run_stream(task=\"What is the molecular weight of water?\"))\n\nasyncio.run(main())", "running_command": null, "expected_input": "What is the integral of x^2?\nWhat is the molecular weight of water?", "expected_output": "The integral of x^2 is (1/3)x^3 + C.\nThe molecular weight of water is approximately 18.015 g/mol."}, {"task_title": "è¿è¡ŒAutoGen Studio", "task_description": "å¯åŠ¨AutoGen Studioä»¥æä¾›ç”¨æˆ·ç•Œé¢ã€‚", "example_code": null, "running_command": "autogenstudio ui --port 8080 --appdir ./my-app", "expected_input": null, "expected_output": "AutoGen Studio is running on http://localhost:8080"}], "setup": {"setup_commands": ["# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"", "# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"", "# First run `npm install -g @playwright/mcp@latest` to install the MCP server.\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    server_params = StdioServerParams(\n        command=\"npx\",\n        args=[\n            \"@playwright/mcp@latest\",\n            \"--headless\",\n        ],\n    )\n    async with McpWorkbench(server_params) as mcp:\n        agent = AssistantAgent(\n            \"web_browsing_assistant\",\n            model_client=model_client,\n            workbench=mcp, # For multiple MCP servers, put them in a list.\n            model_client_stream=True,\n            max_tool_iterations=10,\n        )\n        await Console(agent.run_stream(task=\"Find out how many contributors for the microsoft/autogen repository\"))\n\n\nasyncio.run(main())"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:45:00.365231"}
{"repo_name": "pathwaycom/pathway", "stars": 48953, "language": "Python", "tasks": [{"task_title": "è¯»å–CSVæ•°æ®", "task_description": "ä½¿ç”¨Pathwayåº“ä»CSVæ–‡ä»¶è¯»å–æ•°æ®ï¼Œå¹¶å®šä¹‰æ•°æ®çš„æ¨¡å¼ã€‚", "example_code": "import pathway as pw\n\nclass InputSchema(pw.Schema):\n  value: int\n\ninput_table = pw.io.csv.read(\n  \"./input/\",\n  schema=InputSchema\n)", "running_command": null, "expected_input": "./input/", "expected_output": "è¯»å–çš„è¡¨æ ¼æ•°æ®"}, {"task_title": "è¿‡æ»¤æ•°æ®", "task_description": "å¯¹è¯»å–çš„æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼Œä¿ç•™valueå­—æ®µå¤§äºç­‰äº0çš„è¡Œã€‚", "example_code": "filtered_table = input_table.filter(input_table.value>=0)", "running_command": null, "expected_input": "input_table", "expected_output": "è¿‡æ»¤åçš„è¡¨æ ¼æ•°æ®"}, {"task_title": "èšåˆæ•°æ®", "task_description": "å¯¹è¿‡æ»¤åçš„æ•°æ®è¿›è¡Œèšåˆï¼Œè®¡ç®—valueå­—æ®µçš„æ€»å’Œã€‚", "example_code": "result_table = filtered_table.reduce(\n  sum_value = pw.reducers.sum(filtered_table.value)\n)", "running_command": null, "expected_input": "filtered_table", "expected_output": "èšåˆåçš„ç»“æœè¡¨ï¼ŒåŒ…å«sum_valueå­—æ®µ"}, {"task_title": "å†™å…¥JSONLæ–‡ä»¶", "task_description": "å°†èšåˆç»“æœå†™å…¥JSONLæ ¼å¼çš„æ–‡ä»¶ã€‚", "example_code": "pw.io.jsonlines.write(result_table, \"output.jsonl\")", "running_command": null, "expected_input": "result_table", "expected_output": "output.jsonlæ–‡ä»¶"}, {"task_title": "è¿è¡Œè®¡ç®—", "task_description": "æ‰§è¡ŒPathwayè®¡ç®—ä»¥å¤„ç†æ•°æ®æµã€‚", "example_code": "pw.run()", "running_command": null, "expected_input": null, "expected_output": "è®¡ç®—æ‰§è¡Œçš„ç»“æœ"}], "setup": {"setup_commands": ["pip install -U pathway"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image using the `.dockerignore` file from the repository `pathwaycom/pathway`, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to encapsulate a development environment for a specific application, likely a Rust-based project given the presence of the `target/debug/` directory. The image will contain all necessary dependencies and tools required to compile and run the application efficiently.\n\n### Building Steps\n\n1. **Set Up Your Environment**: Ensure you have Docker installed on your machine. You can verify this by running `docker --version` in your terminal.\n\n2. **Clone the Repository**: Clone the repository to your local machine using Git:\n   ```\n   git clone https://github.com/pathwaycom/pathway.git\n   cd pathway\n   ```\n\n3. **Review the `.dockerignore` File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. In this case, it ignores everything in the `target/debug/` directory, except for any `.whl` files within that directory. This helps to reduce the image size by excluding unnecessary build artifacts.\n\n4. **Build the Docker Image**: Use the Docker build command to create the image. This process will read the Dockerfile in the repository and use the context defined by the `.dockerignore` file to include only the necessary files. The build process will install dependencies specified in the Dockerfile, which may include Rust toolchains, libraries, and any other required packages.\n\n5. **Tag the Image**: Optionally, tag the image with a meaningful name for easier identification. This can be done during the build process.\n\n6. **Run the Docker Container**: Once the image is built, you can run a container from the image. This container will have all the dependencies installed and will allow you to execute the application in an isolated environment.\n\n### Dependencies\nThe built image will likely include:\n- The Rust programming language and its toolchain for compiling the application.\n- Any additional libraries or tools specified in the Dockerfile that are necessary for the application to run correctly.\n- The `.whl` files from the `target/debug/` directory, which may include Python packages if the application has Python components.\n\n### Conclusion\nBy following these steps, you will successfully build and run a Docker image for the application in the `pathwaycom/pathway` repository, ensuring that you have a consistent and isolated environment for development and testing."}}, "timestamp": "2025-10-30T23:45:40.765530"}
{"repo_name": "karpathy/nanoGPT", "stars": 48545, "language": "Python", "tasks": [{"task_title": "Prepare Shakespeare Character Dataset", "task_description": "å‡†å¤‡èå£«æ¯”äºšå­—ç¬¦æ•°æ®é›†ä»¥ä¾›è®­ç»ƒä½¿ç”¨ã€‚", "example_code": null, "running_command": "python data/shakespeare_char/prepare.py", "expected_input": null, "expected_output": "Dataset prepared successfully."}, {"task_title": "Train on Shakespeare Character Dataset", "task_description": "ä½¿ç”¨èå£«æ¯”äºšå­—ç¬¦æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚", "example_code": null, "running_command": "python train.py config/train_shakespeare_char.py", "expected_input": null, "expected_output": "Training completed."}, {"task_title": "Sample from Trained Model", "task_description": "ä»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­ç”Ÿæˆæ ·æœ¬ã€‚", "example_code": null, "running_command": "python sample.py --out_dir=out-shakespeare-char", "expected_input": null, "expected_output": "Generated samples saved to out-shakespeare-char."}, {"task_title": "Train with Custom Parameters", "task_description": "ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°è¿›è¡Œèå£«æ¯”äºšå­—ç¬¦æ•°æ®é›†çš„è®­ç»ƒã€‚", "example_code": null, "running_command": "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0", "expected_input": null, "expected_output": "Training completed with custom parameters."}, {"task_title": "Sample from Model on CPU", "task_description": "åœ¨CPUä¸Šä»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­ç”Ÿæˆæ ·æœ¬ã€‚", "example_code": null, "running_command": "python sample.py --out_dir=out-shakespeare-char --device=cpu", "expected_input": null, "expected_output": "Generated samples saved to out-shakespeare-char."}, {"task_title": "Prepare OpenWebText Dataset", "task_description": "å‡†å¤‡OpenWebTextæ•°æ®é›†ä»¥ä¾›è®­ç»ƒä½¿ç”¨ã€‚", "example_code": null, "running_command": "python data/openwebtext/prepare.py", "expected_input": null, "expected_output": "Dataset prepared successfully."}, {"task_title": "Distributed Training with Torchrun", "task_description": "åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ä½¿ç”¨Torchrunè¿›è¡Œè®­ç»ƒã€‚", "example_code": null, "running_command": "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py", "expected_input": null, "expected_output": "Distributed training started."}, {"task_title": "Distributed Training on Multiple Nodes", "task_description": "åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚", "example_code": null, "running_command": "torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py", "expected_input": null, "expected_output": "Distributed training started on master node."}, {"task_title": "Evaluate GPT-2 Models", "task_description": "è¯„ä¼°ä¸åŒå¤§å°çš„GPT-2æ¨¡å‹ã€‚", "example_code": null, "running_command": "$ python train.py config/eval_gpt2.py\n$ python train.py config/eval_gpt2_medium.py\n$ python train.py config/eval_gpt2_large.py\n$ python train.py config/eval_gpt2_xl.py", "expected_input": null, "expected_output": "Evaluation completed for all specified models."}, {"task_title": "Fine-tune on Shakespeare Dataset", "task_description": "å¯¹èå£«æ¯”äºšæ•°æ®é›†è¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚", "example_code": null, "running_command": "python train.py config/finetune_shakespeare.py", "expected_input": null, "expected_output": "Fine-tuning completed."}, {"task_title": "Sample from Fine-tuned Model", "task_description": "ä»å¾®è°ƒåçš„æ¨¡å‹ä¸­ç”Ÿæˆæ ·æœ¬ã€‚", "example_code": null, "running_command": "python sample.py --init_from=gpt2-xl --start=\"What is the answer to life, the universe, and everything?\" --num_samples=5 --max_new_tokens=100", "expected_input": "What is the answer to life, the universe, and everything?", "expected_output": "Generated text based on the prompt."}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:46:14.163158"}
{"repo_name": "ultralytics/ultralytics", "stars": 48082, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨é¢„è®­ç»ƒçš„YOLOæ¨¡å‹è¿›è¡Œé¢„æµ‹", "task_description": "ä½¿ç”¨é¢„è®­ç»ƒçš„YOLOæ¨¡å‹ï¼ˆä¾‹å¦‚YOLO11nï¼‰å¯¹å›¾åƒè¿›è¡Œé¢„æµ‹ã€‚", "example_code": null, "running_command": "yolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'", "expected_input": "https://ultralytics.com/images/bus.jpg", "expected_output": "é¢„æµ‹ç»“æœï¼ŒåŒ…æ‹¬æ£€æµ‹åˆ°çš„å¯¹è±¡åŠå…¶ä½ç½®"}, {"task_title": "åŠ è½½é¢„è®­ç»ƒçš„YOLOæ¨¡å‹", "task_description": "åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„YOLO11næ¨¡å‹ä»¥è¿›è¡Œåç»­æ“ä½œã€‚", "example_code": "from ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")", "running_command": null, "expected_input": null, "expected_output": "æ¨¡å‹å¯¹è±¡"}, {"task_title": "åœ¨COCO8æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹", "task_description": "åœ¨COCO8æ•°æ®é›†ä¸Šè®­ç»ƒYOLOæ¨¡å‹ï¼ŒæŒ‡å®šè®­ç»ƒçš„è½®æ•°å’Œå›¾åƒå¤§å°ã€‚", "example_code": "train_results = model.train(\n    data=\"coco8.yaml\",\n    epochs=100,\n    imgsz=640,\n    device=\"cpu\",\n)", "running_command": null, "expected_input": {"data": "coco8.yaml", "epochs": 100, "imgsz": 640, "device": "cpu"}, "expected_output": "è®­ç»ƒç»“æœï¼ŒåŒ…æ‹¬æŸå¤±å’Œç²¾åº¦ç­‰æŒ‡æ ‡"}, {"task_title": "è¯„ä¼°æ¨¡å‹æ€§èƒ½", "task_description": "åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½å¹¶è·å–æŒ‡æ ‡ã€‚", "example_code": "metrics = model.val()", "running_command": null, "expected_input": null, "expected_output": "æ¨¡å‹è¯„ä¼°æŒ‡æ ‡"}, {"task_title": "å¯¹å›¾åƒè¿›è¡Œå¯¹è±¡æ£€æµ‹", "task_description": "å¯¹æŒ‡å®šè·¯å¾„çš„å›¾åƒè¿›è¡Œå¯¹è±¡æ£€æµ‹å¹¶æ˜¾ç¤ºç»“æœã€‚", "example_code": "results = model(\"path/to/image.jpg\")\nresults[0].show()", "running_command": null, "expected_input": "path/to/image.jpg", "expected_output": "æ£€æµ‹ç»“æœçš„å¯è§†åŒ–å›¾åƒ"}, {"task_title": "å°†æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼", "task_description": "å°†è®­ç»ƒå¥½çš„YOLOæ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ä»¥ä¾¿äºéƒ¨ç½²ã€‚", "example_code": "path = model.export(format=\"onnx\")", "running_command": null, "expected_input": null, "expected_output": "å¯¼å‡ºæ¨¡å‹çš„è·¯å¾„"}], "setup": {"setup_commands": ["pip install ultralytics"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image using the `.dockerignore` file from the `ultralytics/ultralytics` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to facilitate the development and deployment of machine learning models using the Ultralytics framework. It encapsulates the necessary environment and dependencies to ensure consistent performance across different systems.\n\n### Building Steps\n1. **Prepare Your Environment**: Ensure that Docker is installed and running on your machine. You should also have access to the `ultralytics/ultralytics` repository.\n\n2. **Clone the Repository**: Download the repository to your local machine using Git. This will give you access to the necessary files, including the `.dockerignore`.\n\n3. **Understand the .dockerignore File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This is crucial for optimizing the build process and reducing the image size. It typically excludes:\n   - Python cache files and compiled bytecode.\n   - Development environment files (like virtual environments and IDE configurations).\n   - Project-specific logs and temporary files.\n   - Node.js dependencies, if applicable.\n\n4. **Build the Docker Image**: Navigate to the root directory of the cloned repository. Use the Docker build command to create the image. The build process will read the Dockerfile (not disclosed here) and the `.dockerignore` file to determine what to include in the image. The resulting image will contain:\n   - The Ultralytics framework and its dependencies (like Python libraries for machine learning).\n   - Any additional tools or libraries specified in the Dockerfile that are essential for running the Ultralytics models.\n\n5. **Run the Docker Container**: Once the image is built, you can run a container from the image. This container will have all the necessary dependencies and configurations to execute Ultralytics tasks, such as training models or making predictions.\n\n### Dependencies\nThe image will include essential dependencies such as:\n- Python and relevant libraries for machine learning (e.g., NumPy, PyTorch).\n- Any additional tools required for the specific functionalities of the Ultralytics framework.\n\nBy following these steps, you will successfully build and run a Docker image tailored for the Ultralytics framework, ensuring a smooth development and deployment experience."}}, "timestamp": "2025-10-30T23:46:40.570490"}
{"repo_name": "opendatalab/MinerU", "stars": 47765, "language": "Python", "tasks": [{"task_title": "æ•°æ®å¤„ç†", "task_description": "ä½¿ç”¨MinerUåº“å¤„ç†è¾“å…¥è·¯å¾„ä¸‹çš„æ•°æ®ï¼Œå¹¶å°†ç»“æœè¾“å‡ºåˆ°æŒ‡å®šçš„è¾“å‡ºè·¯å¾„ã€‚", "example_code": null, "running_command": "mineru -p <input_path> -o <output_path>", "expected_input": "<input_path>", "expected_output": "å¤„ç†åçš„æ•°æ®æ–‡ä»¶"}], "setup": {"setup_commands": ["pip install --upgrade pip\npip install uv\nuv pip install -U \"mineru[core]\"", "git clone https://github.com/opendatalab/MinerU.git\ncd MinerU\nuv pip install -e .[core]"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:46:49.779325"}
{"repo_name": "unslothai/unsloth", "stars": 47668, "language": "Python", "tasks": [{"task_title": "åŠ è½½æ•°æ®é›†", "task_description": "ä»æŒ‡å®šçš„URLåŠ è½½LAIONæ•°æ®é›†ï¼Œä½¿ç”¨datasetsåº“çš„load_datasetå‡½æ•°ã€‚", "example_code": "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")", "running_command": null, "expected_input": "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl", "expected_output": "åŠ è½½çš„è®­ç»ƒæ•°æ®é›†å¯¹è±¡"}, {"task_title": "åŠ è½½é¢„è®­ç»ƒæ¨¡å‹", "task_description": "ä½¿ç”¨FastModelç±»ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ¨¡å‹å’Œtokenizerï¼Œå¹¶è®¾ç½®ç›¸å…³å‚æ•°ã€‚", "example_code": "model, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    load_in_8bit = False,\n    load_in_16bit = False,\n    full_finetuning = False,\n)", "running_command": null, "expected_input": "unsloth/gpt-oss-20b", "expected_output": "åŠ è½½çš„æ¨¡å‹å’Œtokenizerå¯¹è±¡"}, {"task_title": "æ¨¡å‹åŠ é€Ÿä¸LoRAæƒé‡æ·»åŠ ", "task_description": "å¯¹æ¨¡å‹è¿›è¡Œè¡¥ä¸å¤„ç†ï¼Œæ·»åŠ å¿«é€ŸLoRAæƒé‡ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚", "example_code": "model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n    use_rslora = False,\n    loftq_config = None,\n)", "running_command": null, "expected_input": "æ¨¡å‹å¯¹è±¡", "expected_output": "ä¼˜åŒ–åçš„æ¨¡å‹å¯¹è±¡"}, {"task_title": "è®­ç»ƒæ¨¡å‹", "task_description": "ä½¿ç”¨SFTTrainerç±»å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ŒæŒ‡å®šè®­ç»ƒæ•°æ®é›†å’Œç›¸å…³å‚æ•°ã€‚", "example_code": "trainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    args = SFTConfig(\n        max_seq_length = max_seq_length,\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 10,\n        max_steps = 60,\n        logging_steps = 1,\n        output_dir = \"outputs\",\n        optim = \"adamw_8bit\",\n        seed = 3407,\n    ),\n)\ntrainer.train()", "running_command": null, "expected_input": "è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹å¯¹è±¡", "expected_output": "è®­ç»ƒè¿‡ç¨‹çš„è¾“å‡ºæ—¥å¿—"}], "setup": {"setup_commands": ["pip install unsloth", "pip install unsloth", "python -m venv unsloth\nsource unsloth/bin/activate\npip install unsloth", "  pip install ninja\n  pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers", "conda create --name unsloth_env \\\n    python=3.11 \\\n    pytorch-cuda=12.1 \\\n    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n    -y\nconda activate unsloth_env\n\npip install unsloth", "  mkdir -p ~/miniconda3\n  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n  rm -rf ~/miniconda3/miniconda.sh\n  ~/miniconda3/bin/conda init bash\n  ~/miniconda3/bin/conda init zsh", "pip install --upgrade pip\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"", "pip install --upgrade pip\npip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"", "pip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"", "wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -", "try: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nimport re\nv = V(re.match(r\"[0-9\\.]{3,}\", torch.__version__).group(0))\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nUSE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI\nif cuda not in (\"11.8\", \"12.1\", \"12.4\", \"12.6\", \"12.8\"): raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v  < V('2.5.1'): x = 'cu{}{}-torch250'\nelif v <= V('2.5.1'): x = 'cu{}{}-torch251'\nelif v  < V('2.7.0'): x = 'cu{}{}-torch260'\nelif v  < V('2.7.9'): x = 'cu{}{}-torch270'\nelif v  < V('2.8.0'): x = 'cu{}{}-torch271'\nelif v  < V('2.8.9'): x = 'cu{}{}-torch280'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nif v > V('2.6.9') and cuda not in (\"11.8\", \"12.6\", \"12.8\"): raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')"], "docker_commands": ["docker run -d -e JUPYTER_PASSWORD=\"mypassword\" \\\n  -p 8888:8888 -p 2222:22 \\\n  -v $(pwd)/work:/workspace/work \\\n  --gpus all \\\n  unsloth/unsloth"], "has_docker_files": false, "docker_setup_descriptions": {"README_commands": "# Guide to Build and Run Docker for unslothai/unsloth\n\n1. **Install Docker**: Ensure Docker is installed on your machine. Follow the official [Docker installation guide](https://docs.docker.com/get-docker/) for your operating system.\n\n2. **Install NVIDIA Container Toolkit** (if using GPUs): Follow the instructions on the [NVIDIA website](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) to set up the NVIDIA Container Toolkit.\n\n3. **Clone the Repository**: Open your terminal and clone the repository:\n   ```bash\n   git clone https://github.com/unslothai/unsloth.git\n   cd unsloth\n   ```\n\n4. **Create a Work Directory**: Ensure you have a local directory for your work:\n   ```bash\n   mkdir work\n   ```\n\n5. **Run the Docker Container**: Execute the following command to start the Docker container:\n   ```bash\n   docker run -d -e JUPYTER_PASSWORD=\"mypassword\" \\\n     -p 8888:8888 -p 2222:22 \\\n     -v $(pwd)/work:/workspace/work \\\n     --gpus all \\\n     unsloth/unsloth\n   ```\n\n6. **Access Jupyter Notebook**: Open your web browser and go to `http://localhost:8888`. Enter the password you set (`mypassword`) to access Jupyter Notebook.\n\n7. **Connect via SSH** (optional): If you need to connect via SSH, use:\n   ```bash\n   ssh -p 2222 user@localhost\n   ```\n   Replace `user` with the appropriate username for the container.\n\n8. **Stop the Container**: To stop the running container, find its ID with:\n   ```bash\n   docker ps\n   ```\n   Then stop it using:\n   ```bash\n   docker stop <container_id>\n   ```\n\n9. **Remove the Container** (optional): If you want to remove the container after stopping it:\n   ```bash\n   docker rm <container_id>\n   ```\n\nNow you are set up to use the unsloth Docker container!"}}, "timestamp": "2025-10-30T23:47:20.102554"}
{"repo_name": "run-llama/llama_index", "stars": 44984, "language": "Python", "tasks": [{"task_title": "åŠ è½½æ•°æ®å¹¶åˆ›å»ºå‘é‡å­˜å‚¨ç´¢å¼•", "task_description": "ä½¿ç”¨SimpleDirectoryReaderåŠ è½½æ–‡æ¡£æ•°æ®ï¼Œå¹¶åˆ›å»ºVectorStoreIndexä»¥ä¾¿åç»­æŸ¥è¯¢ã€‚", "example_code": "import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)", "running_command": null, "expected_input": "YOUR_DATA_DIRECTORYä¸­çš„æ•°æ®", "expected_output": "VectorStoreIndexå¯¹è±¡"}, {"task_title": "è®¾ç½®LLMå’ŒåµŒå…¥æ¨¡å‹", "task_description": "é…ç½®ä½¿ç”¨çš„LLMå’ŒåµŒå…¥æ¨¡å‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åç§°ã€æ¸©åº¦å’Œtokenizerã€‚", "example_code": "import os\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_API_TOKEN\"\n\nfrom llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.replicate import Replicate\nfrom transformers import AutoTokenizer\n\nllama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\nSettings.llm = Replicate(\n    model=llama2_7b_chat,\n    temperature=0.01,\n    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n)\n\nSettings.tokenizer = AutoTokenizer.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\"\n)\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)", "running_command": null, "expected_input": "æ— ", "expected_output": "è®¾ç½®å®Œæˆçš„LLMå’ŒåµŒå…¥æ¨¡å‹"}, {"task_title": "æŸ¥è¯¢ç´¢å¼•", "task_description": "ä½¿ç”¨åˆ›å»ºçš„ç´¢å¼•è¿›è¡ŒæŸ¥è¯¢ï¼Œä»¥è·å–ç›¸å…³ä¿¡æ¯ã€‚", "example_code": "query_engine = index.as_query_engine()\nquery_engine.query(\"YOUR_QUESTION\")", "running_command": null, "expected_input": "YOUR_QUESTION", "expected_output": "æŸ¥è¯¢ç»“æœ"}, {"task_title": "æŒä¹…åŒ–ç´¢å¼•", "task_description": "å°†å½“å‰çš„ç´¢å¼•çŠ¶æ€æŒä¹…åŒ–åˆ°å­˜å‚¨ä¸­ï¼Œä»¥ä¾¿åç»­åŠ è½½ã€‚", "example_code": "index.storage_context.persist()", "running_command": null, "expected_input": "æ— ", "expected_output": "ç´¢å¼•æŒä¹…åŒ–æˆåŠŸ"}, {"task_title": "ä»å­˜å‚¨åŠ è½½ç´¢å¼•", "task_description": "é‡å»ºå­˜å‚¨ä¸Šä¸‹æ–‡å¹¶ä»ä¸­åŠ è½½ç´¢å¼•ï¼Œä»¥ä¾¿æ¢å¤ä¹‹å‰çš„çŠ¶æ€ã€‚", "example_code": "from llama_index.core import StorageContext, load_index_from_storage\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)", "running_command": null, "expected_input": "æ— ", "expected_output": "åŠ è½½çš„VectorStoreIndexå¯¹è±¡"}], "setup": {"setup_commands": ["# custom selection of integrations to work with core\npip install llama-index-core\npip install llama-index-llms-openai\npip install llama-index-llms-replicate\npip install llama-index-embeddings-huggingface", "cd <desired-package-folder>\npip install poetry\npoetry install --with dev"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:47:59.748647"}
{"repo_name": "coqui-ai/TTS", "stars": 43228, "language": "Python", "tasks": [{"task_title": "List available TTS models", "task_description": "åˆ—å‡ºå¯ç”¨çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚", "example_code": "print(TTS().list_models())", "running_command": null, "expected_input": null, "expected_output": "['model1', 'model2', ...]"}, {"task_title": "Run TTS with multilingual voice cloning", "task_description": "ä½¿ç”¨å¤šè¯­è¨€è¯­éŸ³å…‹éš†æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼Œå¹¶è¾“å‡ºä¸ºéŸ³é¢‘æ³¢å½¢ã€‚", "example_code": "wav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")", "running_command": null, "expected_input": {"text": "Hello world!", "speaker_wav": "my/cloning/audio.wav", "language": "en"}, "expected_output": "éŸ³é¢‘æ³¢å½¢æ•°æ®"}, {"task_title": "Run TTS and save output to file", "task_description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶ä¸­ã€‚", "example_code": "tts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")", "running_command": null, "expected_input": {"text": "Hello world!", "speaker_wav": "my/cloning/audio.wav", "language": "en", "file_path": "output.wav"}, "expected_output": "éŸ³é¢‘æ–‡ä»¶ output.wav"}, {"task_title": "Voice cloning in multiple languages", "task_description": "ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œè¯­éŸ³å…‹éš†ï¼Œå°†ä¸åŒè¯­è¨€çš„æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚", "example_code": "tts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")", "running_command": null, "expected_input": {"text": "C'est le clonage de la voix.", "speaker_wav": "my/cloning/audio.wav", "language": "fr-fr", "file_path": "output.wav"}, "expected_output": "éŸ³é¢‘æ–‡ä»¶ output.wav"}, {"task_title": "Voice conversion", "task_description": "å°†æºéŸ³é¢‘çš„å£°éŸ³è½¬æ¢ä¸ºç›®æ ‡éŸ³é¢‘çš„å£°éŸ³ï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚", "example_code": "tts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")", "running_command": null, "expected_input": {"source_wav": "my/source.wav", "target_wav": "my/target.wav", "file_path": "output.wav"}, "expected_output": "éŸ³é¢‘æ–‡ä»¶ output.wav"}, {"task_title": "Text-to-speech with voice conversion", "task_description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼ŒåŒæ—¶è¿›è¡Œå£°éŸ³è½¬æ¢ï¼Œå¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚", "example_code": "tts.tts_with_vc_to_file(\"Wie sage ich auf Italienisch, dass ich dich liebe?\", speaker_wav=\"target/speaker.wav\", file_path=\"output.wav\")", "running_command": null, "expected_input": {"text": "Wie sage ich auf Italienisch, dass ich dich liebe?", "speaker_wav": "target/speaker.wav", "file_path": "output.wav"}, "expected_output": "éŸ³é¢‘æ–‡ä»¶ output.wav"}], "setup": {"setup_commands": ["pip install TTS", "git clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras"], "docker_commands": ["docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run a Docker image using the \"Dockerfile\" from the coqui-ai/TTS repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to provide an isolated environment for running the Coqui TTS (Text-to-Speech) system. It includes all necessary dependencies to ensure that the TTS models can be executed efficiently, leveraging GPU capabilities with CUDA support.\n\n### Building the Image\n1. **Base Image**: The build process starts with a base image that includes NVIDIA's CUDA toolkit, specifically tailored for Ubuntu 22.04. This ensures that the image can utilize GPU acceleration for deep learning tasks.\n\n2. **System Dependencies**: The image installs essential system packages, including compilers (gcc, g++), Python 3, and various development libraries. These are crucial for building and running the TTS application and its dependencies.\n\n3. **Python Dependencies**: The image installs Python packages required for the TTS system, including PyTorch and Torchaudio, which are fundamental for handling audio processing and model inference. The installation is optimized for CUDA 11.8 to ensure compatibility with the base image.\n\n4. **Repository Setup**: The contents of the TTS repository are copied into the image, setting the working directory to facilitate access to the application files.\n\n5. **Installation**: The final step involves running a make command to install the TTS application, ensuring that all components are correctly set up within the Docker environment.\n\n### Running the Image\nOnce the image is built, you can run it using Docker. The entry point is set to the TTS command-line interface, allowing you to interact with the application directly. \n\n- To start a bash shell within the container for interactive use, you can override the entry point.\n- You can list available TTS models by executing a specific Python script within the container.\n- To start the TTS server with a specified model, you can run the server script with the desired model name.\n\n### Summary\nThis Docker image encapsulates the Coqui TTS system, providing a ready-to-use environment with all necessary dependencies for text-to-speech tasks. By following the outlined steps, you can build and run the image effectively, enabling you to leverage advanced TTS capabilities in your applications.", ".dockerignore": "To build and run a Docker image for the Coqui AI Text-to-Speech (TTS) project, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to provide a consistent environment for running the Coqui AI TTS server. It encapsulates all necessary dependencies, including Python libraries and system packages required for text-to-speech synthesis. This allows developers and users to easily deploy the TTS service without worrying about local environment configurations.\n\n### Building the Image\n1. **Prepare Your Environment**: Ensure you have Docker installed on your machine. You should also have access to the Coqui AI TTS repository.\n\n2. **Create a .dockerignore File**: The `.dockerignore` file is crucial as it prevents unnecessary files and directories from being included in the Docker image. This includes version control files, build artifacts, test outputs, and Python cache files. By excluding these, you reduce the image size and improve build performance.\n\n3. **Build the Docker Image**: Use the Docker build command to create the image. This process will read the Dockerfile and the .dockerignore file to determine what to include in the image. The resulting image will contain all the necessary dependencies for running the TTS server.\n\n### Running the Image\n1. **Start the Docker Container**: Once the image is built, you can run it using the Docker run command. This command will start a new container from the image, mapping port 5002 on your host to port 5002 in the container, which is where the TTS server will listen for requests.\n\n2. **Access the Container**: You can enter the container's shell using the specified entry point. This allows you to interact with the environment directly.\n\n3. **List Available Models**: Inside the container, you can execute a command to list all available TTS models. This is useful for understanding what models you can use for text-to-speech synthesis.\n\n4. **Start the TTS Server**: Finally, you can start the TTS server by specifying a model name. This will initiate the service, allowing you to send text inputs for speech synthesis.\n\nBy following these steps, you will have a fully functional TTS server running in a Docker container, ready to process text and generate speech outputs."}}, "timestamp": "2025-10-30T23:48:35.710357"}
{"repo_name": "docling-project/docling", "stars": 42697, "language": "Python", "tasks": [{"task_title": "Convert Document from URL", "task_description": "This task converts a document from a specified URL to a different format using the DocumentConverter class.", "example_code": "from docling.document_converter import DocumentConverter\n\nsource = \"https://arxiv.org/pdf/2408.09869\"  # document per local path or URL\nconverter = DocumentConverter()\nresult = converter.convert(source)\nprint(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\"", "running_command": null, "expected_input": "https://arxiv.org/pdf/2408.09869", "expected_output": "## Docling Technical Report[...]"}, {"task_title": "Convert Document using CLI", "task_description": "This task converts a document from a specified URL using the docling command line interface.", "example_code": null, "running_command": "docling https://arxiv.org/pdf/2206.01062", "expected_input": "https://arxiv.org/pdf/2206.01062", "expected_output": "Conversion result output"}, {"task_title": "Convert Document with Specific Pipeline and Model", "task_description": "This task converts a document using a specific pipeline and VLM model specified in the command line.", "example_code": null, "running_command": "docling --pipeline vlm --vlm-model granite_docling https://arxiv.org/pdf/2206.01062", "expected_input": "https://arxiv.org/pdf/2206.01062", "expected_output": "Conversion result output with specified pipeline"}], "setup": {"setup_commands": ["pip install docling"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the `docling-project/docling` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to provide a lightweight environment for running the Docling tool, which is used for managing and processing documentation. This image is based on Python 3.11 and includes essential libraries and dependencies required for running Docling and its associated models.\n\n### Building the Image\n1. **Base Image**: The build process starts with a slim version of Python 3.11, ensuring a minimal footprint while providing the necessary Python environment.\n2. **Environment Configuration**: An environment variable is set to configure SSH for Git operations, allowing for seamless access to repositories without strict host key checking.\n3. **Dependency Installation**: The image installs several system dependencies, including libraries for graphics processing and essential tools like `curl`, `wget`, and `git`. This ensures that the environment can handle various tasks required by Docling.\n4. **Python Package Installation**: The Docling package is installed via pip, specifically configured to use CPU support for PyTorch. This is suitable for environments where GPU resources are not available.\n5. **Model Preparation**: The image includes a step to download necessary model weights using Docling tools, ensuring that the required resources are available for processing.\n6. **Thread Management**: An environment variable is set to limit the number of threads used by the application, preventing potential performance issues in containerized environments.\n\n### Built Image\nThe resulting Docker image is optimized for running Docling with CPU support, equipped with all necessary dependencies and pre-downloaded models. It is ready for immediate use in processing documentation tasks.\n\n### Running the Image\nTo run the Docker container:\n- Use the command to start the container, setting the `DOCLING_ARTIFACTS_PATH` environment variable to specify where to store model weights. This allows the container to utilize the pre-downloaded models effectively.\n- Once inside the container, navigate to the appropriate directory and execute the provided Python script (`minimal.py`) to start using Docling.\n\nThis setup provides a streamlined and efficient way to leverage Docling's capabilities within a Dockerized environment."}}, "timestamp": "2025-10-30T23:48:59.056592"}
{"repo_name": "mem0ai/mem0", "stars": 42301, "language": "Python", "tasks": [{"task_title": "ä¸AIè¿›è¡Œå¯¹è¯å¹¶åˆ©ç”¨è®°å¿†", "task_description": "æ­¤ä»»åŠ¡å±•ç¤ºäº†å¦‚ä½•ä¸AIè¿›è¡Œå¯¹è¯ï¼ŒåŒæ—¶åˆ©ç”¨è®°å¿†ç³»ç»Ÿæ¥å¢å¼ºå›åº”çš„ç›¸å…³æ€§ã€‚é€šè¿‡æœç´¢ç”¨æˆ·çš„è®°å¿†å¹¶ç”ŸæˆåŸºäºè¿™äº›è®°å¿†çš„AIå›ç­”ã€‚", "example_code": "from openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()", "running_command": null, "expected_input": "Hello, how are you?", "expected_output": "AI: I'm doing well, thank you! How can I assist you today?"}], "setup": {"setup_commands": ["pip install mem0ai", "npm install mem0ai"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:49:17.998652"}
{"repo_name": "virattt/ai-hedge-fund", "stars": 42141, "language": "Python", "tasks": [{"task_title": "è·å–è‚¡ç¥¨æ•°æ®", "task_description": "é€šè¿‡æŒ‡å®šè‚¡ç¥¨ä»£ç è·å–ç›¸å…³çš„é‡‘èæ•°æ®ï¼Œæ”¯æŒå¤šä¸ªè‚¡ç¥¨ä»£ç ã€‚", "example_code": null, "running_command": "poetry run python src/main.py --ticker AAPL,MSFT,NVDA", "expected_input": "AAPL,MSFT,NVDA", "expected_output": "è·å–åˆ°çš„è‚¡ç¥¨æ•°æ®"}, {"task_title": "è·å–è‚¡ç¥¨æ•°æ®å¹¶ä½¿ç”¨Ollamaæ¨¡å‹", "task_description": "åœ¨è·å–è‚¡ç¥¨æ•°æ®çš„åŒæ—¶ï¼Œä½¿ç”¨Ollamaæ¨¡å‹è¿›è¡Œå¤„ç†ã€‚", "example_code": null, "running_command": "poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama", "expected_input": "AAPL,MSFT,NVDA", "expected_output": "è·å–åˆ°çš„è‚¡ç¥¨æ•°æ®åŠOllamaå¤„ç†ç»“æœ"}, {"task_title": "è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„è‚¡ç¥¨æ•°æ®", "task_description": "é€šè¿‡æŒ‡å®šè‚¡ç¥¨ä»£ç å’Œæ—¥æœŸèŒƒå›´è·å–ç›¸å…³çš„é‡‘èæ•°æ®ã€‚", "example_code": null, "running_command": "poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01", "expected_input": "AAPL,MSFT,NVDA, 2024-01-01, 2024-03-01", "expected_output": "2024å¹´1æœˆ1æ—¥è‡³2024å¹´3æœˆ1æ—¥æœŸé—´çš„è‚¡ç¥¨æ•°æ®"}, {"task_title": "å›æµ‹è‚¡ç¥¨ç­–ç•¥", "task_description": "é€šè¿‡æŒ‡å®šè‚¡ç¥¨ä»£ç è¿›è¡Œè‚¡ç¥¨ç­–ç•¥çš„å›æµ‹ã€‚", "example_code": null, "running_command": "poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA", "expected_input": "AAPL,MSFT,NVDA", "expected_output": "å›æµ‹ç»“æœ"}], "setup": {"setup_commands": ["git clone https://github.com/virattt/ai-hedge-fund.git\ncd ai-hedge-fund", "curl -sSL https://install.python-poetry.org | python3 -", "poetry install"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run the Docker image for the repository `virattt/ai-hedge-fund`, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image is designed to encapsulate the environment necessary for developing and running an AI-driven hedge fund application. It ensures that all dependencies are installed and configured correctly, providing a consistent environment across different machines. This image typically includes libraries for data analysis, machine learning, and any other tools required for the application to function effectively.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `virattt/ai-hedge-fund` repository to your local machine. This will give you access to the necessary files, including the Docker configuration.\n\n2. **Prepare the Environment**: Ensure that Docker is installed and running on your machine. You can verify this by running a simple Docker command in your terminal.\n\n3. **Create a `.dockerignore` File**: The `.dockerignore` file is crucial as it specifies which files and directories should be excluded from the Docker build context. This helps to keep the image size small and avoids including unnecessary files, such as version control directories, local environment settings, IDE configurations, logs, and OS-specific files.\n\n4. **Build the Docker Image**: Use the Docker CLI to build the image. This process will read the Dockerfile (not disclosed here) and the `.dockerignore` file to create an image that contains all the necessary dependencies for the application. The build process will install required packages, set up the environment, and prepare the application for execution.\n\n5. **Verify the Built Image**: After the build process completes, check the list of available Docker images to confirm that your new image has been created successfully. You can do this by running a command that lists all images on your system.\n\n### Running the Docker Image\n1. **Run the Container**: Once the image is built, you can run it as a container. This will start the application in an isolated environment, allowing it to operate without interfering with other applications or services on your machine.\n\n2. **Access the Application**: Depending on how the application is configured, you may need to expose certain ports or set environment variables when running the container. This will allow you to interact with the application, whether through a web interface or API.\n\n### Dependencies\nThe image will typically include:\n- Python and relevant libraries for data manipulation (e.g., Pandas, NumPy).\n- Machine learning frameworks (e.g., TensorFlow, PyTorch).\n- Any additional tools required for data retrieval, processing, and analysis.\n\nBy following these steps, you will successfully build and run the Docker image for the `virattt/ai-hedge-fund` project, ensuring a smooth development and execution environment for your AI hedge fund application."}}, "timestamp": "2025-10-30T23:49:45.748507"}
{"repo_name": "streamlit/streamlit", "stars": 42013, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨æ»‘å—é€‰æ‹©å€¼", "task_description": "è¯¥ä»»åŠ¡å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Streamlitçš„æ»‘å—ç»„ä»¶æ¥é€‰æ‹©ä¸€ä¸ªå€¼ï¼Œå¹¶è®¡ç®—è¯¥å€¼çš„å¹³æ–¹ã€‚", "example_code": "import streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)", "running_command": null, "expected_input": "ä»»æ„æ•°å­—ï¼ˆé€šè¿‡æ»‘å—é€‰æ‹©ï¼‰", "expected_output": "xçš„å¹³æ–¹å€¼ï¼Œä¾‹å¦‚ï¼š\"3 squared is 9\""}], "setup": {"setup_commands": ["$ pip install streamlit\n$ streamlit hello"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:49:54.211876"}
{"repo_name": "zai-org/ChatGLM-6B", "stars": 41149, "language": "Python", "tasks": [{"task_title": "åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨", "task_description": "ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹ï¼Œè®¾ç½®ä¸ºåŠç²¾åº¦å¹¶ç§»åŠ¨åˆ°GPUä¸Šã€‚", "example_code": "from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "æ¨¡å‹å¯¹è¯", "task_description": "ä½¿ç”¨åŠ è½½çš„æ¨¡å‹å’Œåˆ†è¯å™¨è¿›è¡Œå¯¹è¯ï¼Œè¿”å›æ¨¡å‹çš„å“åº”å’Œå†å²è®°å½•ã€‚", "example_code": "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)", "running_command": null, "expected_input": "ä½ å¥½", "expected_output": "ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"}, {"task_title": "ç»§ç»­å¯¹è¯", "task_description": "åœ¨å·²æœ‰çš„å¯¹è¯å†å²åŸºç¡€ä¸Šï¼Œç»§ç»­ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼Œè·å–æ–°çš„å“åº”ã€‚", "example_code": "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\nprint(response)", "running_command": null, "expected_input": "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", "expected_output": "æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•: ..."}, {"task_title": "æ¨¡å‹é‡åŒ–", "task_description": "åŠ è½½é‡åŒ–åçš„æ¨¡å‹ï¼Œæ”¯æŒ4/8ä½é‡åŒ–ã€‚", "example_code": "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).quantize(8).half().cuda()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨æœ¬åœ°æ¨¡å‹", "task_description": "ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹ï¼Œå¹¶è®¾ç½®ä¸ºåŠç²¾åº¦ã€‚", "example_code": "model = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).half().to('mps')", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "åœ¨å¤šä¸ªGPUä¸ŠåŠ è½½æ¨¡å‹", "task_description": "ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°åœ¨å¤šä¸ªGPUä¸ŠåŠ è½½æ¨¡å‹ã€‚", "example_code": "from utils import load_model_on_gpus\nmodel = load_model_on_gpus(\"THUDM/chatglm-6b\", num_gpus=2)", "running_command": null, "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["git clone https://huggingface.co/THUDM/chatglm-6b", "GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b", "git clone https://github.com/THUDM/ChatGLM-6B\ncd ChatGLM-6B", "curl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"ä½ å¥½\", \"history\": []}'"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:50:29.209480"}
{"repo_name": "psf/black", "stars": 41106, "language": "Python", "tasks": [{"task_title": "æ ¼å¼åŒ–Pythonä»£ç ", "task_description": "ä½¿ç”¨Blackåº“å¯¹æŒ‡å®šçš„æ–‡ä»¶æˆ–ç›®å½•è¿›è¡Œä»£ç æ ¼å¼åŒ–ã€‚", "example_code": null, "running_command": "black {source_file_or_directory}", "expected_input": "source_file_or_directory", "expected_output": "Formatted code in the specified file or directory."}, {"task_title": "æ ¼å¼åŒ–Pythonä»£ç ï¼ˆæ¨¡å—æ–¹å¼ï¼‰", "task_description": "é€šè¿‡Pythonæ¨¡å—æ–¹å¼è°ƒç”¨Blackåº“å¯¹æŒ‡å®šçš„æ–‡ä»¶æˆ–ç›®å½•è¿›è¡Œä»£ç æ ¼å¼åŒ–ã€‚", "example_code": null, "running_command": "python -m black {source_file_or_directory}", "expected_input": "source_file_or_directory", "expected_output": "Formatted code in the specified file or directory."}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "To build and run the Docker image for the `psf/black` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image is designed to encapsulate the `black` code formatter for Python, providing a consistent environment for formatting Python code. It includes all necessary dependencies and tools required to run `black` effectively.\n\n### Building the Image\n1. **Base Image**: The build process starts from a lightweight Python 3.12 slim image, which minimizes the image size while still providing the necessary Python environment.\n\n2. **Setting Up the Build Environment**:\n   - A directory named `/src` is created to hold the source code.\n   - The entire content of the repository is copied into this directory.\n\n3. **Environment Variables**:\n   - The `VIRTUAL_ENV` variable is set to specify the location of the virtual environment.\n   - The `HATCH_BUILD_HOOKS_ENABLE` variable is enabled to allow for custom build hooks during the package build process.\n\n4. **Installing Build Tools**:\n   - Essential build tools and dependencies are installed, including `build-essential`, `git`, and `python3-dev`. These tools are necessary for compiling the `black` package and its dependencies.\n\n5. **Creating a Virtual Environment**:\n   - A Python virtual environment is created at the specified `VIRTUAL_ENV` location.\n\n6. **Installing Build Dependencies**:\n   - The `hatch`, `hatch-fancy-pypi-readme`, and `hatch-vcs` packages are installed to facilitate the building of the `black` package.\n\n7. **Building and Installing `black`**:\n   - The `pip` tool is upgraded, and then the `black` package is built and installed from the source code located in `/src`. This includes the installation of additional dependencies like `colorama`, `d`, and `uvloop`.\n\n### Final Image Creation\nAfter the build stage, a second slim Python 3.12 image is created. The virtual environment containing the installed `black` package is copied from the builder stage to this final image. The `PATH` environment variable is updated to include the virtual environment's `bin` directory, ensuring that the `black` command is easily accessible.\n\n### Running the Image\nTo run the Docker container, execute the command that invokes the `black` formatter. This command will utilize the installed `black` package within the virtual environment, allowing you to format Python files as needed.\n\n### Summary\nThis Docker image provides a streamlined, isolated environment for using the `black` code formatter, ensuring that all dependencies are correctly managed and that the tool can be executed without conflicts or issues related to the host system's Python environment."}}, "timestamp": "2025-10-30T23:50:49.244088"}
{"repo_name": "deepspeedai/DeepSpeed", "stars": 40561, "language": "Python", "tasks": [{"task_title": "DeepSpeed Report", "task_description": "ç”ŸæˆDeepSpeedçš„æ€§èƒ½æŠ¥å‘Šï¼Œæä¾›æ¨¡å‹è®­ç»ƒçš„è¯¦ç»†ä¿¡æ¯å’Œæ€§èƒ½åˆ†æã€‚", "example_code": "import deepspeed\n\ndeepspeed.report()", "running_command": null, "expected_input": null, "expected_output": "DeepSpeed Performance Report: ... (åŒ…å«è®­ç»ƒå‚æ•°ã€æ€§èƒ½æŒ‡æ ‡ç­‰)"}], "setup": {"setup_commands": ["pip install deepspeed"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:51:03.629776"}
{"repo_name": "gradio-app/gradio", "stars": 40367, "language": "Python", "tasks": [{"task_title": "Greet User with Intensity", "task_description": "è¿™ä¸ªä»»åŠ¡é€šè¿‡ç”¨æˆ·è¾“å…¥çš„åå­—å’Œå¼ºåº¦æ¥ç”Ÿæˆé—®å€™è¯­ã€‚å¼ºåº¦å†³å®šäº†é—®å€™è¯­ä¸­æ„Ÿå¹å·çš„æ•°é‡ã€‚", "example_code": "import gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()", "running_command": null, "expected_input": {"name": "Alice", "intensity": 3}, "expected_output": "Hello, Alice!!!"}, {"task_title": "Simple Greeting", "task_description": "è¿™ä¸ªä»»åŠ¡æ¥å—ç”¨æˆ·çš„åå­—å¹¶è¿”å›ä¸€ä¸ªç®€å•çš„é—®å€™è¯­ã€‚", "example_code": "import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter ğŸš€", "running_command": null, "expected_input": "Bob", "expected_output": "Hello Bob!"}], "setup": {"setup_commands": ["pip install --upgrade gradio"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "To build and run a Docker image for the Gradio application from the `gradio-app/gradio` repository, follow these steps:\n\n### Purpose of the Docker Image\nThe Docker image serves to encapsulate the Gradio application, which is designed for creating and sharing machine learning demos. This image includes all necessary dependencies to run Gradio, ensuring a consistent environment across different systems. It streamlines the deployment process, allowing developers to focus on building and testing their applications without worrying about environment discrepancies.\n\n### Building Steps\n1. **Clone the Repository**: Start by cloning the `gradio-app/gradio` repository to your local machine. This will give you access to the necessary files, including the `.dockerignore` file.\n\n2. **Review the `.dockerignore` File**: The `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This is crucial for keeping the image size manageable and ensuring sensitive or unnecessary files are not included. Key exclusions include:\n   - Python build artifacts (e.g., `.eggs/`, `dist/`, and compiled Python files).\n   - JavaScript build outputs and static files.\n   - Environment variables and secrets (e.g., `.env`).\n   - Test files and coverage reports.\n   - Demo files and temporary artifacts.\n\n3. **Prepare the Dockerfile**: While the specific content of the Dockerfile is not disclosed, it typically includes instructions to set up the base image (likely a Python image), install dependencies from a requirements file, and configure the application to run.\n\n4. **Build the Docker Image**: Use Docker's build command to create the image. This process will read the Dockerfile and the `.dockerignore` file to determine which files to include and exclude. The resulting image will contain the Gradio application and its dependencies.\n\n5. **Run the Docker Container**: After building the image, you can run it as a container. This will start the Gradio application, making it accessible for use. The container will operate in an isolated environment, ensuring that all dependencies are satisfied.\n\n### Dependencies\nThe image will typically include:\n- Python and necessary libraries for running Gradio.\n- Any additional packages specified in the requirements file, which may include libraries for machine learning, web frameworks, and other utilities.\n\nBy following these steps, you will successfully build and run the Gradio application in a Docker container, leveraging the benefits of containerization for development and deployment."}}, "timestamp": "2025-10-30T23:51:28.869896"}
{"repo_name": "crewAIInc/crewAI", "stars": 39888, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºæ–°çš„å›¢é˜Ÿ", "task_description": "ä½¿ç”¨CLIå‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„å›¢é˜Ÿï¼ŒæŒ‡å®šé¡¹ç›®åç§°ã€‚", "example_code": null, "running_command": "crewai create crew", "expected_input": "project_name", "expected_output": "å›¢é˜Ÿåˆ›å»ºæˆåŠŸ"}, {"task_title": "åˆ›å»ºæœ€æ–°çš„AIå¼€å‘å›¢é˜Ÿ", "task_description": "é€šè¿‡CLIå‘½ä»¤åˆ›å»ºä¸€ä¸ªåä¸º'latest-ai-development'çš„å›¢é˜Ÿã€‚", "example_code": null, "running_command": "crewai create crew latest-ai-development", "expected_input": "latest-ai-development", "expected_output": "å›¢é˜Ÿ'latest-ai-development'åˆ›å»ºæˆåŠŸ"}], "setup": {"setup_commands": ["pip install crewai", "pip install 'crewai[tools]'"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T23:51:38.807522"}
