{
  "tasks": [
    {
      "task_title": "加载数据集",
      "task_description": "从指定的URL加载LAION数据集，使用datasets库的load_dataset功能。",
      "example_code": "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")",
      "running_command": null,
      "expected_input": "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl",
      "expected_output": "加载的数据集对象"
    },
    {
      "task_title": "加载预训练模型",
      "task_description": "使用FastModel加载预训练的语言模型，支持4位量化以减少内存占用。",
      "example_code": "model, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    load_in_8bit = False,\n    load_in_16bit = False,\n    full_finetuning = False\n)",
      "running_command": null,
      "expected_input": "unsloth/gpt-oss-20b",
      "expected_output": "加载的模型和分词器对象"
    },
    {
      "task_title": "模型微调",
      "task_description": "使用SFTTrainer对模型进行微调，配置训练参数并开始训练。",
      "example_code": "trainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    args = SFTConfig(\n        max_seq_length = max_seq_length,\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 10,\n        max_steps = 60,\n        logging_steps = 1,\n        output_dir = \"outputs\",\n        optim = \"adamw_8bit\",\n        seed = 3407,\n    ),\n)\ntrainer.train()",
      "running_command": null,
      "expected_input": "训练数据集和配置参数",
      "expected_output": "训练过程的日志输出和模型保存"
    }
  ],
  "setup": {
    "setup_commands": [
      "pip install unsloth",
      "pip install unsloth",
      "python -m venv unsloth\nsource unsloth/bin/activate\npip install unsloth",
      "  pip install ninja\n  pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers",
      "conda create --name unsloth_env \\\n    python=3.11 \\\n    pytorch-cuda=12.1 \\\n    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n    -y\nconda activate unsloth_env\n\npip install unsloth",
      "  mkdir -p ~/miniconda3\n  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n  rm -rf ~/miniconda3/miniconda.sh\n  ~/miniconda3/bin/conda init bash\n  ~/miniconda3/bin/conda init zsh",
      "pip install --upgrade pip\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"",
      "pip install --upgrade pip\npip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"",
      "pip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"",
      "wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -",
      "try: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nimport re\nv = V(re.match(r\"[0-9\\.]{3,}\", torch.__version__).group(0))\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nUSE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI\nif cuda not in (\"11.8\", \"12.1\", \"12.4\", \"12.6\", \"12.8\"): raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v  < V('2.5.1'): x = 'cu{}{}-torch250'\nelif v <= V('2.5.1'): x = 'cu{}{}-torch251'\nelif v  < V('2.7.0'): x = 'cu{}{}-torch260'\nelif v  < V('2.7.9'): x = 'cu{}{}-torch270'\nelif v  < V('2.8.0'): x = 'cu{}{}-torch271'\nelif v  < V('2.8.9'): x = 'cu{}{}-torch280'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nif v > V('2.6.9') and cuda not in (\"11.8\", \"12.6\", \"12.8\"): raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')"
    ],
    "docker_commands": [
      "docker run -d -e JUPYTER_PASSWORD=\"mypassword\" \\\n  -p 8888:8888 -p 2222:22 \\\n  -v $(pwd)/work:/workspace/work \\\n  --gpus all \\\n  unsloth/unsloth"
    ],
    "docker_files": {}
  },
  "input_to_gpt": {
    "repo_name": "unslothai/unsloth",
    "num_code_blocks": 2,
    "total_length": 2425,
    "code_blocks": [
      "SFTConfig(\n    dataset_num_proc=1,\n    ...\n)",
      "from unsloth import FastLanguageModel, FastModel\nimport torch\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nmax_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!\n# Get LAION dataset\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", #or choose any model\n\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b\",\n    max_seq_length = 2048, # Choose any for long context!\n    load_in_4bit = True,  # 4-bit quantization. False = 16-bit LoRA.\n    load_in_8bit = False, # 8-bit quantization\n    load_in_16bit = False, # [NEW!] 16-bit LoRA\n    full_finetuning = False, # Use for full fine-tuning.\n    # token = \"hf_...\", # use one if using gated models\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    args = SFTConfig(\n        max_seq_length = max_seq_length,\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 10,\n        max_steps = 60,\n        logging_steps = 1,\n        output_dir = \"outputs\",\n        optim = \"adamw_8bit\",\n        seed = 3407,\n    ),\n)\ntrainer.train()\n\n# Go to https://docs.unsloth.ai for advanced tips like\n# (1) Saving to GGUF / merging to 16bit for vLLM\n# (2) Continued training from a saved LoRA adapter\n# (3) Adding an evaluation loop / OOMs\n# (4) Customized chat templates"
    ]
  }
}