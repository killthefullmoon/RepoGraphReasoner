{
  "tasks": [
    {
      "task_title": "Transcribe audio files",
      "task_description": "This task transcribes audio files into text using the specified model.",
      "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])",
      "running_command": null,
      "expected_input": "audio.mp3",
      "expected_output": "Transcribed text from audio.mp3"
    },
    {
      "task_title": "Specify language for transcription",
      "task_description": "This task transcribes an audio file with a specified language.",
      "example_code": null,
      "running_command": "whisper japanese.wav --language Japanese",
      "expected_input": "japanese.wav",
      "expected_output": "Transcribed text from japanese.wav in Japanese"
    },
    {
      "task_title": "Translate audio content",
      "task_description": "This task translates the spoken content of an audio file into another language.",
      "example_code": null,
      "running_command": "whisper japanese.wav --model medium --language Japanese --task translate",
      "expected_input": "japanese.wav",
      "expected_output": "Translated text from japanese.wav"
    },
    {
      "task_title": "Detect spoken language",
      "task_description": "This task detects the language spoken in an audio file.",
      "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")",
      "running_command": null,
      "expected_input": "audio.mp3",
      "expected_output": "Detected language: [language]"
    }
  ],
  "setup": {
    "setup_commands": [
      "# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg",
      "pip install setuptools-rust"
    ],
    "docker_commands": [],
    "docker_files": {},
    "docker_setup_descriptions": null
  },
  "input_to_gpt": {
    "repo_name": "openai/whisper",
    "num_code_blocks": 6,
    "total_length": 926,
    "code_blocks": [
      "whisper audio.flac audio.mp3 audio.wav --model turbo",
      "whisper japanese.wav --language Japanese",
      "whisper japanese.wav --model medium --language Japanese --task translate",
      "whisper --help",
      "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])",
      "import whisper\n\nmodel = whisper.load_model(\"turbo\")\n\n# load audio and pad/trim it to fit 30 seconds\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\n\n# make log-Mel spectrogram and move to the same device as the model\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\n# detect the spoken language\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")\n\n# decode the audio\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\n\n# print the recognized text\nprint(result.text)"
    ]
  }
}