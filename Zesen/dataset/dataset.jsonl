{"repo_name": "Significant-Gravitas/AutoGPT", "stars": 179386, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†", "task_description": "ä½¿ç”¨agentå‘½ä»¤æ¥åˆ›å»ºã€å¯åŠ¨å’Œåœæ­¢ä»£ç†ã€‚", "example_code": null, "running_command": "cli.py agent", "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨åŸºå‡†æµ‹è¯•", "task_description": "ä½¿ç”¨benchmarkå‘½ä»¤æ¥å¯åŠ¨åŸºå‡†æµ‹è¯•å¹¶åˆ—å‡ºæµ‹è¯•å’Œç±»åˆ«ã€‚", "example_code": null, "running_command": "cli.py benchmark", "expected_input": null, "expected_output": null}, {"task_title": "å®‰è£…ä¾èµ–", "task_description": "ä½¿ç”¨setupå‘½ä»¤æ¥å®‰è£…ç³»ç»Ÿæ‰€éœ€çš„ä¾èµ–ã€‚", "example_code": null, "running_command": "cli.py setup", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` from Significant-Gravitas/AutoGPT\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify which files and directories should be excluded from the Docker build context. This helps reduce the size of the context sent to the Docker daemon, improving build performance and security by preventing unnecessary files from being included in the image.\n\n#### Semantic Meaning of the File\n\n1. **Default Ignore Rule**: The line `*` indicates that all files and directories should be ignored by default.\n2. **Selective Inclusion**: Subsequent lines with `!` specify exceptions to the default rule, allowing specific files and directories to be included in the build context. \n3. **Structured Organization**: The file is organized into sections for different components of the project (e.g., Platform, Classic), ensuring only relevant files for each component are included.\n4. **Explicit Re-ignoring**: The last two lines explicitly re-ignore certain directories like `__pycache__` and hidden files, which are typically not needed for the build.\n\n#### Actionable Steps to Build and Run Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/Significant-Gravitas/AutoGPT.git\n   cd AutoGPT\n   ```\n\n2. **Build the Docker Image**:\n   Ensure you are in the root directory of the cloned repository and run:\n   ```bash\n   docker build -t autogpt .\n   ```\n\n3. **Run the Docker Container**:\n   After the image is built, you can run it using:\n   ```bash\n   docker run -d --name autogpt_container autogpt\n   ```\n\n4. **Verify the Container is Running**:\n   Check the status of your container with:\n   ```bash\n   docker ps\n   ```\n\n5. **Access Logs (if needed)**:\n   To view logs from the running container:\n   ```bash\n   docker logs autogpt_container\n   ```\n\n#### Related Docker Commands in README\n\nCheck the repository's README for any additional Docker commands or configurations that may be necessary for your specific use case."}}, "timestamp": "2025-10-30T20:31:49.206203"}
{"repo_name": "ytdl-org/youtube-dl", "stars": 138632, "language": "Python", "tasks": [{"task_title": "è·å–è§†é¢‘æ–‡ä»¶å", "task_description": "è·å–æŒ‡å®šè§†é¢‘çš„æ–‡ä»¶åï¼Œæ”¯æŒç‰¹æ®Šå­—ç¬¦", "example_code": null, "running_command": "youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl test video ''_Ã¤â†­ğ•.mp4"}, {"task_title": "è·å–ç®€å•æ–‡ä»¶å", "task_description": "è·å–æŒ‡å®šè§†é¢‘çš„æ–‡ä»¶åï¼Œé™åˆ¶æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦", "example_code": null, "running_command": "youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl_test_video_.mp4"}, {"task_title": "ä¸‹è½½YouTubeæ’­æ”¾åˆ—è¡¨", "task_description": "ä¸‹è½½æŒ‡å®šYouTubeæ’­æ”¾åˆ—è¡¨ä¸­çš„è§†é¢‘ï¼Œå¹¶æŒ‰é¡ºåºå­˜æ”¾åœ¨ä¸åŒç›®å½•ä¸­", "example_code": null, "running_command": "youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_input": "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_output": "ä¸‹è½½çš„æ’­æ”¾åˆ—è¡¨è§†é¢‘"}, {"task_title": "ä¸‹è½½YouTubeé¢‘é“æ‰€æœ‰æ’­æ”¾åˆ—è¡¨", "task_description": "ä¸‹è½½æŒ‡å®šYouTubeé¢‘é“ä¸­çš„æ‰€æœ‰æ’­æ”¾åˆ—è¡¨ï¼Œå¹¶æŒ‰æ’­æ”¾åˆ—è¡¨å­˜æ”¾åœ¨ä¸åŒç›®å½•ä¸­", "example_code": null, "running_command": "youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists", "expected_input": "https://www.youtube.com/user/TheLinuxFoundation/playlists", "expected_output": "ä¸‹è½½çš„é¢‘é“æ‰€æœ‰æ’­æ”¾åˆ—è¡¨è§†é¢‘"}, {"task_title": "ä¸‹è½½Udemyè¯¾ç¨‹", "task_description": "ä¸‹è½½æŒ‡å®šUdemyè¯¾ç¨‹ï¼Œå¹¶æŒ‰ç« èŠ‚å­˜æ”¾åœ¨ä¸åŒç›®å½•ä¸­", "example_code": null, "running_command": "youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/", "expected_input": "https://www.udemy.com/java-tutorial/", "expected_output": "ä¸‹è½½çš„Udemyè¯¾ç¨‹è§†é¢‘"}, {"task_title": "ä¸‹è½½æ•´ä¸ªç³»åˆ—å­£", "task_description": "ä¸‹è½½æŒ‡å®šè§†é¢‘ç³»åˆ—çš„æ‰€æœ‰å­£ï¼Œå¹¶æŒ‰ç³»åˆ—å’Œå­£å­˜æ”¾åœ¨ä¸åŒç›®å½•ä¸­", "example_code": null, "running_command": "youtube-dl -o 'C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s' https://videomore.ru/kino_v_detalayah/5_sezon/367617", "expected_input": "https://videomore.ru/kino_v_detalayah/5_sezon/367617", "expected_output": "ä¸‹è½½çš„ç³»åˆ—å­£è§†é¢‘"}, {"task_title": "æµå¼ä¸‹è½½è§†é¢‘", "task_description": "å°†ä¸‹è½½çš„è§†é¢‘æµè¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º", "example_code": null, "running_command": "youtube-dl -o - BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "è§†é¢‘æµ"}, {"task_title": "ä¸‹è½½æœ€ä½³è§†é¢‘æ ¼å¼", "task_description": "ä¸‹è½½å¯ç”¨çš„æœ€ä½³mp4æ ¼å¼æˆ–å…¶ä»–æ ¼å¼", "example_code": null, "running_command": "youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³è§†é¢‘æ ¼å¼"}, {"task_title": "ä¸‹è½½ä¸è¶…è¿‡480pçš„è§†é¢‘", "task_description": "ä¸‹è½½æœ€ä½³æ ¼å¼ï¼Œä½†ä¸è¶…è¿‡480p", "example_code": null, "running_command": "youtube-dl -f 'bestvideo[height<=480]+bestaudio/best[height<=480]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„480pè§†é¢‘"}, {"task_title": "ä¸‹è½½ä¸è¶…è¿‡50MBçš„è§†é¢‘", "task_description": "ä¸‹è½½æœ€ä½³è§†é¢‘æ ¼å¼ï¼Œä½†ä¸è¶…è¿‡50MB", "example_code": null, "running_command": "youtube-dl -f 'best[filesize<50M]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„ä¸è¶…è¿‡50MBçš„è§†é¢‘"}, {"task_title": "é€šè¿‡HTTP/HTTPSåè®®ä¸‹è½½æœ€ä½³æ ¼å¼", "task_description": "ä¸‹è½½æœ€ä½³æ ¼å¼ï¼Œä½†ä»…é€šè¿‡HTTP/HTTPSåè®®", "example_code": null, "running_command": "youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³æ ¼å¼è§†é¢‘"}, {"task_title": "ä¸‹è½½æœ€ä½³è§†é¢‘å’ŒéŸ³é¢‘æ ¼å¼", "task_description": "ä¸‹è½½æœ€ä½³è§†é¢‘æ ¼å¼å’Œæœ€ä½³éŸ³é¢‘æ ¼å¼ï¼Œä¸è¿›è¡Œåˆå¹¶", "example_code": null, "running_command": "youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€ä½³è§†é¢‘å’ŒéŸ³é¢‘æ ¼å¼"}, {"task_title": "ä¸‹è½½æœ€è¿‘6ä¸ªæœˆä¸Šä¼ çš„è§†é¢‘", "task_description": "ä¸‹è½½åœ¨è¿‡å»6ä¸ªæœˆå†…ä¸Šä¼ çš„è§†é¢‘", "example_code": null, "running_command": "youtube-dl --dateafter now-6months", "expected_input": null, "expected_output": "ä¸‹è½½çš„æœ€è¿‘6ä¸ªæœˆçš„è§†é¢‘"}, {"task_title": "ä¸‹è½½ç‰¹å®šæ—¥æœŸä¸Šä¼ çš„è§†é¢‘", "task_description": "ä¸‹è½½åœ¨1970å¹´1æœˆ1æ—¥ä¸Šä¼ çš„è§†é¢‘", "example_code": null, "running_command": "youtube-dl --date 19700101", "expected_input": null, "expected_output": "ä¸‹è½½çš„ç‰¹å®šæ—¥æœŸçš„è§†é¢‘"}, {"task_title": "ä¸‹è½½2000å¹´ä»£ä¸Šä¼ çš„è§†é¢‘", "task_description": "ä¸‹è½½åœ¨2000å¹´ä»£ä¸Šä¼ çš„è§†é¢‘", "example_code": null, "running_command": "youtube-dl --dateafter 20000101 --datebefore 20091231", "expected_input": null, "expected_output": "ä¸‹è½½çš„2000å¹´ä»£çš„è§†é¢‘"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:32:39.525333"}
{"repo_name": "langflow-ai/langflow", "stars": 136681, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒLangflow", "task_description": "è¯¥ä»»åŠ¡ç”¨äºå¯åŠ¨Langflowåº”ç”¨ç¨‹åºï¼Œå…è®¸ç”¨æˆ·è¿è¡Œç›¸å…³çš„åŠŸèƒ½å’Œæ“ä½œã€‚", "example_code": null, "running_command": "uv run langflow run", "expected_input": null, "expected_output": "åº”ç”¨ç¨‹åºå¯åŠ¨å¹¶è¿è¡Œ"}], "setup": {"setup_commands": ["uv pip install langflow -U"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` in langflow-ai/langflow\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the Docker image, speeds up the build process, and prevents sensitive or unnecessary files from being included in the image. \n\n#### Semantic Meaning of the Entries\n\n1. **`src/frontend/node_modules`**: Excludes the `node_modules` directory, which contains installed packages that can be restored via `npm install`.\n2. **`src/frontend/build`**: Excludes the build artifacts, which can be regenerated.\n3. **`src/frontend/coverage`**: Excludes code coverage reports, typically not needed in the image.\n4. **`src/frontend/test-results`**: Excludes test result files, which are not required for production.\n5. **`src/frontend/playwright-report`**: Excludes Playwright test reports, similar to test results.\n6. **`src/frontend/.dspy_cache`**: Excludes cache files generated by development tools.\n7. **`**/.DS_Store`**: Excludes macOS-specific metadata files.\n8. **`**/__pycache__`**: Excludes Python bytecode cache directories.\n9. **`**/*.pyc`**: Excludes Python compiled files.\n10. **`**/.pytest_cache`**: Excludes pytest cache files.\n11. **`**/.venv`**: Excludes virtual environment directories.\n12. **`**/.env`**: Excludes environment variable files, which may contain sensitive information.\n\n#### Building and Running Docker\n\nTo build and run the Docker container for the `langflow-ai/langflow` repository, follow these steps:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/langflow-ai/langflow.git\n   cd langflow\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image. Make sure you are in the root directory of the repository where the `Dockerfile` is located.\n   ```bash\n   docker build -t langflow .\n   ```\n\n3. **Run the Docker Container**:\n   After building the image, run the container using:\n   ```bash\n   docker run -d -p 8080:8080 langflow\n   ```\n\n4. **Access the Application**:\n   Open your web browser and navigate to `http://localhost:8080` to access the application.\n\n#### Note\n\nEnsure that you have Docker installed and running on your machine before executing these commands. Adjust the port mapping (`-p 8080:8080`) as necessary based on your application's configuration."}}, "timestamp": "2025-10-30T20:32:58.750268"}
{"repo_name": "yt-dlp/yt-dlp", "stars": 133286, "language": "Python", "tasks": [{"task_title": "Download video with specified filename", "task_description": "This command downloads a video and specifies the output filename format.", "example_code": null, "running_command": "yt-dlp --print filename -o 'test video.%(ext)s' BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "test video.webm"}, {"task_title": "Download video with title in filename", "task_description": "This command downloads a video and names the file based on the video's title, including handling special characters.", "example_code": null, "running_command": "yt-dlp --print filename -o '%(title)s.%(ext)s' BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl test video ''_Ã¤â†­ğ•.webm"}, {"task_title": "Restrict filenames", "task_description": "This command downloads a video and restricts the filename to avoid special characters.", "example_code": null, "running_command": "yt-dlp --print filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames", "expected_input": "BaW_jenozKc", "expected_output": "youtube-dl_test_video_.webm"}, {"task_title": "Download YouTube playlist videos in separate directory", "task_description": "This command downloads all videos from a YouTube playlist into a directory indexed by the video order in the playlist.", "example_code": null, "running_command": "yt-dlp -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' 'https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re'", "expected_input": "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re", "expected_output": "Videos downloaded in specified directory structure"}, {"task_title": "Download Udemy course", "task_description": "This command downloads an entire Udemy course, organizing chapters into separate directories.", "example_code": null, "running_command": "yt-dlp -u user -p password -P '~/MyVideos' -o '%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' 'https://www.udemy.com/java-tutorial'", "expected_input": "https://www.udemy.com/java-tutorial", "expected_output": "Course chapters downloaded in specified directory structure"}, {"task_title": "Download video and subtitles", "task_description": "This command downloads a video and its subtitles, saving them in specified directories.", "example_code": null, "running_command": "yt-dlp -P 'C:/MyVideos' -P 'temp:tmp' -P 'subtitle:subs' -o '%(uploader)s/%(title)s.%(ext)s' BaW_jenozKc --write-subs", "expected_input": "BaW_jenozKc", "expected_output": "Video and subtitles downloaded in specified directories"}, {"task_title": "Stream video to stdout", "task_description": "This command streams the video directly to standard output.", "example_code": null, "running_command": "yt-dlp -o - BaW_jenozKc", "expected_input": "BaW_jenozKc", "expected_output": "Streaming video data"}, {"task_title": "Download best video and audio formats", "task_description": "This command downloads the best video-only and audio-only formats and merges them.", "example_code": null, "running_command": "yt-dlp -f 'bv+ba/b'", "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Best video and audio formats downloaded and merged"}, {"task_title": "Download video with specific duration filter", "task_description": "This command downloads only videos longer than a minute.", "example_code": "def longer_than_a_minute(info, *, incomplete):\n    duration = info.get('duration')\n    if duration and duration < 60:\n        return 'The video is too short'\n\nydl_opts = {\n    'match_filter': longer_than_a_minute,\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    error_code = ydl.download(URLS)", "running_command": null, "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Video downloaded if longer than a minute"}, {"task_title": "Custom logger for download progress", "task_description": "This command sets up a custom logger to handle download progress messages.", "example_code": "class MyLogger:\n    def debug(self, msg):\n        pass\n\nydl_opts = {\n    'logger': MyLogger(),\n}\n\nwith yt_dlp.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(URLS)", "running_command": null, "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Download progress logged"}, {"task_title": "Custom post-processor for additional processing", "task_description": "This command defines a custom post-processor for additional processing after download.", "example_code": "class MyCustomPP(yt_dlp.postprocessor.PostProcessor):\n    def run(self, info):\n        return [], info\n\nwith yt_dlp.YoutubeDL() as ydl:\n    ydl.add_post_processor(MyCustomPP(), when='pre_process')\n    ydl.download(URLS)", "running_command": null, "expected_input": "https://www.youtube.com/watch?v=BaW_jenozKc", "expected_output": "Video downloaded with custom processing"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:33:51.819126"}
{"repo_name": "deepseek-ai/DeepSeek-V3", "stars": 100074, "language": "Python", "tasks": [{"task_title": "FP8æƒé‡è½¬æ¢ä¸ºBF16", "task_description": "æ­¤ä»»åŠ¡å°†FP8æ ¼å¼çš„æƒé‡è½¬æ¢ä¸ºBF16æ ¼å¼ï¼Œä½¿ç”¨äº†fp8_cast_bf16.pyè„šæœ¬ã€‚", "example_code": null, "running_command": "python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights", "expected_input": "/path/to/fp8_weights, /path/to/bf16_weights", "expected_output": "è½¬æ¢å®Œæˆï¼Œç”Ÿæˆçš„BF16æƒé‡æ–‡ä»¶"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:34:07.807491"}
{"repo_name": "nvbn/thefuck", "stars": 94518, "language": "Python", "tasks": [{"task_title": "Fix Git Push Command", "task_description": "Automatically corrects the git push command by setting the upstream branch.", "example_code": null, "running_command": "fuck", "expected_input": "git push", "expected_output": "git push --set-upstream origin master"}, {"task_title": "Fix Misspelled Command", "task_description": "Suggests the correct command when a misspelled command is entered.", "example_code": null, "running_command": "fuck", "expected_input": "puthon", "expected_output": "python"}, {"task_title": "Fix Git Branch Command", "task_description": "Automatically corrects the git branch command when a typo is detected.", "example_code": null, "running_command": "fuck", "expected_input": "git brnch", "expected_output": "git branch"}, {"task_title": "Fix Lein Command", "task_description": "Suggests the correct Lein command when an invalid task is entered.", "example_code": null, "running_command": "fuck", "expected_input": "lein rpl", "expected_output": "lein repl"}, {"task_title": "Run thefuck with Confirmation", "task_description": "Executes thefuck with confirmation required.", "example_code": null, "running_command": "fuck --yeah", "expected_input": null, "expected_output": null}, {"task_title": "Re-run Last Command", "task_description": "Re-runs the last fixed command.", "example_code": null, "running_command": "fuck -r", "expected_input": null, "expected_output": null}, {"task_title": "Match Command Example", "task_description": "Defines a function to match commands based on output.", "example_code": "def match(command):\n    return ('permission denied' in command.output.lower() or 'EACCES' in command.output)", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "Get New Command Example", "task_description": "Defines a function to get a new command based on the old one.", "example_code": "def get_new_command(command):\n    return 'sudo {}'.format(command.script)", "running_command": null, "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim [enter/â†‘/â†“/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...", "âœ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\nâœ fuck\nsudo apt-get install vim\n[sudo] password for nvbn:\nReading package lists... Done\n...", "brew install thefuck", "pip install thefuck"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:34:25.497926"}
{"repo_name": "comfyanonymous/ComfyUI", "stars": 92341, "language": "Python", "tasks": [{"task_title": "è¿è¡ŒComfyUIä»¥æ”¯æŒAMDå¡", "task_description": "é€šè¿‡è®¾ç½®HSA_OVERRIDE_GFX_VERSIONç¯å¢ƒå˜é‡æ¥è¿è¡ŒComfyUIï¼Œæ”¯æŒç‰¹å®šçš„AMDæ˜¾å¡ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py", "expected_input": null, "expected_output": "è¿è¡ŒComfyUIåº”ç”¨ç¨‹åº"}, {"task_title": "è¿è¡ŒComfyUIä»¥æ”¯æŒAMD RDNA3å¡", "task_description": "é€šè¿‡è®¾ç½®HSA_OVERRIDE_GFX_VERSIONç¯å¢ƒå˜é‡æ¥è¿è¡ŒComfyUIï¼Œæ”¯æŒAMD RDNA3æ˜¾å¡ã€‚", "example_code": null, "running_command": "HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py", "expected_input": null, "expected_output": "è¿è¡ŒComfyUIåº”ç”¨ç¨‹åº"}, {"task_title": "å¯ç”¨å®éªŒæ€§å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›", "task_description": "åœ¨æŸäº›AMD GPUä¸Šå¯ç”¨å®éªŒæ€§å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ï¼Œè¿™åœ¨æœ€æ–°çš„PyTorchä¸­å¯ä»¥æé«˜é€Ÿåº¦ã€‚", "example_code": null, "running_command": null, "expected_input": null, "expected_output": "å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å·²å¯ç”¨"}], "setup": {"setup_commands": ["pip install comfy-cli\ncomfy install"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:34:40.852048"}
{"repo_name": "fastapi/fastapi", "stars": 91345, "language": "Python", "tasks": [{"task_title": "è¯»å–æ ¹è·¯å¾„", "task_description": "åˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨å¹¶å®šä¹‰ä¸€ä¸ªGETè¯·æ±‚çš„æ ¹è·¯å¾„ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "è¯»å–æŒ‡å®šé¡¹", "task_description": "å®šä¹‰ä¸€ä¸ªGETè¯·æ±‚çš„è·¯å¾„ï¼Œæ¥æ”¶ä¸€ä¸ªitem_idå‚æ•°ï¼Œå¹¶å¯é€‰åœ°æ¥æ”¶ä¸€ä¸ªæŸ¥è¯¢å‚æ•°qï¼Œè¿”å›åŒ…å«è¿™äº›å‚æ•°çš„JSONå“åº”ã€‚", "example_code": "from fastapi import FastAPI\nfrom typing import Union\n\napp = FastAPI()\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": {"item_id": 1, "q": "test"}, "expected_output": "{\"item_id\": 1, \"q\": \"test\"}"}, {"task_title": "å¼‚æ­¥è¯»å–æ ¹è·¯å¾„", "task_description": "åˆ›å»ºä¸€ä¸ªFastAPIåº”ç”¨å¹¶å®šä¹‰ä¸€ä¸ªå¼‚æ­¥GETè¯·æ±‚çš„æ ¹è·¯å¾„ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„JSONå“åº”ã€‚", "example_code": "from fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}", "running_command": null, "expected_input": null, "expected_output": "{\"Hello\": \"World\"}"}, {"task_title": "å¼‚æ­¥è¯»å–æŒ‡å®šé¡¹", "task_description": "å®šä¹‰ä¸€ä¸ªå¼‚æ­¥GETè¯·æ±‚çš„è·¯å¾„ï¼Œæ¥æ”¶ä¸€ä¸ªitem_idå‚æ•°ï¼Œå¹¶å¯é€‰åœ°æ¥æ”¶ä¸€ä¸ªæŸ¥è¯¢å‚æ•°qï¼Œè¿”å›åŒ…å«è¿™äº›å‚æ•°çš„JSONå“åº”ã€‚", "example_code": "from fastapi import FastAPI\nfrom typing import Union\n\napp = FastAPI()\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}", "running_command": null, "expected_input": {"item_id": 2, "q": "example"}, "expected_output": "{\"item_id\": 2, \"q\": \"example\"}"}], "setup": {"setup_commands": ["$ pip install \"fastapi[standard]\"\n\n---> 100%"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:34:58.271794"}
{"repo_name": "openai/whisper", "stars": 90148, "language": "Python", "tasks": [{"task_title": "Transcribe Audio", "task_description": "ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å°†éŸ³é¢‘æ–‡ä»¶è½¬å½•ä¸ºæ–‡æœ¬ã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\nresult = model.transcribe(\"audio.mp3\")\nprint(result[\"text\"])", "running_command": null, "expected_input": "audio.mp3", "expected_output": "è½¬å½•åçš„æ–‡æœ¬"}, {"task_title": "Detect Language", "task_description": "åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶æ£€æµ‹å…¶è¯­è¨€ã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"audio.mp3\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n_, probs = model.detect_language(mel)\nprint(f\"Detected language: {max(probs, key=probs.get)}\")", "running_command": null, "expected_input": "audio.mp3", "expected_output": "Detected language: è¯­è¨€åç§°"}, {"task_title": "Translate Audio", "task_description": "å°†éŸ³é¢‘æ–‡ä»¶ä¸­çš„è¯­è¨€ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ã€‚", "example_code": "import whisper\n\nmodel = whisper.load_model(\"turbo\")\naudio = whisper.load_audio(\"japanese.wav\")\naudio = whisper.pad_or_trim(audio)\nmel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\noptions = whisper.DecodingOptions()\nresult = whisper.decode(model, mel, options)\nprint(result.text)", "running_command": null, "expected_input": "japanese.wav", "expected_output": "ç¿»è¯‘åçš„æ–‡æœ¬"}, {"task_title": "Use CLI for Transcription", "task_description": "é€šè¿‡å‘½ä»¤è¡Œæ¥å£ä½¿ç”¨æ¨¡å‹è¿›è¡ŒéŸ³é¢‘è½¬å½•ã€‚", "example_code": null, "running_command": "whisper audio.flac audio.mp3 audio.wav --model turbo", "expected_input": "audio.flac, audio.mp3, audio.wav", "expected_output": "è½¬å½•åçš„æ–‡æœ¬"}, {"task_title": "Specify Language in CLI", "task_description": "åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šéŸ³é¢‘æ–‡ä»¶çš„è¯­è¨€ã€‚", "example_code": null, "running_command": "whisper japanese.wav --language Japanese", "expected_input": "japanese.wav", "expected_output": "è½¬å½•åçš„æ–‡æœ¬"}, {"task_title": "Translate with CLI", "task_description": "åœ¨å‘½ä»¤è¡Œä¸­å°†éŸ³é¢‘æ–‡ä»¶ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ã€‚", "example_code": null, "running_command": "whisper japanese.wav --model medium --language Japanese --task translate", "expected_input": "japanese.wav", "expected_output": "ç¿»è¯‘åçš„æ–‡æœ¬"}, {"task_title": "Help Command", "task_description": "æ˜¾ç¤ºå‘½ä»¤è¡Œå·¥å…·çš„å¸®åŠ©ä¿¡æ¯ã€‚", "example_code": null, "running_command": "whisper --help", "expected_input": null, "expected_output": "å¸®åŠ©ä¿¡æ¯"}], "setup": {"setup_commands": ["# on Ubuntu or Debian\nsudo apt update && sudo apt install ffmpeg\n\n# on Arch Linux\nsudo pacman -S ffmpeg\n\n# on MacOS using Homebrew (https://brew.sh/)\nbrew install ffmpeg\n\n# on Windows using Chocolatey (https://chocolatey.org/)\nchoco install ffmpeg\n\n# on Windows using Scoop (https://scoop.sh/)\nscoop install ffmpeg", "pip install setuptools-rust"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:35:18.194966"}
{"repo_name": "microsoft/markitdown", "stars": 82287, "language": "Python", "tasks": [{"task_title": "Convert PDF to Markdown", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„æ–‡æ¡£ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf > document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF to Markdown with Output File Option", "task_description": "å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼å¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md", "expected_input": "path-to-file.pdf", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert PDF from Standard Input", "task_description": "é€šè¿‡æ ‡å‡†è¾“å…¥å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "cat path-to-file.pdf | markitdown", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "List Available Plugins", "task_description": "åˆ—å‡ºå¯ç”¨çš„æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --list-plugins", "expected_input": null, "expected_output": "æ’ä»¶åˆ—è¡¨"}, {"task_title": "Use Plugins for Conversion", "task_description": "åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ä½¿ç”¨æ’ä»¶ã€‚", "example_code": null, "running_command": "markitdown --use-plugins path-to-file.pdf", "expected_input": "path-to-file.pdf", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF with Document Intelligence Endpoint", "task_description": "ä½¿ç”¨æŒ‡å®šçš„æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": null, "running_command": "markitdown path-to-file.pdf -o document.md -d -e \"<document_intelligence_endpoint>\"", "expected_input": "path-to-file.pdf, <document_intelligence_endpoint>", "expected_output": "document.mdå†…å®¹"}, {"task_title": "Convert Excel to Markdown without Plugins", "task_description": "å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œç¦ç”¨æ’ä»¶ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) # Set to True to enable plugins\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.xlsx", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert PDF using Document Intelligence Endpoint", "task_description": "ä½¿ç”¨æ–‡æ¡£æ™ºèƒ½ç«¯ç‚¹å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚", "example_code": "from markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"<document_intelligence_endpoint>\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)", "running_command": null, "expected_input": "test.pdf, <document_intelligence_endpoint>", "expected_output": "Markdownå†…å®¹"}, {"task_title": "Convert Image to Markdown using OpenAI", "task_description": "å°†å›¾åƒæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œä½¿ç”¨OpenAIçš„LLMã€‚", "example_code": "from markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)", "running_command": null, "expected_input": "example.jpg", "expected_output": "Markdownå†…å®¹"}], "setup": {"setup_commands": ["python -m venv .venv\nsource .venv/bin/activate", "uv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment", "conda create -n markitdown python=3.12\nconda activate markitdown", "git clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'", "pip install 'markitdown[pdf, docx, pptx]'", "  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test"], "docker_commands": ["docker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for `microsoft/markitdown`\n\nTo build and run the Docker container for the `microsoft/markitdown` project using the provided `Dockerfile`, follow these steps:\n\n#### Step 1: Understanding the Dockerfile\n\n1. **Base Image**: \n   - `FROM python:3.13-slim-bullseye`: This line specifies that the container will be built on a lightweight version of Python 3.13 based on Debian Bullseye.\n\n2. **Environment Variables**:\n   - `ENV DEBIAN_FRONTEND=noninteractive`: Sets the environment to avoid interactive prompts during package installation.\n   - `ENV EXIFTOOL_PATH` and `ENV FFMPEG_PATH`: Define paths for `exiftool` and `ffmpeg`, which are installed later.\n\n3. **Install Dependencies**:\n   - The `RUN apt-get update && apt-get install -y --no-install-recommends` command installs `ffmpeg` and `exiftool`, which are essential for processing multimedia files and metadata.\n\n4. **Conditional Git Installation**:\n   - The `ARG INSTALL_GIT=false` and subsequent `RUN` command allow for optional installation of Git based on the value of `INSTALL_GIT`.\n\n5. **Cleanup**:\n   - `RUN rm -rf /var/lib/apt/lists/*`: Cleans up the package list to reduce image size.\n\n6. **Working Directory**:\n   - `WORKDIR /app`: Sets the working directory inside the container to `/app`.\n\n7. **Copying Files**:\n   - `COPY . /app`: Copies the contents of the current directory on the host to the `/app` directory in the container.\n\n8. **Install Python Packages**:\n   - The `RUN pip --no-cache-dir install` command installs `markitdown` and its sample plugin from the copied files.\n\n9. **User Configuration**:\n   - `ARG USERID=nobody` and `ARG GROUPID=nogroup`: Set default user and group IDs for running the container.\n\n10. **Entrypoint**:\n    - `ENTRYPOINT [ \"markitdown\" ]`: Specifies that the container will run the `markitdown` command by default when started.\n\n#### Step 2: Build the Docker Image\n\nRun the following command in the terminal from the directory containing the `Dockerfile`:\n\n```bash\ndocker build -t markitdown:latest .\n```\n\nThis command builds the Docker image and tags it as `markitdown:latest`.\n\n#### Step 3: Run the Docker Container\n\nTo execute the `markitdown` command, use the following command, replacing `~/your-file.pdf` with the path to your PDF file:\n\n```bash\ndocker run --rm -i markitdown:latest < ~/your-file.pdf > output.md\n```\n\nThis command runs the container, processes the input PDF, and outputs the result to `output.md`. The `--rm` flag ensures that the container is removed after execution.\n\n### Summary\n\nBy following these steps, you will successfully build and run the Docker container for the `microsoft/markitdown` project, enabling you to convert PDF files to Markdown format efficiently.", ".dockerignore": "### Building and Running Docker with `.dockerignore` in the `microsoft/markitdown` Repository\n\n#### Semantic Meaning of `.dockerignore`\n\nThe `.dockerignore` file is used to specify which files and directories should be excluded from the Docker build context. In the provided `.dockerignore` content:\n\n```\n*\n!packages/\n```\n\n- `*` indicates that all files and directories in the context should be ignored by default.\n- `!packages/` is an exception that allows the `packages` directory to be included in the build context.\n\nThis means that when you build the Docker image, only the contents of the `packages` directory will be sent to the Docker daemon, optimizing the build process by excluding unnecessary files.\n\n#### Steps to Build and Run Docker\n\n1. **Build the Docker Image:**\n   Open your terminal and navigate to the directory containing the Dockerfile and the `.dockerignore` file. Run the following command to build the Docker image:\n\n   ```bash\n   docker build -t markitdown:latest .\n   ```\n\n   This command builds the Docker image and tags it as `markitdown:latest`.\n\n2. **Run the Docker Container:**\n   After successfully building the image, you can run the container to convert a PDF file to Markdown. Use the following command, replacing `~/your-file.pdf` with the path to your PDF file:\n\n   ```bash\n   docker run --rm -i markitdown:latest < ~/your-file.pdf > output.md\n   ```\n\n   This command runs the `markitdown` container, takes input from your specified PDF file, and outputs the converted Markdown to `output.md`. The `--rm` flag ensures that the container is removed after it exits.\n\n### Summary\n- The `.dockerignore` file optimizes the build context by excluding all files except for the `packages` directory.\n- Use the provided Docker commands to build and run the application effectively."}}, "timestamp": "2025-10-30T20:36:03.772912"}
{"repo_name": "3b1b/manim", "stars": 81542, "language": "Python", "tasks": [{"task_title": "Opening Manim Example", "task_description": "å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Manimåº“åˆ›å»ºä¸€ä¸ªç®€å•çš„åŠ¨ç”»ç¤ºä¾‹ã€‚", "example_code": "from manim import *\n\nclass OpeningManimExample(Scene):\n    def construct(self):\n        title = Text(\"Welcome to Manim!\")\n        self.play(Write(title))\n        self.wait(2)", "running_command": null, "expected_input": null, "expected_output": "A scene displaying the text 'Welcome to Manim!'"}], "setup": {"setup_commands": ["# Install manimgl\npip install manimgl\n\n# Try it out\nmanimgl", "# Install manimgl\npip install -e .\n\n# Try it out\nmanimgl example_scenes.py OpeningManimExample\n# or\nmanim-render example_scenes.py OpeningManimExample", "    git clone https://github.com/3b1b/manim.git\n    cd manim\n    pip install -e .\n    manimgl example_scenes.py OpeningManimExample", "    brew install ffmpeg mactex", "    arch -arm64 brew install pkg-config cairo", "    git clone https://github.com/3b1b/manim.git\n    cd manim\n    pip install -e .\n    manimgl example_scenes.py OpeningManimExample (make sure to add manimgl to path first.)"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:36:14.354509"}
{"repo_name": "hacksider/Deep-Live-Cam", "stars": 74321, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨CUDAæ‰§è¡Œ", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨CUDAä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider cuda", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨CoreMLæ‰§è¡Œ", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨CoreMLä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python3.10 run.py --execution-provider coreml", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨CoreMLæ‰§è¡Œ", "task_description": "è¯¥ä»»åŠ¡å†æ¬¡ä½¿ç”¨CoreMLä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider coreml", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨DirectMLæ‰§è¡Œ", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨DirectMLä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider directml", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨OpenVINOæ‰§è¡Œ", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨OpenVINOä½œä¸ºæ‰§è¡Œæä¾›è€…è¿è¡Œç¨‹åºã€‚", "example_code": null, "running_command": "python run.py --execution-provider openvino", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["git clone https://github.com/hacksider/Deep-Live-Cam.git\ncd Deep-Live-Cam", "python -m venv venv\nvenv\\Scripts\\activate\npip install -r requirements.txt", "# Ensure you use the installed Python 3.10\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt", "# Install Python 3.11 (specific version is important)\nbrew install python@3.11\n\n# Install tkinter package (required for the GUI)\nbrew install python-tk@3.10\n\n# Create and activate virtual environment with Python 3.11\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt", "# Deactivate the virtual environment\nrm -rf venv\n\n# Reinstall the virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# install the dependencies again\npip install -r requirements.txt\n\n# gfpgan and basicsrs issue fix\npip install git+https://github.com/xinntao/BasicSR.git@master\npip uninstall gfpgan -y\npip install git+https://github.com/TencentARC/GFPGAN.git@master", "pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\npip uninstall onnxruntime onnxruntime-gpu\npip install onnxruntime-gpu==1.21.0", "pip uninstall onnxruntime onnxruntime-silicon\npip install onnxruntime-silicon==1.13.1", "pip uninstall onnxruntime onnxruntime-coreml\npip install onnxruntime-coreml==1.21.0", "pip uninstall onnxruntime onnxruntime-directml\npip install onnxruntime-directml==1.21.0", "pip uninstall onnxruntime onnxruntime-openvino\npip install onnxruntime-openvino==1.21.0"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:36:36.458535"}
{"repo_name": "browser-use/browser-use", "stars": 71948, "language": "Python", "tasks": [{"task_title": "Initialize the browser-use project", "task_description": "Initialize a new project for browser-use with the default template.", "example_code": null, "running_command": "uvx browser-use init --template default", "expected_input": null, "expected_output": null}, {"task_title": "Initialize the browser-use project with custom output", "task_description": "Initialize a new project for browser-use with the default template and specify the output file name.", "example_code": null, "running_command": "uvx browser-use init --template default --output my_agent.py", "expected_input": null, "expected_output": null}, {"task_title": "Add browser-use dependency", "task_description": "Add the browser-use library to the project.", "example_code": null, "running_command": "uv add browser-use", "expected_input": null, "expected_output": null}, {"task_title": "Synchronize project dependencies", "task_description": "Synchronize the project dependencies to ensure everything is up to date.", "example_code": null, "running_command": "uv sync", "expected_input": null, "expected_output": null}, {"task_title": "Install the browser-use library", "task_description": "Install the browser-use library using the uvx command.", "example_code": null, "running_command": "uvx browser-use install", "expected_input": null, "expected_output": null}, {"task_title": "Find the number of stars of a GitHub repository", "task_description": "Use the Agent class to find the number of stars of the browser-use repository utilizing a ChatBrowserUse instance.", "example_code": "from browser_use import Agent, Browser, ChatBrowserUse\nimport asyncio\n\nasync def example():\n    browser = Browser()\n    llm = ChatBrowserUse()\n    agent = Agent(\n        task=\"Find the number of stars of the browser-use repo\",\n        llm=llm,\n        browser=browser,\n    )\n    history = await agent.run()\n    return history\n\nif __name__ == \"__main__\":\n    history = asyncio.run(example())", "running_command": null, "expected_input": null, "expected_output": "History object containing the number of stars"}, {"task_title": "Create a custom tool", "task_description": "Define a custom tool using the Tool decorator and use it in the Agent.", "example_code": "@Tool()\ndef custom_tool(param: str) -> str:\n    \"\"\"Description of what this tool does.\"\"\"\n    return f\"Result: {param}\"\n\nagent = Agent(\n    task=\"Your task\",\n    llm=llm,\n    browser=browser,\n    use_custom_tools=[custom_tool],\n)", "running_command": null, "expected_input": "any string parameter for custom_tool", "expected_output": "Result: {input string}"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for `browser-use`\n\nTo build and run the Docker container using the provided `Dockerfile` from the `browser-use` repository, follow these steps:\n\n#### Step 1: Clone the Repository\n```bash\ngit clone https://github.com/browser-use/browser-use.git\ncd browser-use\n```\n\n#### Step 2: Build the Docker Image\n```bash\ndocker build . -t browseruse --no-cache\n```\n- This command builds the Docker image named `browseruse` without using any cached layers, ensuring a fresh build.\n\n#### Step 3: Run the Docker Container\nTo run the container and mount a local directory for data persistence:\n```bash\ndocker run -v \"$PWD/data\":/data browseruse\n```\n- This command mounts the local `data` directory to the `/data` directory in the container.\n\nTo check the version of the `browser-use` application:\n```bash\ndocker run -v \"$PWD/data\":/data browseruse --version\n```\n\n#### Step 4: Multi-Architecture Build (Optional)\nIf you need to build for multiple architectures:\n```bash\ndocker buildx create --use\ndocker buildx build . --platform=linux/amd64,linux/arm64 --push -t browseruse/browseruse:some-tag\n```\n- This command sets up a build environment for multiple architectures and pushes the built image to a specified tag.\n\n### Semantic Meaning of the Dockerfile\nThe `Dockerfile` defines a containerized environment for the `browser-use` application, which automates web tasks for AI agents. Key components include:\n\n- **Base Image**: Starts from `python:3.12-slim`, ensuring a lightweight Python environment.\n- **Labels**: Metadata about the image, including name, maintainer, and documentation links.\n- **Environment Variables**: Configures the container's environment, including timezone, language, and user settings.\n- **User Creation**: Sets up a non-privileged user (`browseruse`) for running the application securely.\n- **Dependency Installation**: Installs necessary system and Python dependencies, including Chromium and Playwright.\n- **Working Directory**: Sets the working directory to `/app`, where the application code resides.\n- **Entrypoint**: Specifies the command to run when the container starts, which is the `browser-use` application.\n\nThis setup ensures that the application runs in a consistent environment, with all dependencies managed and isolated from the host system.", ".dockerignore": "### Building and Running Docker with `.dockerignore` in `browser-use`\n\n#### Overview of `.dockerignore`\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, speeding up the build process and ensuring sensitive or unnecessary files are not included in the Docker image.\n\n#### Semantic Meaning of the Entries\n1. **Documentation and Static Files**: \n   - `docs/` and `static/` are excluded as they are not needed for the application runtime.\n   \n2. **Development and Configuration Files**:\n   - Directories like `.claude/`, `.github/`, and files like `.vscode/`, `.idea/` are excluded to prevent IDE-specific configurations and project management files from being included.\n\n3. **Cache Files**:\n   - Files and directories such as `.DS_Store`, `__pycache__/`, and various cache directories are excluded to avoid unnecessary bloat in the image.\n\n4. **Virtual Environments**:\n   - Excluding `.venv` and `venv/` ensures that local Python environments do not get packaged into the image.\n\n5. **Editor and Build Artifacts**:\n   - Folders like `dist/` and editor-specific files are ignored to keep the image clean.\n\n6. **Data Files**:\n   - Common data file types (e.g., `.gif`, `.txt`, `.pdf`, etc.) are excluded unless they are essential for the application.\n\n7. **Secrets and Sensitive Files**:\n   - Files like `secrets.env`, `.env`, and other sensitive credentials are explicitly excluded to enhance security.\n\n#### Actionable Steps to Build and Run Docker\n\n1. **Ensure Docker is Installed**: Make sure you have Docker installed on your machine. You can download it from [Docker's official site](https://www.docker.com/get-started).\n\n2. **Navigate to the Project Directory**:\n   ```bash\n   cd path/to/browser-use\n   ```\n\n3. **Build the Docker Image**:\n   Use the following command to build the Docker image, replacing `your-image-name` with your desired image name:\n   ```bash\n   docker build -t your-image-name .\n   ```\n\n4. **Run the Docker Container**:\n   After building the image, you can run it using:\n   ```bash\n   docker run -d your-image-name\n   ```\n\n5. **Verify the Container is Running**:\n   Check the running containers with:\n   ```bash\n   docker ps\n   ```\n\nBy following these steps, you will efficiently build and run your Docker container while ensuring that unnecessary and sensitive files are excluded from the build context."}}, "timestamp": "2025-10-30T20:37:26.963556"}
{"repo_name": "pallets/flask", "stars": 70688, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºä¸€ä¸ªç®€å•çš„Flaskåº”ç”¨", "task_description": "è¿™ä¸ªä»»åŠ¡å±•ç¤ºäº†å¦‚ä½•åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„Flaskåº”ç”¨ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªè·¯ç”±æ¥è¿”å›ä¸€ä¸ªç®€å•çš„æ–‡æœ¬å“åº”ã€‚", "example_code": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello, World!\"", "running_command": null, "expected_input": null, "expected_output": "Hello, World!"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:37:54.347203"}
{"repo_name": "sherlock-project/sherlock", "stars": 69963, "language": "Python", "tasks": [{"task_title": "Check a single username", "task_description": "This task checks the availability of a single username across various social networks.", "example_code": null, "running_command": "sherlock user123", "expected_input": "user123", "expected_output": "[{\"username\": \"user123\", \"links\": [\"https://www.1337x.to/user/user123/\", ...]}]"}, {"task_title": "Check multiple usernames", "task_description": "This task checks the availability of multiple usernames across various social networks.", "example_code": null, "running_command": "sherlock user1 user2 user3", "expected_input": "user1 user2 user3", "expected_output": "[{\"username\": \"user1\", \"links\": [...]}, {\"username\": \"user2\", \"links\": [...]}, {\"username\": \"user3\", \"links\": [...]}]"}, {"task_title": "Display help information", "task_description": "This task displays the help message and usage information for the Sherlock tool.", "example_code": null, "running_command": "sherlock --help", "expected_input": null, "expected_output": "usage: sherlock [-h] [--version] [--verbose] ..."}, {"task_title": "Check usernames using JSON input", "task_description": "This task checks the availability of usernames by providing a JSON formatted input.", "example_code": null, "running_command": "echo '{\"usernames\":[\"user123\"]}' | apify call -so netmilk/sherlock", "expected_input": "{\"usernames\":[\"user123\"]}", "expected_output": "[{\"username\": \"user123\", \"links\": [\"https://www.1337x.to/user/user123/\", ...]}]"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for the Sherlock Project\n\nTo build and run the Docker image for the Sherlock project using the provided `Dockerfile`, follow these steps:\n\n#### 1. Update Version Information\nBefore building the Docker image, ensure you update the following in the `Dockerfile`:\n- **VERSION_TAG**: Set this to match the version specified in `sherlock/__init__.py`.\n- **VCS_REF**: Update this to reflect the full commit hash of the tagged version.\n\n#### 2. Build the Docker Image\nRun the following command to build the Docker image, tagging it with both the specific version and `latest`:\n\n```bash\ndocker build -t sherlock/sherlock:<VERSION_TAG> -t sherlock/sherlock:latest .\n```\nReplace `<VERSION_TAG>` with the actual version number you updated in the Dockerfile.\n\n#### 3. Understanding the Dockerfile\n- **Base Image**: The Dockerfile uses `python:3.12-slim-bullseye` as the base image for both the build and runtime stages.\n- **Working Directory**: The working directory is set to `/sherlock`, where the application will reside.\n- **Package Installation**: The `pip3 install` command installs the Sherlock project using the specified version tag, ensuring the latest version of pip is used without caching.\n- **Environment Variables**: The `SHERLOCK_ENV` variable is set to `docker`, indicating the environment context.\n- **Labels**: Metadata about the image is added using labels, including version, VCS reference, and project URL.\n- **Entry Point**: The container will execute the `sherlock` command by default when it starts.\n\n#### 4. Running the Docker Container\nAfter building the image, you can run the container using:\n\n```bash\ndocker run --rm sherlock/sherlock:latest [options]\n```\nReplace `[options]` with any command-line arguments you want to pass to the Sherlock application.\n\n### Related README Docker Commands\nRefer to the projectâ€™s README for additional Docker commands and usage examples specific to the Sherlock project.", ".dockerignore": "### Building and Running Docker with `.dockerignore` in the Sherlock Project\n\n#### Purpose of `.dockerignore`\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the Docker image and speeds up the build process by preventing unnecessary files from being sent to the Docker daemon.\n\n#### Semantic Meaning of the `.dockerignore` File\n- **`.git/`**: Excludes the entire Git repository, preventing version control files from being included.\n- **`.vscode/`**: Excludes Visual Studio Code settings and configurations.\n- **`screenshot/`**: Excludes any screenshots that are not needed for the application.\n- **`tests/`**: Excludes test files, which are typically not required for production builds.\n- **`*.txt`**: Excludes all text files, except those explicitly allowed.\n- **`!/requirements.txt`**: This negation rule allows the inclusion of the `requirements.txt` file, which is essential for installing dependencies.\n- **`venv/`**: Excludes the virtual environment directory, which is not needed in the Docker image.\n- **`devel/`**: Excludes development-related files or directories.\n\n#### Building and Running Docker\n1. **Build the Docker Image**:\n   Navigate to the root of the `sherlock-project` repository and run the following command:\n   ```bash\n   docker build -t sherlock .\n   ```\n\n2. **Run the Docker Container**:\n   After building the image, run the container using:\n   ```bash\n   docker run -it --rm sherlock\n   ```\n\n### Summary\nUtilize the `.dockerignore` file to optimize your Docker builds by excluding unnecessary files, while ensuring essential files like `requirements.txt` are included. Follow the provided commands to build and run the Docker container for the Sherlock project."}}, "timestamp": "2025-10-30T20:38:33.599035"}
{"repo_name": "xtekky/gpt4free", "stars": 65456, "language": "Python", "tasks": [{"task_title": "è¿è¡Œå›¾å½¢ç”¨æˆ·ç•Œé¢", "task_description": "å¯åŠ¨g4fåº“çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚", "example_code": "from g4f.gui import run_gui\nrun_gui()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨CLIå·¥å…·å›¾å½¢ç•Œé¢", "task_description": "é€šè¿‡CLIå¯åŠ¨g4fåº“çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ŒæŒ‡å®šç«¯å£å’Œè°ƒè¯•æ¨¡å¼ã€‚", "example_code": null, "running_command": "python -m g4f.cli gui --port 8080 --debug", "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨CLIå·¥å…·", "task_description": "é€šè¿‡CLIå¯åŠ¨g4fåº“ï¼ŒæŒ‡å®šç«¯å£å’Œè°ƒè¯•æ¨¡å¼ã€‚", "example_code": null, "running_command": "python -m g4f --port 8080 --debug", "expected_input": null, "expected_output": null}, {"task_title": "èŠå¤©ç”Ÿæˆ", "task_description": "ä½¿ç”¨g4fåº“çš„å®¢æˆ·ç«¯ç”ŸæˆèŠå¤©å›å¤ï¼ŒæŒ‡å®šæ¨¡å‹å’Œç”¨æˆ·æ¶ˆæ¯ã€‚", "example_code": "from g4f.client import Client\n\nclient = Client()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    web_search=False\n)\nprint(response.choices[0].message.content)", "running_command": null, "expected_input": "Hello, how are you?", "expected_output": "..."}, {"task_title": "å›¾åƒç”Ÿæˆ", "task_description": "ä½¿ç”¨g4fåº“çš„å®¢æˆ·ç«¯ç”Ÿæˆå›¾åƒï¼ŒæŒ‡å®šæ¨¡å‹å’Œæç¤ºå†…å®¹ã€‚", "example_code": "from g4f.client import Client\n\nclient = Client()\nresponse = client.images.generate(\n    model=\"flux\",\n    prompt=\"a white siamese cat\",\n    response_format=\"url\"\n)\nprint(f\"Generated image URL: {response.data[0].url}\")", "running_command": null, "expected_input": "a white siamese cat", "expected_output": "Generated image URL: ..."}, {"task_title": "å¼‚æ­¥èŠå¤©ç”Ÿæˆ", "task_description": "ä½¿ç”¨g4fåº“çš„å¼‚æ­¥å®¢æˆ·ç«¯ç”ŸæˆèŠå¤©å›å¤ï¼ŒæŒ‡å®šæ¨¡å‹å’Œç”¨æˆ·æ¶ˆæ¯ã€‚", "example_code": "from g4f.client import AsyncClient\nimport asyncio\n\nasync def main():\n    client = AsyncClient()\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing briefly\"}],\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())", "running_command": null, "expected_input": "Explain quantum computing briefly", "expected_output": "..."}], "setup": {"setup_commands": ["pip install -U g4f[all]", "git clone https://github.com/xtekky/gpt4free.git\ncd gpt4free\npip install -r requirements.txt\npip install -e .", "pip install -U g4f[all]"], "docker_commands": ["   docker pull hlohaus789/g4f", "   docker run -p 8080:8080 -p 7900:7900 \\\n     --shm-size=\"2g\" \\\n     -v ${PWD}/har_and_cookies:/app/har_and_cookies \\\n     -v ${PWD}/generated_media:/app/generated_media \\\n     hlohaus789/g4f:latest", "mkdir -p ${PWD}/har_and_cookies ${PWD}/generated_media\nchown -R 1000:1000 ${PWD}/har_and_cookies ${PWD}/generated_media\n\ndocker run \\\n  -p 1337:8080 -p 8080:8080 \\\n  -v ${PWD}/har_and_cookies:/app/har_and_cookies \\\n  -v ${PWD}/generated_media:/app/generated_media \\\n  hlohaus789/g4f:latest-slim"], "has_docker_files": true, "docker_setup_descriptions": {"docker-compose.yml": "### Building and Running Docker with `docker-compose.yml` for `xtekky/gpt4free`\n\nTo build and run the Docker container for the `gpt4free` service using the provided `docker-compose.yml` file, follow these steps:\n\n1. **Understanding the `docker-compose.yml` File:**\n   - **Version:** Specifies the Docker Compose file format version (3 in this case).\n   - **Services:** Defines the services that will be run. Here, we have a single service named `gpt4free`.\n     - **Image:** Specifies the Docker image to use (`hlohaus789/g4f:latest`).\n     - **shm_size:** Allocates shared memory size (2GB) for the container, which is important for applications that require significant memory.\n     - **Build:** Indicates that the service should be built from a Dockerfile located in the `docker` directory of the current context.\n     - **Volumes:** Mounts the current directory (`.`) to `/app` in the container, allowing for persistent data storage and access to local files.\n     - **Ports:** Maps host ports to container ports:\n       - `8080` on the host to `8080` in the container (main service).\n       - `1337` on the host to `8080` in the container (alternative access).\n       - `7900` on the host to `7900` in the container (additional service).\n     - **Environment:** Sets the environment variable `OLLAMA_HOST` to `host.docker.internal`, enabling the container to access the host machine.\n\n2. **Building and Running the Docker Container:**\n   - **Prerequisites:** Ensure you have Docker and Docker Compose installed on your machine.\n   - **Clone the Repository:**\n     ```bash\n     git clone https://github.com/xtekky/gpt4free.git\n     cd gpt4free\n     ```\n   - **Create Required Directories:**\n     ```bash\n     mkdir -p ${PWD}/har_and_cookies ${PWD}/generated_media\n     chown -R 1000:1000 ${PWD}/har_and_cookies ${PWD}/generated_media\n     ```\n   - **Build and Start the Service:**\n     ```bash\n     docker-compose up --build\n     ```\n\n3. **Accessing the Application:**\n   - Once the container is running, you can access the application via:\n     - `http://localhost:8080` for the main service.\n     - `http://localhost:1337` for the alternative access.\n     - `http://localhost:7900` for the additional service.\n\nBy following these steps, you will successfully build and run the `gpt4free` service using Docker Compose, allowing you to leverage the functionalities provided by the application."}}, "timestamp": "2025-10-30T20:39:07.894445"}
{"repo_name": "OpenHands/OpenHands", "stars": 64582, "language": "Python", "tasks": [{"task_title": "Launch the GUI server", "task_description": "å¯åŠ¨OpenHandsçš„å›¾å½¢ç”¨æˆ·ç•Œé¢æœåŠ¡å™¨ã€‚", "example_code": null, "running_command": "uvx --python 3.12 --from openhands-ai openhands serve", "expected_input": null, "expected_output": "GUI server is running"}, {"task_title": "Launch the CLI", "task_description": "å¯åŠ¨OpenHandsçš„å‘½ä»¤è¡Œç•Œé¢ã€‚", "example_code": null, "running_command": "uvx --python 3.12 --from openhands-ai openhands", "expected_input": null, "expected_output": "CLI is running"}], "setup": {"setup_commands": [], "docker_commands": ["docker pull docker.openhands.dev/openhands/runtime:0.60-nikolaik\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:0.60-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands:/.openhands \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.openhands.dev/openhands/openhands:0.60"], "has_docker_files": true, "docker_setup_descriptions": {"docker-compose.yml": "### Building and Running Docker with `docker-compose.yml` for OpenHands\n\nTo build and run the OpenHands application using the provided `docker-compose.yml` file, follow these steps:\n\n1. **Prerequisites**:\n   - Ensure you have Docker and Docker Compose installed on your machine.\n\n2. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/OpenHands/OpenHands.git\n   cd OpenHands\n   ```\n\n3. **Build and Start the Application**:\n   Run the following command in the directory containing the `docker-compose.yml` file:\n   ```bash\n   docker-compose up --build\n   ```\n\n### Semantic Meaning of the `docker-compose.yml` File\n\n- **services**: Defines the services that make up your application. In this case, there is one service named `openhands`.\n\n- **openhands**: This is the main service for the OpenHands application.\n  - **build**: Specifies how to build the Docker image.\n    - **context**: The directory containing the Dockerfile (current directory `./`).\n    - **dockerfile**: The path to the Dockerfile used for building the image (`./containers/app/Dockerfile`).\n  - **image**: Names the built image (`openhands:latest`).\n  - **container_name**: Sets the name of the container, allowing for dynamic naming based on the `DATE` environment variable.\n  - **environment**: Defines environment variables for the container.\n    - `SANDBOX_RUNTIME_CONTAINER_IMAGE`: Specifies the image for the sandbox runtime, with a default value.\n    - `WORKSPACE_MOUNT_PATH`: Sets the path for the workspace mount, defaulting to the current directory's `workspace`.\n  - **ports**: Maps port 3000 on the host to port 3000 on the container.\n  - **extra_hosts**: Adds a host entry to allow the container to communicate with the host.\n  - **volumes**: Mounts necessary volumes for Docker socket access and user data.\n    - `/var/run/docker.sock`: Allows the container to communicate with the Docker daemon.\n    - `~/.openhands`: Mounts the user's OpenHands configuration directory.\n    - `${WORKSPACE_BASE}`: Maps the workspace directory for shared access.\n  - **pull_policy**: Indicates that the image should be built rather than pulled from a registry.\n  - **stdin_open**: Keeps the standard input open for the container.\n  - **tty**: Allocates a pseudo-TTY, useful for interactive applications.\n\n### Related Docker Commands\n\nIf you prefer to run the application without Docker Compose, you can use the following command:\n\n```bash\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:0.60-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands:/.openhands \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.openhands.dev/openhands/openhands:0.60\n```\n\nThis command runs the OpenHands application in a Docker container with the necessary environment variables and volume mounts.", ".dockerignore": "### Building and Running Docker with OpenHands\n\nTo build and run the OpenHands application using Docker, follow these steps, ensuring you understand the purpose of the `.dockerignore` file included in the repository.\n\n#### Understanding `.dockerignore`\n\nThe `.dockerignore` file specifies which files and directories should be excluded from the Docker build context. This helps to reduce the size of the Docker image and speeds up the build process by preventing unnecessary files from being sent to the Docker daemon. The contents of the `.dockerignore` file for the OpenHands repository are as follows:\n\n- **NodeJS Dependencies**: Excludes `frontend/node_modules` to avoid including local dependencies in the image.\n- **Configuration Files**: Excludes all `.ini` and `.yml` files but includes `pyproject.toml`, ensuring only essential configuration is included.\n- **Documentation**: Excludes all markdown files except `README.md`, keeping only the primary documentation.\n- **Hidden Files**: Excludes all hidden files and directories (those starting with a dot) and `__pycache__` directories to avoid unnecessary clutter.\n- **Unneeded Directories**: Excludes specific directories like `/dev_config/`, `/docs/`, `/evaluation/`, `/tests/`, and the file `CITATION.cff`, which are not required for the runtime environment.\n\n#### Steps to Build and Run Docker\n\n1. **Pull the Docker Image**:\n   Execute the following command to pull the required Docker image:\n   ```bash\n   docker pull docker.openhands.dev/openhands/runtime:0.60-nikolaik\n   ```\n\n2. **Run the Docker Container**:\n   Use the command below to run the OpenHands application in a Docker container:\n   ```bash\n   docker run -it --rm --pull=always \\\n       -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:0.60-nikolaik \\\n       -e LOG_ALL_EVENTS=true \\\n       -v /var/run/docker.sock:/var/run/docker.sock \\\n       -v ~/.openhands:/.openhands \\\n       -p 3000:3000 \\\n       --add-host host.docker.internal:host-gateway \\\n       --name openhands-app \\\n       docker.openhands.dev/openhands/openhands:0.60\n   ```\n\n#### Notes:\n- Ensure Docker is installed and running on your machine.\n- Adjust volume mounts (`-v`) as necessary for your environment.\n- The `--add-host` flag allows the container to access the host's network, which may be necessary for certain configurations.\n\nBy following these steps, you can effectively build and run the OpenHands application using Docker while adhering to best practices in managing your build context with the `.dockerignore` file."}}, "timestamp": "2025-10-30T20:39:44.985952"}
{"repo_name": "PaddlePaddle/PaddleOCR", "stars": 62062, "language": "Python", "tasks": [{"task_title": "PP-OCRv5 Inference", "task_description": "ä½¿ç”¨PP-OCRv5è¿›è¡ŒOCRæ¨ç†ï¼Œå¤„ç†å›¾åƒå¹¶æå–æ–‡æœ¬ä¿¡æ¯ã€‚", "example_code": null, "running_command": "paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png", "expected_output": "æå–çš„æ–‡æœ¬ä¿¡æ¯"}, {"task_title": "PP-StructureV3 Inference", "task_description": "ä½¿ç”¨PP-StructureV3è¿›è¡Œæ–‡æ¡£ç»“æ„åˆ†æï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯ã€‚", "example_code": null, "running_command": "paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png", "expected_output": "ç»“æ„åŒ–çš„æ–‡æ¡£ä¿¡æ¯"}, {"task_title": "PP-ChatOCRv4 Inference", "task_description": "ä½¿ç”¨PP-ChatOCRv4è¿›è¡ŒOCRæ¨ç†ï¼Œå¹¶åˆ©ç”¨Qianfan APIè¿›è¡Œæ–‡æœ¬åˆ†æã€‚", "example_code": null, "running_command": "paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png", "expected_output": "åŸºäºé©¾é©¶å®¤å‡†ä¹˜äººæ•°çš„åˆ†æç»“æœ"}, {"task_title": "PaddleOCR-VL Inference", "task_description": "ä½¿ç”¨PaddleOCR-VLè¿›è¡Œæ–‡æ¡£è§£æï¼Œå¤„ç†å›¾åƒå¹¶æå–ä¿¡æ¯ã€‚", "example_code": null, "running_command": "paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png", "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png", "expected_output": "æ–‡æ¡£è§£æç»“æœ"}, {"task_title": "Initialize PaddleOCR Instance", "task_description": "åˆå§‹åŒ–PaddleOCRå®ä¾‹ï¼Œè®¾ç½®å‚æ•°å¹¶è¿›è¡ŒOCRæ¨ç†ã€‚", "example_code": "from paddleocr import PaddleOCR\nocr = PaddleOCR(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False,\n    use_textline_orientation=False)\nresult = ocr.predict(\n    input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png\")", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png", "expected_output": "æå–çš„æ–‡æœ¬ä¿¡æ¯"}, {"task_title": "PPStructureV3 Prediction", "task_description": "ä½¿ç”¨PPStructureV3è¿›è¡Œå›¾åƒåˆ†æï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯å¹¶ä¿å­˜ã€‚", "example_code": "from paddleocr import PPStructureV3\npipeline = PPStructureV3(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False\n)\noutput = pipeline.predict(\n    input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png\",\n)\nfor res in output:\n    res.print() \n    res.save_to_json(save_path=\"output\")", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png", "expected_output": "ç»“æ„åŒ–çš„æ–‡æ¡£ä¿¡æ¯"}, {"task_title": "PPChatOCRv4 Visual Prediction", "task_description": "ä½¿ç”¨PPChatOCRv4è¿›è¡Œè§†è§‰é¢„æµ‹ï¼Œæå–ä¿¡æ¯å¹¶è¿›è¡ŒèŠå¤©äº¤äº’ã€‚", "example_code": "from paddleocr import PPChatOCRv4Doc\npipeline = PPChatOCRv4Doc(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False\n)\nvisual_predict_res = pipeline.visual_predict(\n    input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png\",\n    use_common_ocr=True,\n    use_seal_recognition=True,\n    use_table_recognition=True,\n)", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png", "expected_output": "è§†è§‰é¢„æµ‹ç»“æœ"}, {"task_title": "PaddleOCRVL Prediction", "task_description": "ä½¿ç”¨PaddleOCRVLè¿›è¡Œè§†è§‰æ¨ç†ï¼Œæå–ä¿¡æ¯å¹¶ä¿å­˜ã€‚", "example_code": "from paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png\")\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"output\")\n    res.save_to_markdown(save_path=\"output\")", "running_command": null, "expected_input": "https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png", "expected_output": "è§†è§‰æ¨ç†ç»“æœ"}], "setup": {"setup_commands": ["# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series\npython -m pip install paddleocr\n# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.\n# python -m pip install \"paddleocr[all]\""], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:40:28.466664"}
{"repo_name": "hiyouga/LLaMA-Factory", "stars": 61275, "language": "Python", "tasks": [{"task_title": "åŒæ­¥ä¾èµ–", "task_description": "åŒæ­¥é¢å¤–çš„torchå’Œmetricsä¾èµ–ï¼Œå…è®¸é¢„å‘å¸ƒç‰ˆæœ¬ã€‚", "example_code": null, "running_command": "uv sync --extra torch --extra metrics --prerelease=allow", "expected_input": null, "expected_output": null}, {"task_title": "è®­ç»ƒæ¨¡å‹ï¼ˆé¢„è®­ç»ƒï¼‰", "task_description": "ä½¿ç”¨æŒ‡å®šçš„YAMLé…ç½®æ–‡ä»¶è¿›è¡Œæ¨¡å‹çš„é¢„è®­ç»ƒã€‚", "example_code": null, "running_command": "uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml", "expected_input": null, "expected_output": null}, {"task_title": "è®­ç»ƒæ¨¡å‹ï¼ˆå¾®è°ƒï¼‰", "task_description": "ä½¿ç”¨æŒ‡å®šçš„YAMLé…ç½®æ–‡ä»¶è¿›è¡Œæ¨¡å‹çš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚", "example_code": null, "running_command": "llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml", "expected_input": null, "expected_output": null}, {"task_title": "èŠå¤©æ¨ç†", "task_description": "ä½¿ç”¨æŒ‡å®šçš„YAMLé…ç½®æ–‡ä»¶è¿›è¡ŒèŠå¤©æ¨ç†ã€‚", "example_code": null, "running_command": "llamafactory-cli chat examples/inference/llama3_lora_sft.yaml", "expected_input": null, "expected_output": null}, {"task_title": "å¯¼å‡ºæ¨¡å‹", "task_description": "å¯¼å‡ºç»è¿‡å¾®è°ƒçš„æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„YAMLé…ç½®æ–‡ä»¶ã€‚", "example_code": null, "running_command": "llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml", "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨Web UI", "task_description": "å¯åŠ¨LLaMA Factoryçš„Webç”¨æˆ·ç•Œé¢ã€‚", "example_code": null, "running_command": "llamafactory-cli webui", "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨CUDA Dockerå®¹å™¨", "task_description": "åœ¨CUDAç¯å¢ƒä¸­å¯åŠ¨Dockerå®¹å™¨å¹¶è¿›å…¥bashã€‚", "example_code": null, "running_command": "cd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash", "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨NPU Dockerå®¹å™¨", "task_description": "åœ¨NPUç¯å¢ƒä¸­å¯åŠ¨Dockerå®¹å™¨å¹¶è¿›å…¥bashã€‚", "example_code": null, "running_command": "cd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash", "expected_input": null, "expected_output": null}, {"task_title": "å¯åŠ¨ROCM Dockerå®¹å™¨", "task_description": "åœ¨ROCMç¯å¢ƒä¸­å¯åŠ¨Dockerå®¹å™¨å¹¶è¿›å…¥bashã€‚", "example_code": null, "running_command": "cd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash", "expected_input": null, "expected_output": null}, {"task_title": "APIæ¨ç†", "task_description": "å¯åŠ¨APIæœåŠ¡è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨æŒ‡å®šçš„YAMLé…ç½®æ–‡ä»¶ã€‚", "example_code": null, "running_command": "API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨ModelScope Hub", "task_description": "è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨ModelScope Hubã€‚", "example_code": null, "running_command": "export USE_MODELSCOPE_HUB=1", "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨OpenMind Hub", "task_description": "è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨OpenMind Hubã€‚", "example_code": null, "running_command": "export USE_OPENMIND_HUB=1", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["pip install --upgrade huggingface_hub\nhuggingface-cli login", "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation", "pip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"", "pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl", "# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh", "# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .", "git clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install ."], "docker_commands": ["docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest", "docker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash", "docker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash", "docker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash"], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker for hiyouga/LLaMA-Factory\n\n#### Overview of `.dockerignore`\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, speeding up the build process and preventing unnecessary files from being included in the final image. In the provided `.dockerignore`, the following entries are excluded:\n\n- **Development and configuration files**: `.vscode`, `.git`, `.github`, `.venv`, `.dockerignore`, `.gitattributes`, `.gitignore`\n- **Cache and output directories**: `cache`, `docker`, `saves`, `hf_cache`, `ms_cache`, `om_cache`, `shared_data`, `output`\n\nThese exclusions ensure that only relevant files are included in the Docker image, maintaining a clean and efficient build.\n\n#### Step-by-Step Instructions\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/hiyouga/LLaMA-Factory.git\n   cd LLaMA-Factory\n   ```\n\n2. **Build the Docker Image**:\n   You can build the Docker image using one of the provided Dockerfiles. For example, to build with CUDA support:\n   ```bash\n   docker build -f ./docker/docker-cuda/Dockerfile \\\n       --build-arg PIP_INDEX=https://pypi.org/simple \\\n       --build-arg EXTRAS=metrics \\\n       -t llamafactory:latest .\n   ```\n\n3. **Run the Docker Container**:\n   After building the image, you can run it with GPU support:\n   ```bash\n   docker run -dit --ipc=host --gpus=all \\\n       -p 7860:7860 \\\n       -p 8000:8000 \\\n       --name llamafactory \\\n       llamafactory:latest\n   ```\n\n4. **Access the Running Container**:\n   To execute commands inside the running container:\n   ```bash\n   docker exec -it llamafactory bash\n   ```\n\n5. **Alternative Builds**:\n   If you need to build for NPU or ROCm, use the respective Dockerfiles:\n   ```bash\n   docker build -f ./docker/docker-npu/Dockerfile \\\n       --build-arg PIP_INDEX=https://pypi.org/simple \\\n       --build-arg EXTRAS=torch-npu,metrics \\\n       -t llamafactory:latest .\n\n   docker build -f ./docker/docker-rocm/Dockerfile \\\n       --build-arg PIP_INDEX=https://pypi.org/simple \\\n       --build-arg EXTRAS=metrics \\\n       -t llamafactory:latest .\n   ```\n\n6. **Run with NPU or ROCm**:\n   For running with NPU:\n   ```bash\n   docker run -dit --ipc=host \\\n       -v /usr/local/dcmi:/usr/local/dcmi \\\n       -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n       -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n       -v /etc/ascend_install.info:/etc/ascend_install.info \\\n       -p 7860:7860 \\\n       -p 8000:8000 \\\n       --device /dev/davinci0 \\\n       --device /dev/davinci_manager \\\n       --device /dev/devmm_svm \\\n       --device /dev/hisi_hdc \\\n       --name llamafactory \\\n       llamafactory:latest\n   ```\n\n   For running with ROCm:\n   ```bash\n   docker run -dit --ipc=host \\\n       -p 7860:7860 \\\n       -p 8000:8000 \\\n       --device /dev/kfd \\\n       --device /dev/dri \\\n       --name llamafactory \\\n       llamafactory:latest\n   ```\n\nBy following these steps, you can efficiently build and run the Docker container for the LLaMA-Factory project while ensuring that unnecessary files are excluded from the build context."}}, "timestamp": "2025-10-30T20:41:17.852324"}
{"repo_name": "localstack/localstack", "stars": 61022, "language": "Python", "tasks": [{"task_title": "å¯åŠ¨LocalStack", "task_description": "ä½¿ç”¨LocalStack CLIå¯åŠ¨LocalStackæœåŠ¡ï¼Œè¿è¡Œåœ¨Dockeræ¨¡å¼ä¸‹ã€‚", "example_code": null, "running_command": "localstack start -d", "expected_input": null, "expected_output": "starting LocalStack in Docker mode ğŸ³"}, {"task_title": "æŸ¥çœ‹æœåŠ¡çŠ¶æ€", "task_description": "ä½¿ç”¨LocalStack CLIæŸ¥çœ‹å½“å‰å¯ç”¨æœåŠ¡çš„çŠ¶æ€ã€‚", "example_code": null, "running_command": "localstack status services", "expected_input": null, "expected_output": "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Service                  â”ƒ Status      â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ acm                      â”‚ âœ” available â”‚\nâ”‚ apigateway               â”‚ âœ” available â”‚\nâ”‚ cloudformation           â”‚ âœ” available â”‚\nâ”‚ cloudwatch               â”‚ âœ” available â”‚\nâ”‚ config                   â”‚ âœ” available â”‚\nâ”‚ dynamodb                 â”‚ âœ” available â”‚\n..."}, {"task_title": "åˆ›å»ºSQSé˜Ÿåˆ—", "task_description": "ä½¿ç”¨awslocalå‘½ä»¤åˆ›å»ºä¸€ä¸ªåä¸ºsample-queueçš„SQSé˜Ÿåˆ—ã€‚", "example_code": null, "running_command": "awslocal sqs create-queue --queue-name sample-queue", "expected_input": null, "expected_output": "{\n    \"QueueUrl\": \"http://sqs.us-east-1.localhost.localstack.cloud:4566/000000000000/sample-queue\"\n}"}], "setup": {"setup_commands": ["brew install localstack/tap/localstack-cli", "python3 -m pip install localstack"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for LocalStack\n\nTo build and run a Docker container using the provided `Dockerfile` from the `localstack/localstack` repository, follow these steps:\n\n#### 1. **Understanding the Dockerfile**\n\nThe `Dockerfile` is structured into multiple stages to optimize the build process:\n\n- **Base Stage**: \n  - Uses a slim version of Python 3.13.9 as the foundation.\n  - Installs essential OS packages and runtime dependencies, including `nodejs` and `awscli`.\n  - Sets up the working directory and creates a user (`localstack`) with appropriate permissions.\n\n- **Builder Stage**: \n  - Installs build tools and Python dependencies required for LocalStack.\n  - Copies necessary files and scripts for building the runtime environment.\n\n- **Final Stage**: \n  - Combines the base and builder stages, copying over the built environment and LocalStack code.\n  - Installs LocalStack and generates required files for operation.\n  - Exposes necessary ports and sets health checks for the container.\n\n#### 2. **Building the Docker Image**\n\nTo build the Docker image, navigate to the directory containing the `Dockerfile` and run the following command:\n\n```bash\ndocker build -t localstack:latest .\n```\n\nThis command will create a Docker image tagged as `localstack:latest`.\n\n#### 3. **Running the Docker Container**\n\nOnce the image is built, you can run the LocalStack container using the following command:\n\n```bash\ndocker run -d --name localstack -p 4566:4566 -p 4510-4559:4510-4559 localstack:latest\n```\n\n- `-d`: Runs the container in detached mode.\n- `--name localstack`: Names the container \"localstack\".\n- `-p 4566:4566`: Maps the default LocalStack port.\n- `-p 4510-4559:4510-4559`: Maps additional service ports.\n\n#### 4. **Verifying the Setup**\n\nTo verify that LocalStack is running correctly, you can check the logs:\n\n```bash\ndocker logs localstack\n```\n\nAdditionally, you can check the health status of the LocalStack services:\n\n```bash\ndocker exec localstack localstack status services\n```\n\nThis command should return the status of the LocalStack services, confirming that everything is functioning as expected.\n\n### Conclusion\n\nBy following these steps, you will successfully build and run a Docker container for LocalStack using the provided `Dockerfile`. Ensure you have Docker installed and running on your machine before executing these commands.", "docker-compose.yml": "### Building and Running LocalStack with Docker Compose\n\nTo build and run LocalStack using the provided `docker-compose.yml` file from the `localstack/localstack` repository, follow these steps:\n\n1. **Prerequisites**:\n   - Ensure you have Docker and Docker Compose installed on your machine.\n\n2. **Understanding the `docker-compose.yml` File**:\n   - **services**: Defines the services that will run in containers.\n     - **localstack**: The main service for LocalStack.\n       - **container_name**: Sets the name of the container. It can be overridden by the environment variable `LOCALSTACK_DOCKER_NAME`.\n       - **image**: Specifies the Docker image to use, which is `localstack/localstack`.\n       - **ports**: Maps host ports to container ports:\n         - `4566`: LocalStack Gateway (main entry point for AWS services).\n         - `4510-4559`: A range for various external services provided by LocalStack.\n       - **environment**: Configures environment variables for the container:\n         - `DEBUG`: Controls debug logging; defaults to `0` if not set.\n       - **volumes**: Mounts directories into the container:\n         - `${LOCALSTACK_VOLUME_DIR}`: A directory on the host mapped to `/var/lib/localstack` in the container for persistent storage.\n         - `/var/run/docker.sock`: Allows LocalStack to manage Docker containers.\n\n3. **Running LocalStack**:\n   - Navigate to the directory containing the `docker-compose.yml` file.\n   - Run the following command to start LocalStack:\n     ```bash\n     docker-compose up\n     ```\n   - To run it in detached mode (in the background), use:\n     ```bash\n     docker-compose up -d\n     ```\n\n4. **Stopping LocalStack**:\n   - To stop the running LocalStack service, execute:\n     ```bash\n     docker-compose down\n     ```\n\n### Additional Commands\n- **Viewing Logs**: To view logs from the LocalStack container, use:\n  ```bash\n  docker-compose logs -f\n  ```\n\n- **Accessing the LocalStack CLI**: You can execute commands within the LocalStack container using:\n  ```bash\n  docker-compose exec localstack awslocal <command>\n  ```\n\nThis setup allows you to run LocalStack locally, simulating AWS services for development and testing purposes. Adjust the environment variables as needed to customize your LocalStack instance.", ".dockerignore": "### Building and Running Docker with `.dockerignore` in LocalStack\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, speeding up the build process and minimizing unnecessary files in the final image.\n\n#### Semantic Meaning of the Entries\n\n1. **`.venv*`**: Ignores any virtual environment directories, which are not needed in the Docker image.\n2. **`.filesystem` and `**/.filesystem`**: Excludes filesystem-related files, likely used for local development or testing.\n3. **`tests/aws/**/node_modules`**: Ignores Node.js dependencies in the test directories, preventing unnecessary bloat.\n4. **`tests/aws/**/.terraform`**: Excludes Terraform-related files generated during infrastructure provisioning.\n5. **`**/__pycache__`**: Ignores Python bytecode cache directories, which are not required in the image.\n6. **`target/`**: Excludes build artifacts, typically from Java projects, that are not needed in the final image.\n7. **`htmlcov/`**: Ignores coverage reports generated by testing, which are not necessary for the runtime.\n8. **`.coverage`**: Excludes the coverage data file, similar to `htmlcov/`, to keep the image clean.\n\n#### Steps to Build and Run Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/localstack/localstack.git\n   cd localstack\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image, which will automatically respect the `.dockerignore` file:\n   ```bash\n   docker build -t localstack .\n   ```\n\n3. **Run the Docker Container**:\n   Start the LocalStack container using:\n   ```bash\n   docker run -d -p 4566:4566 -p 4510-4559:4510-4559 localstack\n   ```\n\n#### Related README Docker Commands\n\nRefer to the LocalStack README for additional Docker commands and configurations specific to your use case. Common commands include:\n\n- **Starting LocalStack**:\n  ```bash\n  docker-compose up\n  ```\n\n- **Stopping LocalStack**:\n  ```bash\n  docker-compose down\n  ```\n\nBy following these steps and understanding the purpose of the `.dockerignore` file, you can efficiently build and run LocalStack in a Docker environment."}}, "timestamp": "2025-10-30T20:42:09.570077"}
{"repo_name": "openinterpreter/open-interpreter", "stars": 60742, "language": "Python", "tasks": [{"task_title": "Chat with the interpreter", "task_description": "Executes a single command to chat with the interpreter.", "example_code": "interpreter.chat(\"Plot AAPL and META's normalized stock prices\")", "running_command": null, "expected_input": "Plot AAPL and META's normalized stock prices", "expected_output": "Generated plot of AAPL and META's normalized stock prices."}, {"task_title": "Start an interactive chat", "task_description": "Starts an interactive chat session with the interpreter.", "example_code": "interpreter.chat()", "running_command": null, "expected_input": null, "expected_output": "Interactive chat session initiated."}, {"task_title": "Streaming chat response", "task_description": "Streams chat responses for a given message.", "example_code": "for chunk in interpreter.chat(message, display=False, stream=True):\n  print(chunk)", "running_command": null, "expected_input": "What operating system are we on?", "expected_output": "Chunked response about the operating system."}, {"task_title": "Add subtitles to videos", "task_description": "Executes a command to add subtitles to all videos in a specified directory.", "example_code": "interpreter.chat(\"Add subtitles to all videos in /videos.\")", "running_command": null, "expected_input": "Add subtitles to all videos in /videos.", "expected_output": "Subtitles added to all videos in /videos."}, {"task_title": "Reset and resume chat messages", "task_description": "Resets the interpreter's messages and resumes from a saved state.", "example_code": "messages = interpreter.chat(\"My name is Killian.\")\ninterpreter.messages = []\ninterpreter.messages = messages", "running_command": null, "expected_input": "My name is Killian.", "expected_output": "Interpreter remembers the name 'Killian'."}, {"task_title": "Update system message", "task_description": "Updates the system message with additional instructions.", "example_code": "interpreter.system_message += \"\"\nRun shell commands with -y so the user doesn't have to confirm them.\n\"\"", "running_command": null, "expected_input": null, "expected_output": "System message updated."}, {"task_title": "Set model for interpreter", "task_description": "Sets the model used by the interpreter.", "example_code": "interpreter.llm.model = \"gpt-3.5-turbo\"", "running_command": null, "expected_input": null, "expected_output": "Model set to gpt-3.5-turbo."}, {"task_title": "Chat endpoint in FastAPI", "task_description": "Defines a chat endpoint that streams responses for a given message.", "example_code": "def chat_endpoint(message: str):\n    def event_stream():\n        for result in interpreter.chat(message, stream=True):\n            yield f\"data: {result}\\n\\n\"\n    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")", "running_command": null, "expected_input": "What is the weather today?", "expected_output": "Streaming response for the weather inquiry."}], "setup": {"setup_commands": ["pip install git+https://github.com/OpenInterpreter/open-interpreter.git", "pip install open-interpreter", "pip install fastapi uvicorn\nuvicorn server:app --reload"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for openinterpreter/open-interpreter\n\nTo build and run the Docker container for the openinterpreter/open-interpreter project, follow these steps:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/openinterpreter/open-interpreter.git\n   cd open-interpreter\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image from the provided `Dockerfile`:\n   ```bash\n   docker build -t open-interpreter .\n   ```\n\n3. **Run the Docker Container**:\n   Start the container, mapping port 8000 of the container to port 8000 on your host:\n   ```bash\n   docker run -p 8000:8000 open-interpreter\n   ```\n\n### Semantic Meaning of the Dockerfile\n\n- **Base Image**: The Dockerfile starts with `FROM python:3.11.8`, indicating that it uses Python 3.11.8 as the base image, which is essential for running Python applications.\n\n- **Environment Variables**: \n  - `ENV HOST 0.0.0.0` sets the server host to `0.0.0.0`, allowing the server to be accessed from outside the Docker container.\n\n- **Directory Structure**: \n  - `RUN mkdir -p interpreter scripts` creates necessary directories within the container.\n  - `COPY` commands transfer files from the host into the container, ensuring that the application code and dependencies are available.\n\n- **Port Exposure**: \n  - `EXPOSE 8000` informs Docker that the container listens on port 8000 at runtime, which is crucial for accessing the websocket server.\n\n- **Dependency Installation**: \n  - `RUN pip install \".[server]\"` installs the server dependencies specified in the `pyproject.toml` and `poetry.lock` files, ensuring that the application has all necessary libraries.\n\n- **Entry Point**: \n  - `ENTRYPOINT [\"interpreter\", \"--server\"]` specifies the command that runs when the container starts, launching the LMC-compatible websocket server.\n\nThis Dockerfile effectively sets up an environment for running a websocket server that adheres to the LMC protocol, making it accessible for external connections."}}, "timestamp": "2025-10-30T20:42:37.826177"}
{"repo_name": "FoundationAgents/MetaGPT", "stars": 59149, "language": "Python", "tasks": [{"task_title": "åˆå§‹åŒ–é…ç½®", "task_description": "åˆ›å»ºMetaGPTçš„åˆå§‹é…ç½®æ–‡ä»¶ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚", "example_code": null, "running_command": "metagpt --init-config", "expected_input": null, "expected_output": "åˆ›å»ºäº†~/.metagpt/config2.yamlæ–‡ä»¶"}], "setup": {"setup_commands": ["pip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for FoundationAgents/MetaGPT\n\nTo build and run a Docker container using the provided `Dockerfile` from the FoundationAgents/MetaGPT repository, follow these steps:\n\n#### 1. **Understanding the Dockerfile**\n\n- **Base Image**: The Dockerfile starts with a base image `nikolaik/python-nodejs:python3.9-nodejs20-slim`, which includes both Python 3.9 and Node.js 20 in a lightweight Debian environment.\n  \n- **System Dependencies**: It installs essential Debian packages required by MetaGPT, including libraries for running Chromium and fonts for rendering. This is done in a single `RUN` command to minimize the image size by cleaning up unnecessary files afterward.\n\n- **Global Installation of Mermaid CLI**: The Dockerfile sets environment variables for Chromium and Puppeteer configuration, then installs the Mermaid CLI globally using npm, which is a tool for generating diagrams from text.\n\n- **Application Setup**: The application code is copied into the container, and the working directory is set to `/app/metagpt`. It creates a `workspace` directory and installs Python dependencies listed in `requirements.txt`, as well as the MetaGPT package itself.\n\n- **Container Execution**: The container is configured to run indefinitely using `tail -f /dev/null`, which keeps the container alive for further interaction or processes.\n\n#### 2. **Building the Docker Image**\n\nOpen your terminal and navigate to the directory containing the `Dockerfile`. Run the following command to build the Docker image:\n\n```bash\ndocker build -t metagpt .\n```\n\n- `-t metagpt`: Tags the image with the name `metagpt`.\n- `.`: Indicates that the Dockerfile is in the current directory.\n\n#### 3. **Running the Docker Container**\n\nOnce the image is built, you can run a container using the following command:\n\n```bash\ndocker run -d --name metagpt_container metagpt\n```\n\n- `-d`: Runs the container in detached mode.\n- `--name metagpt_container`: Assigns a name to the running container for easier management.\n\n#### 4. **Verifying the Container is Running**\n\nTo check if your container is running, use:\n\n```bash\ndocker ps\n```\n\nThis command lists all running containers. You should see `metagpt_container` in the list.\n\n#### 5. **Accessing the Container**\n\nIf you need to access the container's shell, you can execute:\n\n```bash\ndocker exec -it metagpt_container /bin/sh\n```\n\nThis allows you to interact with the container directly.\n\n### Summary\n\nBy following these steps, you can successfully build and run the Docker container for the MetaGPT application, ensuring all necessary dependencies and configurations are in place for optimal performance.", ".dockerignore": "### Building and Running Docker with `.dockerignore` in FoundationAgents/MetaGPT\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, improving build performance and security by preventing unnecessary files from being included in the image.\n\n#### Semantic Meaning of the Entries\n\n- **workspace**: Excludes the workspace directory, which may contain local development files that are not needed in the Docker image.\n- **tmp**: Excludes temporary files that could clutter the image and are not required for the application to run.\n- **build**: Excludes build artifacts that are generated during the build process and should not be included in the final image.\n- **dist**: Excludes distribution files that are not necessary for the Docker image, keeping it lightweight.\n- **data**: Excludes data files that may be large or sensitive and are not needed in the image.\n- **geckodriver.log**: Excludes log files generated by geckodriver, which are not relevant for the application runtime.\n\n#### Building and Running Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/FoundationAgents/MetaGPT.git\n   cd MetaGPT\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image. Replace `your-image-name` with your desired image name.\n   ```bash\n   docker build -t your-image-name .\n   ```\n\n3. **Run the Docker Container**:\n   After building the image, run the container using:\n   ```bash\n   docker run -d --name your-container-name your-image-name\n   ```\n\n4. **Verify the Running Container**:\n   Check if the container is running:\n   ```bash\n   docker ps\n   ```\n\n#### Note\nEnsure that the Docker daemon is running on your machine before executing these commands. Adjust any additional parameters as necessary based on your applicationâ€™s requirements."}}, "timestamp": "2025-10-30T20:43:06.274293"}
{"repo_name": "meta-llama/llama", "stars": 58883, "language": "Python", "tasks": [{"task_title": "èŠå¤©å®Œæˆ", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨Llamaæ¨¡å‹è¿›è¡ŒèŠå¤©å®Œæˆï¼ŒåŠ è½½æŒ‡å®šçš„æ£€æŸ¥ç‚¹å’Œåˆ†è¯å™¨ï¼Œå¹¶è®¾ç½®åºåˆ—é•¿åº¦å’Œæ‰¹å¤„ç†å¤§å°ã€‚", "example_code": null, "running_command": "torchrun --nproc_per_node 1 example_chat_completion.py --ckpt_dir llama-2-7b-chat/ --tokenizer_path tokenizer.model --max_seq_len 512 --max_batch_size 6", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["    pip install -e ."], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:43:15.429787"}
{"repo_name": "soimort/you-get", "stars": 56522, "language": "Python", "tasks": [{"task_title": "ä¸‹è½½YouTubeè§†é¢‘", "task_description": "ä½¿ç”¨you-getä»YouTubeä¸‹è½½è§†é¢‘ï¼ŒæŒ‡å®šè§†é¢‘çš„URLã€‚", "example_code": null, "running_command": "you-get", "expected_input": "https://www.youtube.com/watch?v=jNQXAC9IVRw", "expected_output": "Downloading Me at the zoo.webm ...\n 100% (  0.5/  0.5MB) â”œâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”¤[1/1]    6 MB/s\n\nSaving Me at the zoo.en.srt ... Done."}], "setup": {"setup_commands": ["pip install you-get", "[sudo] python -m pip install .", "python -m pip install . --user", "git clone git://github.com/soimort/you-get.git", "brew install you-get", "pip install --upgrade you-get", "pip install --upgrade --force-reinstall git+https://github.com/soimort/you-get@develop"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:43:33.958347"}
{"repo_name": "ultralytics/yolov5", "stars": 55870, "language": "Python", "tasks": [{"task_title": "Load YOLOv5 Model", "task_description": "åŠ è½½ä¸€ä¸ªYOLOv5æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†ã€‚", "example_code": "import torch\n\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")", "running_command": null, "expected_input": null, "expected_output": "æ¨¡å‹å·²åŠ è½½"}, {"task_title": "Perform Inference", "task_description": "å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ¨ç†ï¼Œè‡ªåŠ¨å¤„ç†æ‰¹é‡ã€è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–ã€‚", "example_code": "results = model(\"https://ultralytics.com/images/zidane.jpg\")", "running_command": null, "expected_input": "https://ultralytics.com/images/zidane.jpg", "expected_output": "æ¨ç†ç»“æœå¯¹è±¡"}, {"task_title": "Print Results", "task_description": "å°†æ¨ç†ç»“æœæ‰“å°åˆ°æ§åˆ¶å°ã€‚", "example_code": "results.print()", "running_command": null, "expected_input": null, "expected_output": "æ¨ç†ç»“æœæ‰“å°åˆ°æ§åˆ¶å°"}, {"task_title": "Show Results", "task_description": "åœ¨çª—å£ä¸­æ˜¾ç¤ºæ¨ç†ç»“æœã€‚", "example_code": "results.show()", "running_command": null, "expected_input": null, "expected_output": "æ¨ç†ç»“æœå›¾åƒæ˜¾ç¤º"}, {"task_title": "Save Results", "task_description": "å°†æ¨ç†ç»“æœä¿å­˜åˆ°æŒ‡å®šç›®å½•ã€‚", "example_code": "results.save()", "running_command": null, "expected_input": null, "expected_output": "ç»“æœå·²ä¿å­˜åˆ° runs/detect/exp"}, {"task_title": "Run Inference Using Webcam", "task_description": "ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œæ¨ç†ã€‚", "example_code": null, "running_command": "python detect.py --weights yolov5s.pt --source 0", "expected_input": null, "expected_output": "å®æ—¶æ¨ç†ç»“æœ"}, {"task_title": "Run Inference on Local Image File", "task_description": "å¯¹æœ¬åœ°å›¾åƒæ–‡ä»¶è¿›è¡Œæ¨ç†ã€‚", "example_code": null, "running_command": "python detect.py --weights yolov5s.pt --source img.jpg", "expected_input": "img.jpg", "expected_output": "æ¨ç†ç»“æœ"}, {"task_title": "Train YOLOv5 on COCO", "task_description": "åœ¨COCOæ•°æ®é›†ä¸Šè®­ç»ƒYOLOv5æ¨¡å‹ã€‚", "example_code": null, "running_command": "python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64", "expected_input": null, "expected_output": "è®­ç»ƒå®Œæˆ"}, {"task_title": "Validate the Model", "task_description": "éªŒè¯è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ€§èƒ½ã€‚", "example_code": null, "running_command": "python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640", "expected_input": null, "expected_output": "éªŒè¯ç»“æœ"}, {"task_title": "Run Prediction", "task_description": "å¯¹æŒ‡å®šå›¾åƒè¿›è¡Œé¢„æµ‹ã€‚", "example_code": null, "running_command": "python segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg", "expected_input": "data/images/bus.jpg", "expected_output": "é¢„æµ‹ç»“æœ"}, {"task_title": "Export Model", "task_description": "å°†è®­ç»ƒå¥½çš„æ¨¡å‹å¯¼å‡ºä¸ºONNXæˆ–å…¶ä»–æ ¼å¼ã€‚", "example_code": null, "running_command": "python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0", "expected_input": null, "expected_output": "æ¨¡å‹å·²å¯¼å‡º"}], "setup": {"setup_commands": ["# Install the ultralytics package\npip install ultralytics", "# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` for `ultralytics/yolov5`\n\n#### Overview of `.dockerignore`\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps in reducing the size of the Docker image and improving build performance by avoiding unnecessary files. The entries in this file are similar to those in a `.gitignore`, indicating which files should not be included when building the Docker image.\n\n#### Semantic Meaning of the Entries\n1. **Repo-specific Exclusions**: \n   - Directories like `.git`, `.cache`, and `runs` are excluded to prevent version control and temporary files from bloating the image.\n   - Specific data files and directories related to neural network weights (e.g., `*.pt`, `*.h5`) are excluded to avoid including large model files that may not be needed for the build.\n\n2. **General Python and Environment Exclusions**: \n   - Common Python artifacts (e.g., `__pycache__`, `*.pyc`) and virtual environment directories (e.g., `venv/`, `.venv*`) are excluded to keep the image clean and focused on necessary application code.\n\n3. **IDE and System Files**: \n   - Files and directories related to IDEs (e.g., `.idea/`, `.DS_Store`) and system-specific files are excluded to avoid including user-specific configurations and temporary files.\n\n#### Steps to Build and Run Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/ultralytics/yolov5.git\n   cd yolov5\n   ```\n\n2. **Ensure Docker is Installed**:\n   Make sure you have Docker installed on your machine. You can check by running:\n   ```bash\n   docker --version\n   ```\n\n3. **Build the Docker Image**:\n   Run the following command in the root of the cloned repository:\n   ```bash\n   docker build -t yolov5 .\n   ```\n   This command builds a Docker image named `yolov5`, using the current directory as the build context while respecting the `.dockerignore` file.\n\n4. **Run the Docker Container**:\n   To run the container, use:\n   ```bash\n   docker run --rm -it yolov5\n   ```\n   This command runs the `yolov5` image in an interactive terminal, removing the container after it exits.\n\n5. **Check README for Additional Commands**:\n   Refer to the README file in the repository for any specific Docker commands or usage instructions related to the YOLOv5 model.\n\nBy following these steps, you can efficiently build and run the YOLOv5 Docker container while ensuring that unnecessary files are excluded from the build context."}}, "timestamp": "2025-10-30T20:44:10.417314"}
{"repo_name": "ageitgey/face_recognition", "stars": 55658, "language": "Python", "tasks": [{"task_title": "è·å–äººè„¸ä½ç½®", "task_description": "åŠ è½½å›¾åƒå¹¶è·å–å›¾åƒä¸­æ‰€æœ‰äººè„¸çš„ä½ç½®åæ ‡ã€‚", "example_code": "import face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image)", "running_command": null, "expected_input": "my_picture.jpg", "expected_output": "äººè„¸åæ ‡æ•°ç»„"}, {"task_title": "è·å–äººè„¸ç‰¹å¾ç‚¹", "task_description": "åŠ è½½å›¾åƒå¹¶è·å–æ¯ä¸ªé¢éƒ¨ç‰¹å¾çš„åæ ‡ï¼Œå¦‚çœ¼ç›ã€é¼»å­ç­‰ã€‚", "example_code": "import face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)", "running_command": null, "expected_input": "my_picture.jpg", "expected_output": "é¢éƒ¨ç‰¹å¾ç‚¹æ•°ç»„"}, {"task_title": "äººè„¸ç¼–ç ", "task_description": "åŠ è½½å·²çŸ¥å›¾åƒå’ŒæœªçŸ¥å›¾åƒï¼Œç”Ÿæˆé¢éƒ¨ç¼–ç å¹¶è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯†åˆ«æ˜¯å¦ä¸ºåŒä¸€äººã€‚", "example_code": "import face_recognition\nknown_image = face_recognition.load_image_file(\"biden.jpg\")\nunknown_image = face_recognition.load_image_file(\"unknown.jpg\")\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)", "running_command": null, "expected_input": "biden.jpg, unknown.jpg", "expected_output": "[True/False]"}, {"task_title": "æ‰¹é‡äººè„¸è¯†åˆ«", "task_description": "ä½¿ç”¨CLIå·¥å…·æ‰¹é‡è¯†åˆ«æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æœªçŸ¥äººè„¸ï¼Œå¹¶è¾“å‡ºè¯†åˆ«ç»“æœã€‚", "example_code": null, "running_command": "$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person"}, {"task_title": "äººè„¸æ£€æµ‹", "task_description": "ä½¿ç”¨CLIå·¥å…·æ£€æµ‹æŒ‡å®šæ–‡ä»¶å¤¹ä¸­æ¯å¼ å›¾åƒçš„äººè„¸ä½ç½®ï¼Œå¹¶è¾“å‡ºåæ ‡ã€‚", "example_code": null, "running_command": "$ face_detection  ./folder_with_pictures/", "expected_input": "./folder_with_pictures/", "expected_output": "examples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792"}, {"task_title": "è‡ªå®šä¹‰å®¹å¿åº¦è¿›è¡Œäººè„¸æ¯”è¾ƒ", "task_description": "ä½¿ç”¨CLIå·¥å…·è®¾ç½®å®¹å¿åº¦è¿›è¡Œäººè„¸æ¯”è¾ƒï¼Œè¾“å‡ºè¯†åˆ«ç»“æœã€‚", "example_code": null, "running_command": "$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person"}, {"task_title": "æ˜¾ç¤ºäººè„¸è¯†åˆ«è·ç¦»", "task_description": "ä½¿ç”¨CLIå·¥å…·è¿›è¡Œäººè„¸è¯†åˆ«å¹¶æ˜¾ç¤ºè¯†åˆ«è·ç¦»ã€‚", "example_code": null, "running_command": "$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None"}, {"task_title": "å¹¶è¡Œå¤„ç†äººè„¸è¯†åˆ«", "task_description": "ä½¿ç”¨CLIå·¥å…·è®¾ç½®å¹¶è¡Œå¤„ç†çš„CPUæ•°é‡è¿›è¡Œäººè„¸è¯†åˆ«ã€‚", "example_code": null, "running_command": "$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/", "expected_input": "./pictures_of_people_i_know/, ./unknown_pictures/", "expected_output": "è¯†åˆ«ç»“æœ"}, {"task_title": "æ¯”è¾ƒäººè„¸ç¼–ç ", "task_description": "åŠ è½½å›¾åƒç”Ÿæˆé¢éƒ¨ç¼–ç å¹¶æ¯”è¾ƒä¸¤ä¸ªç¼–ç æ˜¯å¦ä¸ºåŒä¸€äººã€‚", "example_code": "import face_recognition\npicture_of_me = face_recognition.load_image_file(\"me.jpg\")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\nunknown_picture = face_recognition.load_image_file(\"unknown.jpg\")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\nif results[0] == True:\n    print(\"It's a picture of me!\")\nelse:\n    print(\"It's not a picture of me!\")", "running_command": null, "expected_input": "me.jpg, unknown.jpg", "expected_output": "It's a picture of me!/It's not a picture of me!"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for Face Recognition\n\nTo build and run a Docker container using the provided `Dockerfile` from the `ageitgey/face_recognition` repository, follow these steps:\n\n#### Step 1: Clone the Repository\nFirst, clone the repository to your local machine:\n```bash\ngit clone https://github.com/ageitgey/face_recognition.git\ncd face_recognition\n```\n\n#### Step 2: Build the Docker Image\nUse the Docker CLI to build the image from the `Dockerfile`:\n```bash\ndocker build -t face_recognition .\n```\nThis command creates a Docker image named `face_recognition` using the instructions defined in the `Dockerfile`.\n\n#### Step 3: Run the Docker Container\nAfter the image is built, run the container:\n```bash\ndocker run -it --rm face_recognition\n```\nThis command starts a new container from the `face_recognition` image and runs it interactively. The `--rm` flag ensures the container is removed after it stops.\n\n### Semantic Meaning of the Dockerfile\n\n1. **Base Image**: \n   - `FROM python:3.10.3-slim-bullseye`: This line specifies that the image is based on a slim version of Python 3.10.3, which is lightweight and suitable for running Python applications.\n\n2. **System Dependencies**:\n   - The `RUN apt-get` commands install essential system packages and libraries required for building and running the face recognition application. This includes compilers, image processing libraries, and other utilities.\n\n3. **Dlib Installation**:\n   - The Dockerfile clones the `dlib` library from GitHub and installs it. Dlib is a toolkit for machine learning and image processing, crucial for the face recognition functionality.\n\n4. **Application Setup**:\n   - The `COPY` command copies the contents of the local directory into the Docker image. This is where your application code resides.\n   - The subsequent `RUN` command installs any Python dependencies listed in `requirements.txt` and sets up the application.\n\n5. **Command Execution**:\n   - The `CMD` instruction specifies the default command to run when the container starts. In this case, it runs a Python script that recognizes faces in pictures, demonstrating the core functionality of the application.\n\n### Additional Notes\n- If you want to run live webcam examples, you may need to add the line `RUN pip3 install opencv-python==4.1.2.30` to the Dockerfile before the final `CMD` instruction.\n- Ensure Docker is installed and running on your machine before executing these commands.", "docker-compose.yml": "### Building and Running Docker with `docker-compose.yml` for `ageitgey/face_recognition`\n\nTo build and run the Docker container for the `ageitgey/face_recognition` repository using the provided `docker-compose.yml`, follow these steps:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/ageitgey/face_recognition.git\n   cd face_recognition\n   ```\n\n2. **Understand the `docker-compose.yml` File**:\n   - **Version**: Specifies the version of the Docker Compose file format being used (`2.3`).\n   - **Services**: Defines the services that make up your application.\n     - **face_recognition**: This is the main service defined.\n       - **image**: Specifies the Docker image to use (`face_recognition`).\n       - **container_name**: Sets the name of the container to `face_recognition`.\n       - **working_dir**: Sets the working directory inside the container to `/face_recognition/examples`.\n       - **build**: Configures the build context for the Docker image.\n         - **context**: Indicates the directory containing the Dockerfile (current directory).\n         - **dockerfile**: (Commented out) Can be uncommented to use a GPU-enabled Dockerfile if Nvidia-Docker is installed.\n       - **command**: The command to run when the container starts (`python3 -u find_faces_in_picture_cnn.py`), which executes a Python script for face recognition.\n       - **volumes**: Mounts the current directory (`./`) to `/face_recognition` in the container, allowing access to local files.\n       - **runtime**: (Commented out) Can be uncommented to enable GPU support if Nvidia-Docker is used.\n\n3. **Build the Docker Image**:\n   Run the following command to build the Docker image defined in the `docker-compose.yml`:\n   ```bash\n   docker-compose build\n   ```\n\n4. **Run the Docker Container**:\n   Start the container using:\n   ```bash\n   docker-compose up\n   ```\n\n5. **Using GPU (Optional)**:\n   If you want to run the container with GPU support, uncomment the `dockerfile` and `runtime` lines in the `docker-compose.yml`, then rebuild and run the container again.\n\n6. **Stopping the Container**:\n   To stop the running container, use:\n   ```bash\n   docker-compose down\n   ```\n\n### Related Docker Commands from README:\n- Ensure to check the README file for any additional commands or configurations that may be required for specific use cases or setups.\n\nThis concise guide should help you effectively build and run the Docker container for face recognition tasks using the specified `docker-compose.yml`."}}, "timestamp": "2025-10-30T20:45:02.138461"}
{"repo_name": "Textualize/rich", "stars": 54232, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨Richåº“æ‰“å°æ–‡æœ¬", "task_description": "ä½¿ç”¨Richåº“çš„printåŠŸèƒ½æ‰“å°å¸¦æ ·å¼çš„æ–‡æœ¬ã€‚", "example_code": "from rich import print\n\nprint(\"Hello, [bold magenta]World[/bold magenta]!\")", "running_command": null, "expected_input": null, "expected_output": "Hello, World! (with magenta bold styling)"}, {"task_title": "å®‰è£…Prettyæ‰“å°", "task_description": "ä½¿ç”¨Prettyæ¨¡å—å®‰è£…Richåº“çš„Prettyæ‰“å°åŠŸèƒ½ã€‚", "example_code": "from rich import pretty\npretty.install()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "åˆ›å»ºConsoleå¯¹è±¡", "task_description": "åˆ›å»ºä¸€ä¸ªConsoleå¯¹è±¡ä»¥è¿›è¡Œåç»­çš„æ‰“å°æ“ä½œã€‚", "example_code": "from rich.console import Console\n\nconsole = Console()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "æ‰“å°æ–‡æœ¬", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°ç®€å•æ–‡æœ¬ã€‚", "example_code": "console.print(\"Hello\", \"World!\")", "running_command": null, "expected_input": null, "expected_output": "Hello World!"}, {"task_title": "æ‰“å°å¸¦æ ·å¼çš„æ–‡æœ¬", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°å¸¦æ ·å¼çš„æ–‡æœ¬ã€‚", "example_code": "console.print(\"Hello\", \"World!\", style=\"bold red\")", "running_command": null, "expected_input": null, "expected_output": "Hello World! (with bold red styling)"}, {"task_title": "æ‰“å°å¸¦æ ·å¼çš„å¥å­", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°ä¸€ä¸ªåŒ…å«å¤šç§æ–‡æœ¬æ ·å¼çš„å¥å­ã€‚", "example_code": "console.print(\"Where there is a [bold cyan]Will[/bold cyan] there [u]is[/u] a [i]way[/i].\")", "running_command": null, "expected_input": null, "expected_output": "Where there is a Will there is a way. (with various styles)"}, {"task_title": "æ£€æŸ¥å¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•", "task_description": "ä½¿ç”¨inspectæ¨¡å—æ£€æŸ¥ä¸€ä¸ªåˆ—è¡¨çš„å±æ€§å’Œæ–¹æ³•ã€‚", "example_code": "my_list = [\"foo\", \"bar\"]\nfrom rich import inspect\ninspect(my_list, methods=True)", "running_command": null, "expected_input": null, "expected_output": "Output showing methods and properties of list."}, {"task_title": "è®°å½•æ—¥å¿—", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡è®°å½•æ—¥å¿—ä¿¡æ¯ã€‚", "example_code": "console.log(test_data, log_locals=True)", "running_command": null, "expected_input": "[{\"jsonrpc\": \"2.0\", \"method\": \"sum\", \"params\": [None, 1, 2, 4, False, True], \"id\": \"1\"}, ...]", "expected_output": "Log output with local variables."}, {"task_title": "æ‰“å°è¡¨æ ¼", "task_description": "ä½¿ç”¨Tableå¯¹è±¡åˆ›å»ºå¹¶æ‰“å°ä¸€ä¸ªè¡¨æ ¼ã€‚", "example_code": "from rich.table import Table\n\nconsole = Console()\ntable = Table(show_header=True, header_style=\"bold magenta\")\n...console.print(table)", "running_command": null, "expected_input": null, "expected_output": "A formatted table printed in the console."}, {"task_title": "æ˜¾ç¤ºè¿›åº¦æ¡", "task_description": "ä½¿ç”¨trackå‡½æ•°æ˜¾ç¤ºä¸€ä¸ªç®€å•çš„è¿›åº¦æ¡ã€‚", "example_code": "from rich.progress import track\n\nfor step in track(range(100)):\n    do_step(step)", "running_command": null, "expected_input": null, "expected_output": "A progress bar showing completion percentage."}, {"task_title": "æ˜¾ç¤ºä»»åŠ¡çŠ¶æ€", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ˜¾ç¤ºä»»åŠ¡çš„çŠ¶æ€ã€‚", "example_code": "with console.status(\"[bold green]Working on tasks...\") as status:\n    while tasks:\n        task = tasks.pop(0)\n        sleep(1)\n        console.log(f\"{task} complete\")", "running_command": null, "expected_input": null, "expected_output": "Status updates for each task completion."}, {"task_title": "æ‰“å°ç›®å½•å†…å®¹", "task_description": "æ‰“å°æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åã€‚", "example_code": "directory = os.listdir(sys.argv[1])\nprint(Columns(directory))", "running_command": null, "expected_input": "Path to directory", "expected_output": "Formatted list of directory contents."}, {"task_title": "æ‰“å°Markdownæ–‡æœ¬", "task_description": "ä½¿ç”¨Consoleå¯¹è±¡æ‰“å°Markdownæ ¼å¼çš„æ–‡æœ¬ã€‚", "example_code": "with open(\"README.md\") as readme:\n    markdown = Markdown(readme.read())\nconsole.print(markdown)", "running_command": null, "expected_input": "Contents of README.md", "expected_output": "Formatted output of Markdown content."}, {"task_title": "æ‰“å°å¸¦è¯­æ³•é«˜äº®çš„ä»£ç ", "task_description": "ä½¿ç”¨Syntaxå¯¹è±¡æ‰“å°å¸¦è¯­æ³•é«˜äº®çš„ä»£ç ç‰‡æ®µã€‚", "example_code": "syntax = Syntax(my_code, \"python\", theme=\"monokai\", line_numbers=True)\nconsole.print(syntax)", "running_command": null, "expected_input": "Python code string", "expected_output": "Formatted output of the code with syntax highlighting."}], "setup": {"setup_commands": ["python -m pip install rich"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:46:59.680638"}
{"repo_name": "OpenBB-finance/OpenBB", "stars": 54058, "language": "Python", "tasks": [{"task_title": "è·å–å†å²è‚¡ç¥¨ä»·æ ¼", "task_description": "æ­¤ä»»åŠ¡ç”¨äºè·å–æŒ‡å®šè‚¡ç¥¨ï¼ˆå¦‚AAPLï¼‰çš„å†å²ä»·æ ¼æ•°æ®ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºæ•°æ®æ¡†æ ¼å¼ã€‚", "example_code": "from openbb import obb\noutput = obb.equity.price.historical(\"AAPL\")\ndf = output.to_dataframe()", "running_command": null, "expected_input": "AAPL", "expected_output": "ä¸€ä¸ªåŒ…å«AAPLå†å²ä»·æ ¼æ•°æ®çš„æ•°æ®æ¡†"}], "setup": {"setup_commands": ["pip install \"openbb[all]\""], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:47:07.800186"}
{"repo_name": "psf/requests", "stars": 53426, "language": "Python", "tasks": [{"task_title": "HTTP GETè¯·æ±‚", "task_description": "ä½¿ç”¨requestsåº“å‘é€HTTP GETè¯·æ±‚ï¼Œå¹¶è¿›è¡ŒåŸºæœ¬çš„èº«ä»½éªŒè¯ï¼Œè·å–å“åº”çŠ¶æ€ç ã€å¤´ä¿¡æ¯ã€ç¼–ç å’Œå†…å®¹ã€‚", "example_code": "import requests\n\nr = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))\n\nr.status_code\nr.headers['content-type']\nr.encoding\nr.text\nr.json()", "running_command": null, "expected_input": "https://httpbin.org/basic-auth/user/pass", "expected_output": "200, 'application/json; charset=utf8', 'utf-8', '{\"authenticated\": true, ...}', {'authenticated': True, ...}"}], "setup": {"setup_commands": ["$ python -m pip install requests", "git clone -c fetch.fsck.badTimezone=ignore https://github.com/psf/requests.git"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:47:52.064529"}
{"repo_name": "RVC-Boss/GPT-SoVITS", "stars": 51939, "language": "Python", "tasks": [{"task_title": "éŸ³é¢‘åˆ‡ç‰‡", "task_description": "è¯¥ä»»åŠ¡å°†åŸå§‹éŸ³é¢‘æ–‡ä»¶æˆ–ç›®å½•ä¸­çš„éŸ³é¢‘åˆ†å‰²æˆå¤šä¸ªå­éŸ³é¢‘ç‰‡æ®µï¼Œæ”¯æŒæ ¹æ®éŸ³é‡é˜ˆå€¼ã€æœ€å°é•¿åº¦å’Œæœ€å°é—´éš”ç­‰å‚æ•°è¿›è¡Œåˆ‡å‰²ã€‚", "example_code": null, "running_command": "python audio_slicer.py", "expected_input": "<path_to_original_audio_file_or_directory>, <directory_where_subdivided_audio_clips_will_be_saved>, <volume_threshold>, <minimum_duration_of_each_subclip>, <shortest_time_gap_between_adjacent_subclips>, <step_size_for_computing_volume_curve>", "expected_output": "ä¿å­˜åˆ°æŒ‡å®šç›®å½•çš„å¤šä¸ªå­éŸ³é¢‘ç‰‡æ®µ"}, {"task_title": "è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ - FunASR", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨ FunASR è¿›è¡ŒéŸ³é¢‘çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œå°†è¾“å…¥éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬è¾“å‡ºã€‚", "example_code": null, "running_command": "python tools/asr/funasr_asr.py", "expected_input": "<input>", "expected_output": "<output_text>"}, {"task_title": "è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ - FasterWhisper", "task_description": "è¯¥ä»»åŠ¡ä½¿ç”¨ FasterWhisper è¿›è¡ŒéŸ³é¢‘çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œæ”¯æŒæŒ‡å®šè¯­è¨€å’Œç²¾åº¦ã€‚", "example_code": null, "running_command": "python ./tools/asr/fasterwhisper_asr.py", "expected_input": "<input>", "expected_output": "<output_text>"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for GPT-SoVITS\n\nTo build and run the Docker container for the GPT-SoVITS project from the repository `RVC-Boss/GPT-SoVITS`, follow these steps:\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/RVC-Boss/GPT-SoVITS.git\n   cd GPT-SoVITS\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image. This command will utilize the `Dockerfile` in the repository.\n   ```bash\n   docker build -t gpt-sovits .\n   ```\n\n3. **Run the Docker Container**:\n   Start a container from the built image. Adjust the ports as necessary for your application.\n   ```bash\n   docker run -it -p 9871:9871 -p 9872:9872 -p 9873:9873 -p 9874:9874 -p 9880:9880 gpt-sovits\n   ```\n\n### Semantic Meaning of the Dockerfile\n\n- **Base Image**: The Dockerfile starts with a base image `xxxxrt666/torch-base:cu${CUDA_VERSION}-${TORCH_BASE}`, which is a pre-configured environment for PyTorch with CUDA support, specified by the `CUDA_VERSION` and `TORCH_BASE` arguments.\n\n- **Environment Variables**: It sets up several environment variables, including `CUDA_VERSION`, `LITE`, `WORKFLOW`, and `TARGETPLATFORM`, which can be used to customize the build and runtime behavior of the container.\n\n- **Working Directory**: The `WORKDIR` directive sets the working directory to `/workspace/GPT-SoVITS`, where all subsequent commands will be executed.\n\n- **Dependency Installation**: The `RUN` commands execute scripts to install Miniconda and additional Python dependencies specified in `requirements.txt` and `extra-req.txt`.\n\n- **Exposed Ports**: The `EXPOSE` directive indicates that the container will listen on ports 9871 to 9880, which are likely used for the applicationâ€™s services.\n\n- **Python Path**: The `PYTHONPATH` environment variable is set to ensure that Python can find the necessary modules within the workspace.\n\n- **Cleanup and Symlinks**: The `CMD` instruction includes a series of commands to clean up unnecessary files and create symbolic links to model directories, ensuring that the application can access the required resources at runtime.\n\nThis Dockerfile effectively sets up a reproducible environment for running the GPT-SoVITS project, ensuring all dependencies and configurations are in place.", "docker-compose.yaml": "### Building and Running Docker with `docker-compose.yaml` for RVC-Boss/GPT-SoVITS\n\nTo build and run the Docker containers defined in the `docker-compose.yaml` file from the RVC-Boss/GPT-SoVITS repository, follow these steps:\n\n#### Step 1: Install Docker and Docker Compose\nEnsure you have Docker and Docker Compose installed on your machine. You can download them from the official [Docker website](https://www.docker.com/get-started).\n\n#### Step 2: Clone the Repository\nClone the RVC-Boss/GPT-SoVITS repository to your local machine:\n```bash\ngit clone https://github.com/RVC-Boss/GPT-SoVITS.git\ncd GPT-SoVITS\n```\n\n#### Step 3: Review the `docker-compose.yaml` File\nThe `docker-compose.yaml` file defines multiple services for running different versions of the GPT-SoVITS application. Each service is configured with the following key attributes:\n\n- **version**: Specifies the version of the Docker Compose file format (3.8 in this case).\n- **services**: Contains definitions for multiple containers:\n  - **Container Names**: Each service has a unique name (e.g., `GPT-SoVITS-CU126`, `GPT-SoVITS-CU128`).\n  - **Image**: Specifies the Docker image to use for each service, indicating different versions (e.g., `latest-cu126`, `latest-cu128`).\n  - **Ports**: Maps container ports (9871-9880) to the host, allowing access to the application.\n  - **Volumes**: Mounts the current directory and specific model directories to the container for persistent storage and access to necessary files.\n  - **Environment Variables**: Sets `is_half=true`, which may configure the model to use half-precision for performance.\n  - **tty and stdin_open**: Allows interaction with the container's terminal.\n  - **shm_size**: Allocates 16GB of shared memory, which is useful for applications requiring large memory space.\n  - **restart**: Configured to restart the container unless it is explicitly stopped.\n  - **runtime**: Specifies the use of the NVIDIA runtime for GPU support.\n\n#### Step 4: Build and Run the Containers\nExecute the following command in the terminal to build and run the services defined in the `docker-compose.yaml` file:\n```bash\ndocker-compose up -d\n```\nThis command will start all defined services in detached mode.\n\n#### Step 5: Access the Application\nOnce the containers are running, you can access the application through the mapped ports (e.g., `http://localhost:9871`, `http://localhost:9872`, etc.).\n\n#### Step 6: Stopping the Containers\nTo stop the running containers, use:\n```bash\ndocker-compose down\n```\n\n### Additional Notes\n- Ensure your Docker installation supports NVIDIA GPUs if you plan to use the `runtime: nvidia` option.\n- Check the README file in the repository for any additional commands or configurations specific to your use case.\n\nBy following these steps, you will successfully build and run the GPT-SoVITS application using Docker.", ".dockerignore": "### Building and Running Docker with .dockerignore for RVC-Boss/GPT-SoVITS\n\n#### Overview of `.dockerignore`\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps reduce the size of the build context sent to the Docker daemon, improving build performance and security by preventing sensitive or unnecessary files from being included in the Docker image.\n\n#### Semantic Meaning of the Provided `.dockerignore`\n1. **Model Weights and Output**: \n   - `GPT_SoVITS/pretrained_models/*`, `tools/asr/models/*`, `tools/uvr5/uvr5_weights/*`: Excludes pre-trained models and weights that are not needed for building the image.\n   \n2. **Development Artifacts**:\n   - `.git`, `.DS_Store`, `.vscode`, `*.pyc`, `env`, `runtime`, `output`, `logs`, etc.: Excludes version control files, IDE configurations, and temporary files that are not relevant to the application runtime.\n\n3. **Python Specific Files**:\n   - `__pycache__/`, `*.py[cod]`, `*.so`: Excludes compiled Python files and C extensions which are not necessary for the Docker image.\n\n4. **Build Artifacts**:\n   - Directories like `build/`, `dist/`, and files like `*.egg-info/`: Excludes packaging and distribution artifacts.\n\n5. **Testing and Coverage Reports**:\n   - Excludes directories and files related to testing, such as `.tox/`, `.coverage`, and `nosetests.xml`.\n\n6. **Environment and Configuration Files**:\n   - Excludes environment files (`.env`, `venv/`) and configuration files that may contain sensitive information.\n\n7. **Documentation and Caches**:\n   - Excludes documentation build directories and cache files from various tools (e.g., `docs/_build/`, `.mypy_cache/`).\n\n#### Steps to Build and Run Docker\n\n1. **Build the Docker Image**:\n   Navigate to the root directory of the repository where the `Dockerfile` is located and run the following command:\n   ```bash\n   docker build -t gpt-sovits .\n   ```\n\n2. **Run the Docker Container**:\n   After the image is built, run the container using:\n   ```bash\n   docker run -d --name gpt-sovits-container gpt-sovits\n   ```\n\n3. **Verify the Container is Running**:\n   Check the status of the running container:\n   ```bash\n   docker ps\n   ```\n\n4. **Access Logs (if needed)**:\n   To view logs from the container, use:\n   ```bash\n   docker logs gpt-sovits-container\n   ```\n\n5. **Stop and Remove the Container**:\n   When done, stop and remove the container with:\n   ```bash\n   docker stop gpt-sovits-container\n   docker rm gpt-sovits-container\n   ```\n\nBy following these steps and utilizing the `.dockerignore` effectively, you can ensure a clean, efficient Docker build process for the RVC-Boss/GPT-SoVITS project."}}, "timestamp": "2025-10-30T20:48:45.255225"}
{"repo_name": "microsoft/autogen", "stars": 51248, "language": "Python", "tasks": [{"task_title": "ç®€å•é—®å€™", "task_description": "ä½¿ç”¨AssistantAgentä¸OpenAIæ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œæ‰§è¡Œç®€å•çš„é—®å€™ä»»åŠ¡ã€‚", "example_code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    agent = AssistantAgent(\"assistant\", model_client=model_client)\n    print(await agent.run(task=\"Say 'Hello World!'\"))\n    await model_client.close()\n\nasyncio.run(main())", "running_command": null, "expected_input": "Say 'Hello World!'", "expected_output": "'Hello World!'"}, {"task_title": "æ•°å­¦é—®é¢˜æ±‚è§£", "task_description": "ä½¿ç”¨æ•°å­¦ä¸“å®¶åŠ©æ‰‹è§£å†³ç§¯åˆ†é—®é¢˜ã€‚", "example_code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.tools import AgentTool\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    math_agent = AssistantAgent(\n        \"math_expert\",\n        model_client=model_client,\n        system_message=\"You are a math expert.\",\n        description=\"A math expert assistant.\",\n        model_client_stream=True,\n    )\n    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)\n    agent = AssistantAgent(\n        \"assistant\",\n        system_message=\"You are a general assistant. Use expert tools when needed.\",\n        model_client=model_client,\n        model_client_stream=True,\n        tools=[math_agent_tool],\n        max_tool_iterations=10,\n    )\n    await Console(agent.run_stream(task=\"What is the integral of x^2?\"))\n\nasyncio.run(main())", "running_command": null, "expected_input": "What is the integral of x^2?", "expected_output": "The integral of x^2 is (1/3)x^3 + C."}, {"task_title": "åŒ–å­¦é—®é¢˜æ±‚è§£", "task_description": "ä½¿ç”¨åŒ–å­¦ä¸“å®¶åŠ©æ‰‹è§£å†³åˆ†å­é‡é—®é¢˜ã€‚", "example_code": "import asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.tools import AgentTool\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    chemistry_agent = AssistantAgent(\n        \"chemistry_expert\",\n        model_client=model_client,\n        system_message=\"You are a chemistry expert.\",\n        description=\"A chemistry expert assistant.\",\n        model_client_stream=True,\n    )\n    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)\n    agent = AssistantAgent(\n        \"assistant\",\n        system_message=\"You are a general assistant. Use expert tools when needed.\",\n        model_client=model_client,\n        model_client_stream=True,\n        tools=[chemistry_agent_tool],\n        max_tool_iterations=10,\n    )\n    await Console(agent.run_stream(task=\"What is the molecular weight of water?\"))\n\nasyncio.run(main())", "running_command": null, "expected_input": "What is the molecular weight of water?", "expected_output": "The molecular weight of water (H2O) is approximately 18.015 g/mol."}, {"task_title": "å¯åŠ¨AutoGen Studio", "task_description": "é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨AutoGen Studioåº”ç”¨ã€‚", "example_code": null, "running_command": "autogenstudio ui --port 8080 --appdir ./my-app", "expected_input": null, "expected_output": "AutoGen Studio running on http://localhost:8080"}], "setup": {"setup_commands": ["# Install AgentChat and OpenAI client from Extensions\npip install -U \"autogen-agentchat\" \"autogen-ext[openai]\"", "# Install AutoGen Studio for no-code GUI\npip install -U \"autogenstudio\"", "# First run `npm install -g @playwright/mcp@latest` to install the MCP server.\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4.1\")\n    server_params = StdioServerParams(\n        command=\"npx\",\n        args=[\n            \"@playwright/mcp@latest\",\n            \"--headless\",\n        ],\n    )\n    async with McpWorkbench(server_params) as mcp:\n        agent = AssistantAgent(\n            \"web_browsing_assistant\",\n            model_client=model_client,\n            workbench=mcp, # For multiple MCP servers, put them in a list.\n            model_client_stream=True,\n            max_tool_iterations=10,\n        )\n        await Console(agent.run_stream(task=\"Find out how many contributors for the microsoft/autogen repository\"))\n\n\nasyncio.run(main())"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:49:17.296169"}
{"repo_name": "pathwaycom/pathway", "stars": 48952, "language": "Python", "tasks": [{"task_title": "è¯»å–CSVæ•°æ®", "task_description": "è¿æ¥åˆ°CSVæ ¼å¼çš„æ•°æ®æ–‡ä»¶ï¼Œå¹¶å®šä¹‰æ•°æ®çš„æ¨¡å¼ã€‚", "example_code": "import pathway as pw\n\nclass InputSchema(pw.Schema):\n  value: int\n\ninput_table = pw.io.csv.read(\n  \"./input/\",\n  schema=InputSchema\n)", "running_command": null, "expected_input": "./input/", "expected_output": "è¡¨æ ¼æ•°æ®ä»¥InputSchemaå®šä¹‰çš„ç»“æ„åŠ è½½"}, {"task_title": "è¿‡æ»¤æ•°æ®", "task_description": "è¿‡æ»¤å‡ºå€¼å¤§äºç­‰äº0çš„æ•°æ®è¡Œã€‚", "example_code": "filtered_table = input_table.filter(input_table.value>=0)", "running_command": null, "expected_input": "input_table", "expected_output": "ä»…åŒ…å«value >= 0çš„è¡Œ"}, {"task_title": "èšåˆæ•°æ®", "task_description": "å¯¹è¿‡æ»¤åçš„æ•°æ®è¿›è¡Œæ±‚å’Œæ“ä½œï¼Œè®¡ç®—valueçš„æ€»å’Œã€‚", "example_code": "result_table = filtered_table.reduce(\n  sum_value = pw.reducers.sum(filtered_table.value)\n)", "running_command": null, "expected_input": "filtered_table", "expected_output": "åŒ…å«sum_valueå­—æ®µçš„èšåˆç»“æœ"}, {"task_title": "å°†ç»“æœå†™å…¥JSON Linesæ ¼å¼", "task_description": "å°†è®¡ç®—ç»“æœå†™å…¥åˆ°æŒ‡å®šçš„JSON Linesæ–‡ä»¶ä¸­ã€‚", "example_code": "pw.io.jsonlines.write(result_table, \"output.jsonl\")", "running_command": null, "expected_input": "result_table", "expected_output": "è¾“å‡ºåˆ°output.jsonlæ–‡ä»¶"}, {"task_title": "è¿è¡Œè®¡ç®—", "task_description": "æ‰§è¡Œæ‰€æœ‰å®šä¹‰çš„è®¡ç®—æ“ä½œã€‚", "example_code": "pw.run()", "running_command": null, "expected_input": null, "expected_output": "æ‰§è¡Œæ‰€æœ‰æ•°æ®å¤„ç†ä»»åŠ¡"}], "setup": {"setup_commands": ["pip install -U pathway"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` in `pathwaycom/pathway`\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify which files and directories should be excluded from the Docker build context. This helps optimize the build process by reducing the size of the context sent to the Docker daemon, speeding up builds, and preventing unnecessary files from being included in the image.\n\n#### Semantic Meaning of the Provided `.dockerignore` Content\n\n- `target/debug/`: This line excludes the entire `target/debug/` directory from the Docker build context. This is typically where build artifacts are stored during development, and excluding it helps keep the image lightweight.\n  \n- `!target/**/*.whl`: This line is an exception rule that includes all `.whl` (wheel) files located anywhere within the `target` directory and its subdirectories. This is useful for ensuring that any built Python packages are included in the Docker image, even though the rest of the `target/debug/` directory is excluded.\n\n#### Steps to Build and Run Docker\n\n1. **Navigate to the Repository Directory**:\n   ```bash\n   cd pathwaycom/pathway\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image. Replace `your-image-name` with your desired image name.\n   ```bash\n   docker build -t your-image-name .\n   ```\n\n3. **Run the Docker Container**:\n   After building the image, run a container using the following command. Replace `your-image-name` with the name you used in the build step.\n   ```bash\n   docker run -d your-image-name\n   ```\n\n#### Additional Notes\n\n- Ensure that Docker is installed and running on your machine before executing the commands.\n- You may refer to the repository's README for any specific Docker commands or configurations that may be necessary for your application."}}, "timestamp": "2025-10-30T20:49:51.414821"}
{"repo_name": "karpathy/nanoGPT", "stars": 48528, "language": "Python", "tasks": [{"task_title": "å‡†å¤‡èå£«æ¯”äºšå­—ç¬¦æ•°æ®", "task_description": "æ‰§è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ä»¥å¤„ç†èå£«æ¯”äºšå­—ç¬¦æ•°æ®é›†ã€‚", "example_code": null, "running_command": "python data/shakespeare_char/prepare.py", "expected_input": null, "expected_output": "æ•°æ®å‡†å¤‡å®Œæˆçš„æ¶ˆæ¯"}, {"task_title": "è®­ç»ƒèå£«æ¯”äºšå­—ç¬¦æ¨¡å‹", "task_description": "ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒèå£«æ¯”äºšå­—ç¬¦æ¨¡å‹ã€‚", "example_code": null, "running_command": "python train.py config/train_shakespeare_char.py", "expected_input": null, "expected_output": "è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯"}, {"task_title": "ç”Ÿæˆèå£«æ¯”äºšå­—ç¬¦æ ·æœ¬", "task_description": "ç”ŸæˆåŸºäºèå£«æ¯”äºšå­—ç¬¦æ¨¡å‹çš„æ–‡æœ¬æ ·æœ¬ã€‚", "example_code": null, "running_command": "python sample.py --out_dir=out-shakespeare-char", "expected_input": null, "expected_output": "ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬"}, {"task_title": "è®­ç»ƒèå£«æ¯”äºšå­—ç¬¦æ¨¡å‹ï¼ˆå¸¦å‚æ•°ï¼‰", "task_description": "ä½¿ç”¨å¤šç§å‚æ•°è®­ç»ƒèå£«æ¯”äºšå­—ç¬¦æ¨¡å‹ã€‚", "example_code": null, "running_command": "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0", "expected_input": null, "expected_output": "è®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†æ—¥å¿—ä¿¡æ¯"}, {"task_title": "ç”Ÿæˆèå£«æ¯”äºšå­—ç¬¦æ ·æœ¬ï¼ˆå¸¦è®¾å¤‡å‚æ•°ï¼‰", "task_description": "ç”ŸæˆåŸºäºèå£«æ¯”äºšå­—ç¬¦æ¨¡å‹çš„æ–‡æœ¬æ ·æœ¬ï¼Œå¹¶æŒ‡å®šè®¾å¤‡ã€‚", "example_code": null, "running_command": "python sample.py --out_dir=out-shakespeare-char --device=cpu", "expected_input": null, "expected_output": "ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬"}, {"task_title": "å‡†å¤‡OpenWebTextæ•°æ®", "task_description": "æ‰§è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ä»¥å¤„ç†OpenWebTextæ•°æ®é›†ã€‚", "example_code": null, "running_command": "python data/openwebtext/prepare.py", "expected_input": null, "expected_output": "æ•°æ®å‡†å¤‡å®Œæˆçš„æ¶ˆæ¯"}, {"task_title": "åˆ†å¸ƒå¼è®­ç»ƒGPT-2æ¨¡å‹", "task_description": "åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šåˆ†å¸ƒå¼è®­ç»ƒGPT-2æ¨¡å‹ã€‚", "example_code": null, "running_command": "torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py", "expected_input": null, "expected_output": "è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯"}, {"task_title": "åˆ†å¸ƒå¼è®­ç»ƒGPT-2æ¨¡å‹ï¼ˆä¸»èŠ‚ç‚¹ï¼‰", "task_description": "åœ¨ä¸»èŠ‚ç‚¹ä¸Šå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒGPT-2æ¨¡å‹ã€‚", "example_code": null, "running_command": "torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py", "expected_input": null, "expected_output": "è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯"}, {"task_title": "åˆ†å¸ƒå¼è®­ç»ƒGPT-2æ¨¡å‹ï¼ˆå·¥ä½œèŠ‚ç‚¹ï¼‰", "task_description": "åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒGPT-2æ¨¡å‹ã€‚", "example_code": null, "running_command": "torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py", "expected_input": null, "expected_output": "è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯"}, {"task_title": "è¯„ä¼°GPT-2æ¨¡å‹", "task_description": "ä½¿ç”¨ä¸åŒé…ç½®æ–‡ä»¶è¯„ä¼°GPT-2æ¨¡å‹ã€‚", "example_code": null, "running_command": "$ python train.py config/eval_gpt2.py\n$ python train.py config/eval_gpt2_medium.py\n$ python train.py config/eval_gpt2_large.py\n$ python train.py config/eval_gpt2_xl.py", "expected_input": null, "expected_output": "è¯„ä¼°ç»“æœçš„æ—¥å¿—ä¿¡æ¯"}, {"task_title": "å¾®è°ƒèå£«æ¯”äºšæ¨¡å‹", "task_description": "ä½¿ç”¨é…ç½®æ–‡ä»¶å¾®è°ƒèå£«æ¯”äºšæ¨¡å‹ã€‚", "example_code": null, "running_command": "python train.py config/finetune_shakespeare.py", "expected_input": null, "expected_output": "å¾®è°ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯"}, {"task_title": "ç”Ÿæˆæ–‡æœ¬æ ·æœ¬ï¼ˆæŒ‡å®šåˆå§‹æ–‡æœ¬ï¼‰", "task_description": "ä»æŒ‡å®šçš„åˆå§‹æ–‡æœ¬ç”Ÿæˆå¤šä¸ªæ–°æ–‡æœ¬æ ·æœ¬ã€‚", "example_code": null, "running_command": "python sample.py --init_from=gpt2-xl --start=\"What is the answer to life, the universe, and everything?\" --num_samples=5 --max_new_tokens=100", "expected_input": "What is the answer to life, the universe, and everything?", "expected_output": "ç”Ÿæˆçš„æ–‡æœ¬æ ·æœ¬"}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:50:23.043242"}
{"repo_name": "ultralytics/ultralytics", "stars": 48069, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨é¢„è®­ç»ƒYOLOæ¨¡å‹è¿›è¡Œé¢„æµ‹", "task_description": "ä½¿ç”¨é¢„è®­ç»ƒçš„YOLOæ¨¡å‹ï¼ˆä¾‹å¦‚YOLO11nï¼‰å¯¹ç»™å®šå›¾åƒè¿›è¡Œé¢„æµ‹ã€‚", "example_code": null, "running_command": "yolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'", "expected_input": "https://ultralytics.com/images/bus.jpg", "expected_output": "é¢„æµ‹ç»“æœï¼ŒåŒ…æ‹¬æ£€æµ‹åˆ°çš„å¯¹è±¡åŠå…¶ä½ç½®ä¿¡æ¯"}, {"task_title": "åŠ è½½é¢„è®­ç»ƒYOLOæ¨¡å‹", "task_description": "åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„YOLO11næ¨¡å‹ä»¥è¿›è¡Œåç»­æ“ä½œã€‚", "example_code": "from ultralytics import YOLO\n\nmodel = YOLO(\"yolo11n.pt\")", "running_command": null, "expected_input": null, "expected_output": "æ¨¡å‹å¯¹è±¡"}, {"task_title": "è®­ç»ƒYOLOæ¨¡å‹", "task_description": "åœ¨COCO8æ•°æ®é›†ä¸Šè®­ç»ƒYOLOæ¨¡å‹ï¼Œè®¾ç½®è®­ç»ƒçš„è½®æ•°å’Œå›¾åƒå¤§å°ã€‚", "example_code": "train_results = model.train(\n    data=\"coco8.yaml\",\n    epochs=100,\n    imgsz=640,\n    device=\"cpu\",\n)", "running_command": null, "expected_input": "coco8.yaml, 100, 640, cpu", "expected_output": "è®­ç»ƒç»“æœï¼ŒåŒ…æ‹¬è®­ç»ƒæŸå¤±å’Œå…¶ä»–æŒ‡æ ‡"}, {"task_title": "è¯„ä¼°æ¨¡å‹æ€§èƒ½", "task_description": "è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ï¼Œè·å–ç›¸å…³æŒ‡æ ‡ã€‚", "example_code": "metrics = model.val()", "running_command": null, "expected_input": null, "expected_output": "è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚mAPç­‰"}, {"task_title": "åœ¨å›¾åƒä¸Šè¿›è¡Œå¯¹è±¡æ£€æµ‹", "task_description": "å¯¹ç»™å®šçš„å›¾åƒè¿›è¡Œå¯¹è±¡æ£€æµ‹ï¼Œå¹¶æ˜¾ç¤ºç»“æœã€‚", "example_code": "results = model(\"path/to/image.jpg\")\nresults[0].show()", "running_command": null, "expected_input": "path/to/image.jpg", "expected_output": "æ£€æµ‹åˆ°çš„å¯¹è±¡åœ¨å›¾åƒä¸Šçš„å¯è§†åŒ–ç»“æœ"}, {"task_title": "å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼", "task_description": "å°†è®­ç»ƒå¥½çš„æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼Œä»¥ä¾¿äºéƒ¨ç½²ã€‚", "example_code": "path = model.export(format=\"onnx\")", "running_command": null, "expected_input": "onnx", "expected_output": "å¯¼å‡ºæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„"}], "setup": {"setup_commands": ["pip install ultralytics"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` from `ultralytics/ultralytics`\n\n#### Semantic Meaning of `.dockerignore`\n\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, speeding up the build process and preventing unnecessary files from being included in the Docker image. The entries in the `.dockerignore` file are patterns that match files and directories, ensuring that only relevant files are included in the image.\n\n#### Contents of `.dockerignore`\n\n1. **Python Artifacts**: \n   - Excludes compiled Python files (`*.pyc`, `*.pyo`, `*.pyd`, etc.), cache directories (`__pycache__`, `.pytest_cache`, etc.), and build artifacts (`dist`, `build`, etc.) to keep the image clean and lightweight.\n\n2. **Development Files**: \n   - Ignores environment files (`.env`, `.venv`, `env/`, `venv/`, `ENV/`), IDE configuration files (`.idea`, `.vscode`), and temporary files (e.g., `.DS_Store`, `*.swp`, `*.swo`) that are not needed in production.\n\n3. **Project-Specific Logs**: \n   - Excludes log files (`*.log`, `benchmarks.log`, `runs/`) that are generated during development or testing, which are not necessary for the final image.\n\n4. **Dependencies**: \n   - Ignores `node_modules/` to prevent including unnecessary Node.js dependencies if they are not required for the Docker image.\n\n#### Actionable Steps to Build and Run Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/ultralytics/ultralytics.git\n   cd ultralytics\n   ```\n\n2. **Build the Docker Image**:\n   Ensure you are in the root directory of the cloned repository and run:\n   ```bash\n   docker build -t ultralytics .\n   ```\n\n3. **Run the Docker Container**:\n   After successfully building the image, run the container:\n   ```bash\n   docker run -it --rm ultralytics\n   ```\n\n4. **Verify the Build**:\n   Check that the container is running and that the application behaves as expected.\n\n#### Related README Docker Commands\n\nRefer to the repository's README for any specific Docker commands or configurations that may be necessary for your use case. These might include environment variable settings or volume mounts that are essential for the application to function correctly within the Docker container."}}, "timestamp": "2025-10-30T20:50:53.624076"}
{"repo_name": "opendatalab/MinerU", "stars": 47753, "language": "Python", "tasks": [{"task_title": "æ•°æ®å¤„ç†", "task_description": "è¯¥ä»»åŠ¡é€šè¿‡æŒ‡å®šè¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„æ¥å¤„ç†æ•°æ®ï¼Œä½¿ç”¨äº†MinerUå·¥å…·ã€‚", "example_code": null, "running_command": "mineru -p <input_path> -o <output_path>", "expected_input": "<input_path>, <output_path>", "expected_output": "å¤„ç†åçš„æ•°æ®æ–‡ä»¶"}], "setup": {"setup_commands": ["pip install --upgrade pip\npip install uv\nuv pip install -U \"mineru[core]\"", "git clone https://github.com/opendatalab/MinerU.git\ncd MinerU\nuv pip install -e .[core]"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:51:03.979121"}
{"repo_name": "unslothai/unsloth", "stars": 47662, "language": "Python", "tasks": [{"task_title": "åŠ è½½æ•°æ®é›†", "task_description": "ä»æŒ‡å®šçš„URLåŠ è½½LAIONæ•°æ®é›†ï¼Œæ”¯æŒJSONæ ¼å¼ã€‚", "example_code": "from datasets import load_dataset\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")", "running_command": null, "expected_input": "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl", "expected_output": "åŠ è½½çš„è®­ç»ƒæ•°æ®é›†"}, {"task_title": "åŠ è½½é¢„è®­ç»ƒæ¨¡å‹", "task_description": "ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæ”¯æŒ4ä½é‡åŒ–ã€‚", "example_code": "model, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    load_in_8bit = False,\n    load_in_16bit = False,\n    full_finetuning = False,\n)", "running_command": null, "expected_input": "æ¨¡å‹åç§°: unsloth/gpt-oss-20b", "expected_output": "åŠ è½½çš„æ¨¡å‹å’Œåˆ†è¯å™¨å®ä¾‹"}, {"task_title": "æ¨¡å‹è¡¥ä¸ä¸LoRAæƒé‡æ·»åŠ ", "task_description": "å¯¹æ¨¡å‹è¿›è¡Œè¡¥ä¸å¤„ç†ï¼Œå¹¶æ·»åŠ å¿«é€ŸLoRAæƒé‡ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚", "example_code": "model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n    use_rslora = False,\n    loftq_config = None,\n)", "running_command": null, "expected_input": "æ¨¡å‹å¯¹è±¡", "expected_output": "ä¼˜åŒ–åçš„æ¨¡å‹å®ä¾‹"}, {"task_title": "è®­ç»ƒæ¨¡å‹", "task_description": "ä½¿ç”¨SFTTrainerå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé…ç½®è®­ç»ƒå‚æ•°ã€‚", "example_code": "trainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    args = SFTConfig(\n        max_seq_length = max_seq_length,\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 10,\n        max_steps = 60,\n        logging_steps = 1,\n        output_dir = \"outputs\",\n        optim = \"adamw_8bit\",\n        seed = 3407,\n    ),\n)\ntrainer.train()", "running_command": null, "expected_input": "è®­ç»ƒæ•°æ®é›†ã€æ¨¡å‹ã€åˆ†è¯å™¨åŠè®­ç»ƒå‚æ•°", "expected_output": "æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯"}], "setup": {"setup_commands": ["pip install unsloth", "pip install unsloth", "python -m venv unsloth\nsource unsloth/bin/activate\npip install unsloth", "  pip install ninja\n  pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers", "conda create --name unsloth_env \\\n    python=3.11 \\\n    pytorch-cuda=12.1 \\\n    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n    -y\nconda activate unsloth_env\n\npip install unsloth", "  mkdir -p ~/miniconda3\n  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n  rm -rf ~/miniconda3/miniconda.sh\n  ~/miniconda3/bin/conda init bash\n  ~/miniconda3/bin/conda init zsh", "pip install --upgrade pip\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"", "pip install --upgrade pip\npip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"", "pip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"", "wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -", "try: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nimport re\nv = V(re.match(r\"[0-9\\.]{3,}\", torch.__version__).group(0))\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nUSE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI\nif cuda not in (\"11.8\", \"12.1\", \"12.4\", \"12.6\", \"12.8\"): raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v  < V('2.5.1'): x = 'cu{}{}-torch250'\nelif v <= V('2.5.1'): x = 'cu{}{}-torch251'\nelif v  < V('2.7.0'): x = 'cu{}{}-torch260'\nelif v  < V('2.7.9'): x = 'cu{}{}-torch270'\nelif v  < V('2.8.0'): x = 'cu{}{}-torch271'\nelif v  < V('2.8.9'): x = 'cu{}{}-torch280'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nif v > V('2.6.9') and cuda not in (\"11.8\", \"12.6\", \"12.8\"): raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')"], "docker_commands": ["docker run -d -e JUPYTER_PASSWORD=\"mypassword\" \\\n  -p 8888:8888 -p 2222:22 \\\n  -v $(pwd)/work:/workspace/work \\\n  --gpus all \\\n  unsloth/unsloth"], "has_docker_files": false, "docker_setup_descriptions": {"README_commands": "# Guide to Build and Run Docker for unslothai/unsloth\n\n1. **Install Docker**: Ensure Docker is installed on your machine. Follow the installation guide for your operating system from the [Docker website](https://docs.docker.com/get-docker/).\n\n2. **Install NVIDIA Container Toolkit** (if using GPUs): Follow the instructions on the [NVIDIA website](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) to set up the NVIDIA Container Toolkit.\n\n3. **Clone the Repository**: Open your terminal and clone the repository:\n   ```bash\n   git clone https://github.com/unslothai/unsloth.git\n   cd unsloth\n   ```\n\n4. **Create a Work Directory**: Ensure you have a `work` directory in the current path:\n   ```bash\n   mkdir work\n   ```\n\n5. **Run the Docker Container**: Execute the following command to start the Docker container:\n   ```bash\n   docker run -d -e JUPYTER_PASSWORD=\"mypassword\" \\\n     -p 8888:8888 -p 2222:22 \\\n     -v $(pwd)/work:/workspace/work \\\n     --gpus all \\\n     unsloth/unsloth\n   ```\n\n6. **Access Jupyter Notebook**: Open your web browser and navigate to `http://localhost:8888`. Enter the password `mypassword` when prompted.\n\n7. **SSH Access (Optional)**: If you need SSH access, connect using:\n   ```bash\n   ssh -p 2222 user@localhost\n   ```\n\nReplace `user` with the appropriate username for the container.\n\n8. **Stop the Container**: To stop the running container, find its ID with:\n   ```bash\n   docker ps\n   ```\n   Then stop it using:\n   ```bash\n   docker stop <container_id>\n   ```\n\n9. **Remove the Container** (Optional): If you want to remove the container after stopping it:\n   ```bash\n   docker rm <container_id>\n   ```\n\nYou are now set up to use the unslothai/unsloth Docker container!"}}, "timestamp": "2025-10-30T20:51:33.375569"}
{"repo_name": "run-llama/llama_index", "stars": 44984, "language": "Python", "tasks": [{"task_title": "ä½¿ç”¨OpenAI LLMæ„å»ºå‘é‡å­˜å‚¨ç´¢å¼•", "task_description": "ä»æŒ‡å®šæ•°æ®ç›®å½•åŠ è½½æ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨OpenAI LLMæ„å»ºå‘é‡å­˜å‚¨ç´¢å¼•ã€‚", "example_code": "import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)", "running_command": null, "expected_input": "YOUR_DATA_DIRECTORY", "expected_output": "VectorStoreIndexå¯¹è±¡"}, {"task_title": "ä½¿ç”¨Replicate LLMå’ŒHuggingFaceåµŒå…¥æ„å»ºç´¢å¼•", "task_description": "è®¾ç½®Replicate LLMå’ŒHuggingFaceåµŒå…¥æ¨¡å‹ï¼ŒåŠ è½½æ•°æ®å¹¶æ„å»ºå‘é‡å­˜å‚¨ç´¢å¼•ã€‚", "example_code": "import os\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_API_TOKEN\"\n\nfrom llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.replicate import Replicate\nfrom transformers import AutoTokenizer\n\nllama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\nSettings.llm = Replicate(\n    model=llama2_7b_chat,\n    temperature=0.01,\n    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n)\n\nSettings.tokenizer = AutoTokenizer.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\"\n)\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)", "running_command": null, "expected_input": "YOUR_DATA_DIRECTORY", "expected_output": "VectorStoreIndexå¯¹è±¡"}, {"task_title": "æŸ¥è¯¢å‘é‡å­˜å‚¨ç´¢å¼•", "task_description": "ä½¿ç”¨æ„å»ºçš„å‘é‡å­˜å‚¨ç´¢å¼•è¿›è¡ŒæŸ¥è¯¢ã€‚", "example_code": "query_engine = index.as_query_engine()\nquery_engine.query(\"YOUR_QUESTION\")", "running_command": null, "expected_input": "YOUR_QUESTION", "expected_output": "æŸ¥è¯¢ç»“æœ"}, {"task_title": "æŒä¹…åŒ–ç´¢å¼•å­˜å‚¨ä¸Šä¸‹æ–‡", "task_description": "å°†ç´¢å¼•çš„å­˜å‚¨ä¸Šä¸‹æ–‡æŒä¹…åŒ–ä»¥ä¾¿åç»­ä½¿ç”¨ã€‚", "example_code": "index.storage_context.persist()", "running_command": null, "expected_input": null, "expected_output": "æŒä¹…åŒ–æˆåŠŸ"}, {"task_title": "ä»å­˜å‚¨ä¸­åŠ è½½ç´¢å¼•", "task_description": "é‡å»ºå­˜å‚¨ä¸Šä¸‹æ–‡å¹¶ä»å­˜å‚¨ä¸­åŠ è½½ç´¢å¼•ã€‚", "example_code": "from llama_index.core import StorageContext, load_index_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\nindex = load_index_from_storage(storage_context)", "running_command": null, "expected_input": "./storage", "expected_output": "VectorStoreIndexå¯¹è±¡"}, {"task_title": "éªŒè¯é™æ€æ–‡ä»¶", "task_description": "ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·éªŒè¯é™æ€æ–‡ä»¶çš„å®Œæ•´æ€§ã€‚", "example_code": null, "running_command": "#!/bin/bash\nSTATIC_DIR=\"venv/lib/python3.13/site-packages/llama_index/core/_static\"\nREPO=\"run-llama/llama_index\"\n\nfind \"$STATIC_DIR\" -type f | while read -r file; do\n    echo \"Verifying: $file\"\n    gh attestation verify \"$file\" -R \"$REPO\" || echo \"Failed to verify: $file\"\ndone", "expected_input": null, "expected_output": "éªŒè¯æˆåŠŸæˆ–å¤±è´¥ä¿¡æ¯"}], "setup": {"setup_commands": ["# custom selection of integrations to work with core\npip install llama-index-core\npip install llama-index-llms-openai\npip install llama-index-llms-replicate\npip install llama-index-embeddings-huggingface", "cd <desired-package-folder>\npip install poetry\npoetry install --with dev"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:52:12.225293"}
{"repo_name": "coqui-ai/TTS", "stars": 43225, "language": "Python", "tasks": [{"task_title": "åˆ—å‡ºå¯ç”¨çš„TTSæ¨¡å‹", "task_description": "ä½¿ç”¨TTS APIåˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ã€‚", "example_code": "print(TTS().list_models())", "running_command": null, "expected_input": null, "expected_output": "å¯ç”¨æ¨¡å‹åˆ—è¡¨"}, {"task_title": "æ–‡æœ¬è½¬è¯­éŸ³", "task_description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼Œå¹¶è¾“å‡ºä¸ºéŸ³é¢‘æ³¢å½¢æ•°æ®ã€‚", "example_code": "wav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")", "running_command": null, "expected_input": "Hello world!", "expected_output": "éŸ³é¢‘æ³¢å½¢æ•°æ®"}, {"task_title": "æ–‡æœ¬è½¬è¯­éŸ³å¹¶ä¿å­˜åˆ°æ–‡ä»¶", "task_description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æŒ‡å®šçš„éŸ³é¢‘æ–‡ä»¶ä¸­ã€‚", "example_code": "tts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")", "running_command": null, "expected_input": "Hello world!", "expected_output": "ä¿å­˜åˆ°output.wavçš„éŸ³é¢‘æ–‡ä»¶"}, {"task_title": "ä½¿ç”¨ç‰¹å®šæ¨¡å‹è¿›è¡Œæ–‡æœ¬è½¬è¯­éŸ³", "task_description": "ä½¿ç”¨æŒ‡å®šçš„TTSæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚", "example_code": "tts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)", "running_command": null, "expected_input": "Ich bin eine Testnachricht.", "expected_output": "ä¿å­˜åˆ°OUTPUT_PATHçš„éŸ³é¢‘æ–‡ä»¶"}, {"task_title": "å¤šè¯­è¨€è¯­éŸ³å…‹éš†", "task_description": "ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹è¿›è¡Œè¯­éŸ³å…‹éš†ï¼Œå°†ä¸åŒè¯­è¨€çš„æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚", "example_code": "tts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"Isso Ã© clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")", "running_command": null, "expected_input": ["This is voice cloning.", "C'est le clonage de la voix.", "Isso Ã© clonagem de voz."], "expected_output": "ä¿å­˜åˆ°output.wavçš„éŸ³é¢‘æ–‡ä»¶"}, {"task_title": "è¯­éŸ³è½¬æ¢", "task_description": "å°†æºéŸ³é¢‘è½¬æ¢ä¸ºç›®æ ‡éŸ³é¢‘ï¼Œå¹¶ä¿å­˜åˆ°æŒ‡å®šçš„æ–‡ä»¶ä¸­ã€‚", "example_code": "tts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")", "running_command": null, "expected_input": "my/source.wav, my/target.wav", "expected_output": "ä¿å­˜åˆ°output.wavçš„éŸ³é¢‘æ–‡ä»¶"}, {"task_title": "å¸¦æœ‰è¯­éŸ³è½¬æ¢çš„æ–‡æœ¬è½¬è¯­éŸ³", "task_description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³ï¼ŒåŒæ—¶åº”ç”¨è¯­éŸ³è½¬æ¢ï¼Œå¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶ã€‚", "example_code": "tts.tts_with_vc_to_file(\"Wie sage ich auf Italienisch, dass ich dich liebe?\", speaker_wav=\"target/speaker.wav\", file_path=\"output.wav\")", "running_command": null, "expected_input": "Wie sage ich auf Italienisch, dass ich dich liebe?", "expected_output": "ä¿å­˜åˆ°output.wavçš„éŸ³é¢‘æ–‡ä»¶"}], "setup": {"setup_commands": ["pip install TTS", "git clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras"], "docker_commands": ["docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for Coqui-AI TTS\n\nTo build and run the Docker container using the provided `Dockerfile` from the `coqui-ai/TTS` repository, follow these steps:\n\n#### 1. **Understanding the Dockerfile**\n\n- **Base Image**: The Dockerfile starts with an ARG directive that sets the base image to `nvidia/cuda:11.8.0-base-ubuntu22.04`, which is suitable for GPU-accelerated applications.\n  \n- **System Updates and Dependencies**: It updates the package lists and installs essential build tools (`gcc`, `g++`, `make`) and Python-related packages (`python3`, `python3-dev`, `python3-pip`, etc.) necessary for running the TTS application. It also installs `espeak-ng` and `libsndfile1-dev`, which are required for audio processing.\n\n- **Python Package Installation**: The Dockerfile installs `torch` and `torchaudio` from the PyTorch repository with CUDA support, followed by cleaning up the pip cache to reduce image size.\n\n- **Copying Application Code**: It sets the working directory to `/root` and copies the contents of the current directory into the container.\n\n- **Installation Command**: The `make install` command is executed to install the TTS application.\n\n- **Entrypoint and Command**: The container is configured to run the `tts` command by default, with `--help` as the default argument, allowing users to see available options when the container starts.\n\n#### 2. **Building the Docker Image**\n\nRun the following command in the terminal from the directory containing the `Dockerfile`:\n\n```bash\ndocker build -t coqui-tts .\n```\n\nThis command builds the Docker image and tags it as `coqui-tts`.\n\n#### 3. **Running the Docker Container**\n\nTo run the container interactively and access the TTS server, use:\n\n```bash\ndocker run --rm -it -p 5002:5002 coqui-tts\n```\n\nThis command starts the container, removes it after exit (`--rm`), and maps port 5002 from the container to the host, allowing access to the TTS server.\n\n#### 4. **Starting the TTS Server**\n\nOnce inside the container, you can list available models with:\n\n```bash\npython3 TTS/server/server.py --list_models\n```\n\nTo start the TTS server with a specific model, use:\n\n```bash\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits\n```\n\nThis command initializes the TTS server using the specified model.\n\n### Summary\n\nBy following these steps, you can successfully build and run the Coqui-AI TTS Docker container, allowing you to utilize the text-to-speech capabilities provided by the application.", ".dockerignore": "### Building and Running Docker with `.dockerignore` in Coqui-AI/TTS\n\n#### Overview of `.dockerignore`\nThe `.dockerignore` file is used to specify which files and directories should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, improving build performance and security by preventing unnecessary files from being included in the Docker image.\n\n#### Contents of `.dockerignore`\nThe provided `.dockerignore` file includes the following entries:\n\n- **`.git/`**: Excludes the entire Git repository, preventing version control files from being included.\n- **`Dockerfile`**: Excludes the Dockerfile itself from the build context.\n- **`build/`**: Excludes the build directory, which may contain compiled artifacts.\n- **`dist/`**: Excludes the distribution directory, typically used for packaged releases.\n- **`TTS.egg-info/`**: Excludes metadata files generated by Python package installations.\n- **`tests/outputs/*`**: Excludes all output files from the tests directory.\n- **`tests/train_outputs/*`**: Excludes all training output files from the tests directory.\n- **`__pycache__/`**: Excludes Python cache files, which are not needed for the build.\n- **`*.pyc`**: Excludes all compiled Python files.\n\n#### Actionable Steps to Build and Run Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/coqui-ai/TTS.git\n   cd TTS\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image, ensuring you are in the root directory of the cloned repository:\n   ```bash\n   docker build -t coqui-tts .\n   ```\n\n3. **Run the Docker Container**:\n   Execute the following command to run the Docker container interactively, mapping port 5002:\n   ```bash\n   docker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\n   ```\n\n4. **List Available Models**:\n   Inside the running container, use the following command to list available models:\n   ```bash\n   python3 TTS/server/server.py --list_models\n   ```\n\n5. **Start the TTS Server**:\n   To start the TTS server with a specific model, run:\n   ```bash\n   python3 TTS/server/server.py --model_name tts_models/en/vctk/vits\n   ```\n\nBy following these steps, you will successfully build and run the Docker container for the Coqui-AI TTS project while adhering to the exclusions specified in the `.dockerignore` file."}}, "timestamp": "2025-10-30T20:53:13.163701"}
{"repo_name": "mem0ai/mem0", "stars": 42262, "language": "Python", "tasks": [{"task_title": "ä¸AIèŠå¤©å¹¶ä½¿ç”¨è®°å¿†", "task_description": "è¯¥ä»»åŠ¡å…è®¸ç”¨æˆ·ä¸AIè¿›è¡Œå¯¹è¯ï¼ŒAIä¼šæ ¹æ®ç”¨æˆ·çš„å†å²è®°å¿†ç”Ÿæˆå›ç­”ï¼Œå¹¶åœ¨å¯¹è¯åå°†æ–°ä¿¡æ¯æ·»åŠ åˆ°è®°å¿†ä¸­ã€‚", "example_code": "from openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()", "running_command": null, "expected_input": "Hello, how are you?", "expected_output": "AI: I'm doing well, thank you! How can I assist you today?"}], "setup": {"setup_commands": ["pip install mem0ai", "npm install mem0ai"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:54:02.664507"}
{"repo_name": "virattt/ai-hedge-fund", "stars": 42138, "language": "Python", "tasks": [{"task_title": "è¿è¡Œä¸»ç¨‹åºè·å–è‚¡ç¥¨æ•°æ®", "task_description": "ä½¿ç”¨æŒ‡å®šçš„è‚¡ç¥¨ä»£ç ï¼ˆå¦‚AAPL, MSFT, NVDAï¼‰è¿è¡Œä¸»ç¨‹åºä»¥è·å–ç›¸å…³çš„é‡‘èæ•°æ®ã€‚", "example_code": null, "running_command": "poetry run python src/main.py --ticker AAPL,MSFT,NVDA", "expected_input": "AAPL, MSFT, NVDA", "expected_output": "è·å–åˆ°çš„è‚¡ç¥¨æ•°æ®"}, {"task_title": "ä½¿ç”¨Ollamaè¿è¡Œä¸»ç¨‹åº", "task_description": "åœ¨è·å–è‚¡ç¥¨æ•°æ®æ—¶å¯ç”¨OllamaåŠŸèƒ½ï¼Œä½¿ç”¨æŒ‡å®šçš„è‚¡ç¥¨ä»£ç ã€‚", "example_code": null, "running_command": "poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama", "expected_input": "AAPL, MSFT, NVDA", "expected_output": "è·å–åˆ°çš„è‚¡ç¥¨æ•°æ®ä¸Ollamaç›¸å…³çš„è¾“å‡º"}, {"task_title": "æŒ‡å®šæ—¶é—´èŒƒå›´è·å–è‚¡ç¥¨æ•°æ®", "task_description": "è¿è¡Œä¸»ç¨‹åºä»¥è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨è‚¡ç¥¨ä»£ç å’Œå¼€å§‹ã€ç»“æŸæ—¥æœŸã€‚", "example_code": null, "running_command": "poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01", "expected_input": "AAPL, MSFT, NVDA, 2024-01-01, 2024-03-01", "expected_output": "2024å¹´1æœˆ1æ—¥è‡³2024å¹´3æœˆ1æ—¥çš„è‚¡ç¥¨æ•°æ®"}, {"task_title": "è¿è¡Œå›æµ‹ç¨‹åº", "task_description": "ä½¿ç”¨æŒ‡å®šçš„è‚¡ç¥¨ä»£ç è¿è¡Œå›æµ‹ç¨‹åºä»¥æµ‹è¯•äº¤æ˜“ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚", "example_code": null, "running_command": "poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA", "expected_input": "AAPL, MSFT, NVDA", "expected_output": "å›æµ‹ç»“æœå’Œç­–ç•¥è¡¨ç°"}], "setup": {"setup_commands": ["git clone https://github.com/virattt/ai-hedge-fund.git\ncd ai-hedge-fund", "curl -sSL https://install.python-poetry.org | python3 -", "poetry install"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` from `virattt/ai-hedge-fund`\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the Docker image and speeds up the build process by preventing unnecessary files from being sent to the Docker daemon. The semantic meaning of each entry in the provided `.dockerignore` file is as follows:\n\n1. **Git-related files**:\n   - `.git` and `.gitignore`: Exclude version control metadata to keep the image clean and avoid unnecessary bloat.\n\n2. **Python environment and cache**:\n   - `.venv`: Exclude virtual environment directories to avoid including local dependencies.\n   - `__pycache__/`, `*.py[cod]`, `*$py.class`, `.pytest_cache/`: Exclude compiled Python files and cache directories that are not needed for deployment.\n\n3. **Environment variables**:\n   - `.env`: Exclude environment variable files to prevent sensitive information from being included in the image.\n\n4. **IDE and editor files**:\n   - `.idea/`, `.vscode/`, `*.swp`, `*.swo`: Exclude configuration files and temporary files created by IDEs and text editors.\n\n5. **Logs and data**:\n   - `logs/`, `data/`, `*.log`: Exclude log files and data directories that are not necessary for the application to run.\n\n6. **Operating System specific files**:\n   - `.DS_Store`, `Thumbs.db`: Exclude OS-specific metadata files that do not contribute to the application.\n\n#### Building and Running Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/virattt/ai-hedge-fund.git\n   cd ai-hedge-fund\n   ```\n\n2. **Build the Docker Image**:\n   Use the following command to build the Docker image. Replace `your-image-name` with your desired image name.\n   ```bash\n   docker build -t your-image-name .\n   ```\n\n3. **Run the Docker Container**:\n   After building the image, run the container using:\n   ```bash\n   docker run -d --name your-container-name your-image-name\n   ```\n\n4. **Check Running Containers**:\n   To verify that your container is running, use:\n   ```bash\n   docker ps\n   ```\n\n5. **Stop and Remove the Container** (if needed):\n   To stop and remove the container, use:\n   ```bash\n   docker stop your-container-name\n   docker rm your-container-name\n   ```\n\n#### Note:\n- Ensure that Docker is installed and running on your machine before executing these commands.\n- Review the repository's README for any specific Docker commands or configurations that may be required for your application."}}, "timestamp": "2025-10-30T20:54:31.448542"}
{"repo_name": "streamlit/streamlit", "stars": 42011, "language": "Python", "tasks": [{"task_title": "Slider for value selection", "task_description": "åˆ›å»ºä¸€ä¸ªæ»‘å—ä¾›ç”¨æˆ·é€‰æ‹©ä¸€ä¸ªå€¼ï¼Œå¹¶è®¡ç®—è¯¥å€¼çš„å¹³æ–¹ã€‚", "example_code": "import streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)", "running_command": null, "expected_input": "ä»»æ„æ•°å€¼ï¼ˆé€šè¿‡æ»‘å—é€‰æ‹©ï¼‰", "expected_output": "x squared is x * x"}], "setup": {"setup_commands": ["$ pip install streamlit\n$ streamlit hello"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:54:40.579009"}
{"repo_name": "zai-org/ChatGLM-6B", "stars": 41149, "language": "Python", "tasks": [{"task_title": "åŠ è½½æ¨¡å‹", "task_description": "ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ChatGLM-6Bï¼Œå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚", "example_code": "from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\nmodel = model.eval()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "ä¸æ¨¡å‹å¯¹è¯", "task_description": "ä½¿ç”¨åŠ è½½çš„æ¨¡å‹ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œå¹¶è¿”å›å“åº”ã€‚", "example_code": "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)", "running_command": null, "expected_input": "ä½ å¥½", "expected_output": "ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"}, {"task_title": "ç»§ç»­å¯¹è¯", "task_description": "åœ¨ä¸æ¨¡å‹çš„å¯¹è¯ä¸­ï¼Œä½¿ç”¨å†å²è®°å½•ç»§ç»­æé—®å¹¶è·å–å“åº”ã€‚", "example_code": "response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\nprint(response)", "running_command": null, "expected_input": "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", "expected_output": "æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n\n1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚\n3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚\n5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚"}, {"task_title": "é‡åŒ–æ¨¡å‹", "task_description": "åŠ è½½é‡åŒ–çš„æ¨¡å‹ä»¥å‡å°‘å†…å­˜å ç”¨ï¼Œæ”¯æŒ4/8 bité‡åŒ–ã€‚", "example_code": "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).quantize(8).half().cuda()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "ä½¿ç”¨INT8é‡åŒ–æ¨¡å‹", "task_description": "åŠ è½½INT8é‡åŒ–çš„æ¨¡å‹ä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚", "example_code": "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "åœ¨æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹", "task_description": "ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹å¹¶è®¾ç½®ä¸ºåŠç²¾åº¦ã€‚", "example_code": "model = AutoModel.from_pretrained(\"your local path\", trust_remote_code=True).half().to('mps')", "running_command": null, "expected_input": null, "expected_output": null}, {"task_title": "åœ¨å¤šä¸ªGPUä¸ŠåŠ è½½æ¨¡å‹", "task_description": "ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°åœ¨å¤šä¸ªGPUä¸ŠåŠ è½½æ¨¡å‹ä»¥æé«˜æ€§èƒ½ã€‚", "example_code": "from utils import load_model_on_gpus\nmodel = load_model_on_gpus(\"THUDM/chatglm-6b\", num_gpus=2)", "running_command": null, "expected_input": null, "expected_output": null}], "setup": {"setup_commands": ["git clone https://huggingface.co/THUDM/chatglm-6b", "GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b", "git clone https://github.com/THUDM/ChatGLM-6B\ncd ChatGLM-6B", "curl -X POST \"http://127.0.0.1:8000\" \\\n     -H 'Content-Type: application/json' \\\n     -d '{\"prompt\": \"ä½ å¥½\", \"history\": []}'"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:55:20.980619"}
{"repo_name": "psf/black", "stars": 41105, "language": "Python", "tasks": [{"task_title": "æ ¼å¼åŒ–Pythonä»£ç ", "task_description": "ä½¿ç”¨blackåº“æ ¼å¼åŒ–æŒ‡å®šçš„Pythonæºæ–‡ä»¶æˆ–ç›®å½•ã€‚", "example_code": null, "running_command": "black {source_file_or_directory}", "expected_input": "path/to/your/python_file.py", "expected_output": "Formatted code in the specified file."}, {"task_title": "æ ¼å¼åŒ–Pythonä»£ç ï¼ˆæ¨¡å—æ–¹å¼ï¼‰", "task_description": "é€šè¿‡Pythonæ¨¡å—çš„æ–¹å¼ä½¿ç”¨blackåº“æ ¼å¼åŒ–æŒ‡å®šçš„Pythonæºæ–‡ä»¶æˆ–ç›®å½•ã€‚", "example_code": null, "running_command": "python -m black {source_file_or_directory}", "expected_input": "path/to/your/python_file.py", "expected_output": "Formatted code in the specified file."}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for the `psf/black` Repository\n\n#### Overview of the Dockerfile\n\nThe provided Dockerfile is designed to create a Docker image for the `psf/black` project, a code formatter for Python. It employs a multi-stage build process to optimize the final image size by separating the build environment from the runtime environment.\n\n#### Semantic Breakdown of the Dockerfile\n\n1. **Base Image**: \n   - `FROM python:3.12-slim AS builder`: This line specifies the base image as a lightweight version of Python 3.12, setting up an environment for building the application.\n\n2. **Directory Creation**:\n   - `RUN mkdir /src`: Creates a directory `/src` in the container where the application code will reside.\n\n3. **Code Copying**:\n   - `COPY . /src/`: Copies the entire content of the current directory (the application code) into the `/src` directory in the container.\n\n4. **Environment Variables**:\n   - `ENV VIRTUAL_ENV=/opt/venv`: Sets the virtual environment path.\n   - `ENV HATCH_BUILD_HOOKS_ENABLE=1`: Enables build hooks for the Hatch tool, which is used for building Python packages.\n\n5. **Build Tools Installation**:\n   - `RUN apt update && apt install -y build-essential git python3-dev`: Installs necessary build tools and dependencies required to compile the application.\n\n6. **Virtual Environment Setup**:\n   - `RUN python -m venv $VIRTUAL_ENV`: Creates a virtual environment in the specified path.\n\n7. **Dependency Installation**:\n   - `RUN python -m pip install --no-cache-dir hatch hatch-fancy-pypi-readme hatch-vcs`: Installs Hatch and its related tools without caching to reduce image size.\n   - `RUN . /opt/venv/bin/activate && pip install --no-cache-dir --upgrade pip setuptools`: Activates the virtual environment and upgrades pip and setuptools.\n   - `&& cd /src && hatch build -t wheel`: Navigates to the source directory and builds the package into a wheel format.\n   - `&& pip install --no-cache-dir dist/*-cp*`: Installs the built package from the dist directory.\n   - `&& pip install black[colorama,d,uvloop]`: Installs the `black` formatter with additional optional dependencies.\n\n8. **Final Image Setup**:\n   - `FROM python:3.12-slim`: Starts a new stage with a clean, slim Python image for the final runtime environment.\n   - `COPY --from=builder /opt/venv /opt/venv`: Copies the virtual environment from the builder stage to the final image.\n   - `ENV PATH=\"/opt/venv/bin:$PATH\"`: Updates the PATH environment variable to include the virtual environment's binaries.\n\n9. **Command Execution**:\n   - `CMD [\"/opt/venv/bin/black\"]`: Specifies the default command to run when the container starts, which is the `black` formatter.\n\n#### Instructions to Build and Run\n\n1. **Build the Docker Image**:\n   Open a terminal and navigate to the directory containing the Dockerfile. Run the following command to build the Docker image:\n   ```bash\n   docker build -t black-formatter .\n   ```\n\n2. **Run the Docker Container**:\n   After the image is built, you can run the container with the following command:\n   ```bash\n   docker run --rm -v $(pwd):/workspace black-formatter /workspace\n   ```\n   This command mounts the current directory to `/workspace` in the container, allowing `black` to format the Python files in your project.\n\n#### Note\nEnsure that Docker is installed and running on your machine before executing these commands."}}, "timestamp": "2025-10-30T20:55:45.152170"}
{"repo_name": "deepspeedai/DeepSpeed", "stars": 40560, "language": "Python", "tasks": [{"task_title": "Generate DeepSpeed Report", "task_description": "ä½¿ç”¨DeepSpeedç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„è¯¦ç»†æŠ¥å‘Šï¼ŒåŒ…æ‹¬æ€§èƒ½æŒ‡æ ‡å’Œé…ç½®å‚æ•°ã€‚", "example_code": "import deepspeed\n\ndsr = deepspeed.utils.DSReport()\ndsr.generate()", "running_command": null, "expected_input": null, "expected_output": "DeepSpeed Report: ... (åŒ…å«æ¨¡å‹æ€§èƒ½å’Œé…ç½®çš„è¯¦ç»†ä¿¡æ¯)"}], "setup": {"setup_commands": ["pip install deepspeed"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:55:58.008064"}
{"repo_name": "gradio-app/gradio", "stars": 40365, "language": "Python", "tasks": [{"task_title": "é—®å€™ç”¨æˆ·", "task_description": "åˆ›å»ºä¸€ä¸ªGradioæ¥å£ï¼Œç”¨äºæ ¹æ®ç”¨æˆ·è¾“å…¥çš„åå­—å’Œå¼ºåº¦é—®å€™ç”¨æˆ·ã€‚å¼ºåº¦å†³å®šé—®å€™è¯­çš„é‡å¤æ¬¡æ•°ã€‚", "example_code": "import gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()", "running_command": null, "expected_input": {"name": "Alice", "intensity": 3}, "expected_output": "Hello, Alice!Hello, Alice!Hello, Alice!"}, {"task_title": "ç®€å•é—®å€™", "task_description": "åˆ›å»ºä¸€ä¸ªGradioæ¥å£ï¼Œç”¨äºæ ¹æ®ç”¨æˆ·è¾“å…¥çš„åå­—è¿›è¡Œç®€å•é—®å€™ã€‚", "example_code": "import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)", "running_command": null, "expected_input": "Bob", "expected_output": "Hello Bob!"}], "setup": {"setup_commands": ["pip install --upgrade gradio"], "docker_commands": [], "has_docker_files": true, "docker_setup_descriptions": {".dockerignore": "### Building and Running Docker with `.dockerignore` from `gradio-app/gradio`\n\n#### Overview of `.dockerignore`\n\nThe `.dockerignore` file is used to specify files and directories that should be excluded from the Docker build context. This helps to reduce the size of the context sent to the Docker daemon, improving build performance and security by preventing sensitive or unnecessary files from being included in the Docker image.\n\n#### Semantic Meaning of the Entries\n\n1. **Python Build Artifacts**:\n   - Excludes Python egg files and build directories (`.eggs/`, `dist/`, `build/`) to avoid packaging unnecessary files.\n   - Allows specific metadata files from `gradio.egg-info/` to be included, which are essential for package management.\n\n2. **JavaScript Build Artifacts**:\n   - Excludes static files and CDN resources in `gradio/templates/frontend/` to keep the image clean.\n\n3. **Secrets**:\n   - Excludes `.env` files to prevent sensitive environment variables from being included in the image.\n\n4. **Gradio Run Artifacts**:\n   - Excludes database files and other runtime artifacts (`*.db`, `*.sqlite3`, `gradio/launches.json`, `gradio/hash_seed.txt`) to avoid bloating the image with transient data.\n\n5. **Tests**:\n   - Excludes test coverage reports and related files to streamline the build for production.\n\n6. **Demos**:\n   - Excludes demo-related files that are not needed for the application to run.\n\n7. **Miscellaneous**:\n   - Excludes IDE configurations and system files like `.idea/*`, `.DS_Store`, and backup files to keep the repository clean.\n\n#### Steps to Build and Run Docker\n\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/gradio-app/gradio.git\n   cd gradio\n   ```\n\n2. **Build the Docker Image**:\n   Ensure you are in the root of the repository where the `Dockerfile` is located, then run:\n   ```bash\n   docker build -t gradio-app .\n   ```\n\n3. **Run the Docker Container**:\n   After building the image, you can run it using:\n   ```bash\n   docker run -p 7860:7860 gradio-app\n   ```\n\n4. **Access the Application**:\n   Open your web browser and navigate to `http://localhost:7860` to access the Gradio application.\n\nBy following these steps and understanding the purpose of the `.dockerignore` file, you can efficiently build and run the Gradio application in a Docker container."}}, "timestamp": "2025-10-30T20:56:25.914968"}
{"repo_name": "crewAIInc/crewAI", "stars": 39884, "language": "Python", "tasks": [{"task_title": "åˆ›å»ºå›¢é˜Ÿ", "task_description": "ä½¿ç”¨crewAIå‘½ä»¤è¡Œå·¥å…·åˆ›å»ºä¸€ä¸ªæ–°å›¢é˜Ÿï¼ŒæŒ‡å®šé¡¹ç›®åç§°ã€‚", "example_code": null, "running_command": "crewai create crew latest-ai-development", "expected_input": "latest-ai-development", "expected_output": "å›¢é˜Ÿ 'latest-ai-development' åˆ›å»ºæˆåŠŸ"}], "setup": {"setup_commands": ["pip install crewai", "pip install 'crewai[tools]'"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:56:35.508466"}
{"repo_name": "google-research/bert", "stars": 39616, "language": "Python", "tasks": [{"task_title": "MRPC Classification Training", "task_description": "è®­ç»ƒä¸€ä¸ªç”¨äºMRPCä»»åŠ¡çš„BERTåˆ†ç±»å™¨ï¼Œè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚", "example_code": null, "running_command": "python run_classifier.py --task_name=MRPC --do_train=true --do_eval=true --data_dir=$GLUE_DIR/MRPC --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=/tmp/mrpc_output/", "expected_input": null, "expected_output": null}, {"task_title": "MRPC Classification Prediction", "task_description": "ä½¿ç”¨å·²è®­ç»ƒçš„åˆ†ç±»å™¨å¯¹MRPCä»»åŠ¡è¿›è¡Œé¢„æµ‹ã€‚", "example_code": null, "running_command": "python run_classifier.py --task_name=MRPC --do_predict=true --data_dir=$GLUE_DIR/MRPC --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$TRAINED_CLASSIFIER --max_seq_length=128 --output_dir=/tmp/mrpc_output/", "expected_input": null, "expected_output": null}, {"task_title": "SQuAD Training and Prediction", "task_description": "åœ¨SQuADæ•°æ®é›†ä¸Šè®­ç»ƒå’Œé¢„æµ‹é—®ç­”æ¨¡å‹ã€‚", "example_code": null, "running_command": "python run_squad.py --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v1.1.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v1.1.json --train_batch_size=12 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=/tmp/squad_base/", "expected_input": null, "expected_output": null}, {"task_title": "SQuAD Evaluation", "task_description": "è¯„ä¼°SQuADæ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚", "example_code": null, "running_command": "python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json", "expected_input": null, "expected_output": "{\"f1\": 88.41249612335034, \"exact_match\": 81.2488174077578}"}, {"task_title": "SQuAD Large Training and Prediction", "task_description": "åœ¨SQuADæ•°æ®é›†ä¸Šä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚", "example_code": null, "running_command": "python run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v1.1.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v1.1.json --train_batch_size=24 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://some_bucket/squad_large/ --use_tpu=True --tpu_name=$TPU_NAME", "expected_input": null, "expected_output": "{\"f1\": 90.87081895814865, \"exact_match\": 84.38978240302744}"}, {"task_title": "SQuAD v2.0 Training and Prediction", "task_description": "åœ¨SQuAD v2.0æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ï¼Œæ”¯æŒè´Ÿæ ·æœ¬ã€‚", "example_code": null, "running_command": "python run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v2.0.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v2.0.json --train_batch_size=24 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://some_bucket/squad_large/ --use_tpu=True --tpu_name=$TPU_NAME --version_2_with_negative=True", "expected_input": null, "expected_output": null}, {"task_title": "SQuAD v2.0 Inference with Negative Samples", "task_description": "åœ¨SQuAD v2.0æ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†ï¼Œæ”¯æŒè´Ÿæ ·æœ¬ï¼Œå¹¶è®¾ç½®é˜ˆå€¼ã€‚", "example_code": null, "running_command": "python run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt --do_train=False --train_file=$SQUAD_DIR/train-v2.0.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v2.0.json --train_batch_size=24 --learning_rate=3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://some_bucket/squad_large/ --use_tpu=True --tpu_name=$TPU_NAME --version_2_with_negative=True --null_score_diff_threshold=$THRESH", "expected_input": null, "expected_output": null}, {"task_title": "Feature Extraction", "task_description": "ä»è¾“å…¥æ–‡æœ¬ä¸­æå–ç‰¹å¾å¹¶ä¿å­˜ä¸ºJSONLæ ¼å¼ã€‚", "example_code": "echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt\n\npython extract_features.py --input_file=/tmp/input.txt --output_file=/tmp/output.jsonl --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --layers=-1,-2,-3,-4 --max_seq_length=128 --batch_size=8", "running_command": null, "expected_input": "Who was Jim Henson ? ||| Jim Henson was a puppeteer", "expected_output": null}, {"task_title": "Create Pretraining Data", "task_description": "åˆ›å»ºç”¨äºé¢„è®­ç»ƒçš„TFRecordæ•°æ®ã€‚", "example_code": null, "running_command": "python create_pretraining_data.py --input_file=./sample_text.txt --output_file=/tmp/tf_examples.tfrecord --vocab_file=$BERT_BASE_DIR/vocab.txt --do_lower_case=True --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5", "expected_input": null, "expected_output": null}, {"task_title": "Run Pretraining", "task_description": "è¿è¡ŒBERTæ¨¡å‹çš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚", "example_code": null, "running_command": "python run_pretraining.py --input_file=/tmp/tf_examples.tfrecord --output_dir=/tmp/pretraining_output --do_train=True --do_eval=True --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt --train_batch_size=32 --max_seq_length=128 --max_predictions_per_seq=20 --num_train_steps=20 --num_warmup_steps=10 --learning_rate=2e-5", "expected_input": null, "expected_output": null}], "setup": {"setup_commands": [], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:57:27.936007"}
{"repo_name": "zhayujie/chatgpt-on-wechat", "stars": 39530, "language": "Python", "tasks": [{"task_title": "å¯åŠ¨åº”ç”¨", "task_description": "è¿è¡Œä¸»åº”ç”¨ç¨‹åºï¼Œå¯åŠ¨èŠå¤©æœºå™¨äººæœåŠ¡ã€‚", "example_code": null, "running_command": "python3 app.py", "expected_input": null, "expected_output": "åº”ç”¨å¯åŠ¨æ—¥å¿—ï¼Œæ˜¾ç¤ºæœåŠ¡å·²å¯åŠ¨"}, {"task_title": "åå°è¿è¡Œåº”ç”¨", "task_description": "åœ¨åå°å¯åŠ¨åº”ç”¨ç¨‹åºï¼Œå¹¶å®æ—¶æŸ¥çœ‹æ—¥å¿—è¾“å‡ºã€‚", "example_code": null, "running_command": "nohup python3 app.py & tail -f nohup.out", "expected_input": null, "expected_output": "å®æ—¶æ—¥å¿—è¾“å‡ºï¼Œæ˜¾ç¤ºåº”ç”¨è¿è¡ŒçŠ¶æ€"}, {"task_title": "æŸ¥çœ‹Dockeræ—¥å¿—", "task_description": "æŸ¥çœ‹è¿è¡Œåœ¨Dockerä¸­çš„åº”ç”¨çš„æ—¥å¿—ï¼Œå®æ—¶è¾“å‡ºã€‚", "example_code": null, "running_command": "sudo docker logs -f chatgpt-on-wechat", "expected_input": null, "expected_output": "å®æ—¶æ—¥å¿—è¾“å‡ºï¼Œæ˜¾ç¤ºDockerå®¹å™¨çŠ¶æ€"}], "setup": {"setup_commands": ["bash <(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)", "git clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/", "pip3 install -r requirements.txt"], "docker_commands": ["wget https://cdn.link-ai.tech/code/cow/docker-compose.yml", "sudo docker compose up -d         # è‹¥docker-composeä¸º 1.X ç‰ˆæœ¬ï¼Œåˆ™æ‰§è¡Œ `sudo  docker-compose up -d`"], "has_docker_files": true, "docker_setup_descriptions": {"Dockerfile": "### Building and Running Docker for zhayujie/chatgpt-on-wechat\n\nTo build and run the Docker container using the provided `Dockerfile` from the `zhayujie/chatgpt-on-wechat` repository, follow these steps:\n\n1. **Understand the Dockerfile**:\n   - The `Dockerfile` specifies that the base image for the container is `ghcr.io/zhayujie/chatgpt-on-wechat:latest`. This means it will pull the latest version of the `chatgpt-on-wechat` image from the GitHub Container Registry.\n   - The `ENTRYPOINT` directive sets the command that will be executed when the container starts. In this case, it runs the script located at `/entrypoint.sh`, which likely initializes the application.\n\n2. **Download the Docker Compose File**:\n   Execute the following command to download the Docker Compose configuration:\n   ```bash\n   wget https://cdn.link-ai.tech/code/cow/docker-compose.yml\n   ```\n\n3. **Run the Docker Container**:\n   Use the following command to start the container in detached mode:\n   ```bash\n   sudo docker compose up -d\n   ```\n   - If you are using Docker Compose version 1.X, replace the command with:\n   ```bash\n   sudo docker-compose up -d\n   ```\n\n### Summary\n- The `Dockerfile` pulls a pre-built image and specifies the entry point for the application.\n- Download the Docker Compose file and use it to run the application in a containerized environment."}}, "timestamp": "2025-10-30T20:57:58.617098"}
{"repo_name": "lm-sys/FastChat", "stars": 39198, "language": "Python", "tasks": [{"task_title": "å¯åŠ¨æ§åˆ¶å™¨", "task_description": "å¯åŠ¨FastChatçš„æ§åˆ¶å™¨æœåŠ¡ï¼Œç”¨äºç®¡ç†æ¨¡å‹å’Œè¯·æ±‚ã€‚", "example_code": null, "running_command": "python3 -m fastchat.serve.controller", "expected_input": null, "expected_output": "æ§åˆ¶å™¨æœåŠ¡æ­£åœ¨è¿è¡Œ"}, {"task_title": "å¯åŠ¨æ¨¡å‹å·¥ä½œè€…", "task_description": "å¯åŠ¨FastChatçš„æ¨¡å‹å·¥ä½œè€…ï¼ŒæŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„ã€‚", "example_code": null, "running_command": "python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5", "expected_input": null, "expected_output": "æ¨¡å‹å·¥ä½œè€…å·²åŠ è½½æ¨¡å‹ lmsys/vicuna-7b-v1.5"}, {"task_title": "æµ‹è¯•æ¶ˆæ¯", "task_description": "ä½¿ç”¨æŒ‡å®šæ¨¡å‹åç§°æµ‹è¯•æ¶ˆæ¯ä¼ é€’åŠŸèƒ½ã€‚", "example_code": null, "running_command": "python3 -m fastchat.serve.test_message --model-name vicuna-7b-v1.5", "expected_input": null, "expected_output": "æµ‹è¯•æ¶ˆæ¯æˆåŠŸï¼Œè¿”å›å“åº”"}, {"task_title": "å¯åŠ¨Gradio WebæœåŠ¡å™¨", "task_description": "å¯åŠ¨Gradio WebæœåŠ¡å™¨ä»¥æä¾›ç”¨æˆ·ç•Œé¢ã€‚", "example_code": null, "running_command": "python3 -m fastchat.serve.gradio_web_server", "expected_input": null, "expected_output": "Gradio WebæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ"}, {"task_title": "å¯åŠ¨å¤šå®ä¾‹Gradio WebæœåŠ¡å™¨", "task_description": "å¯åŠ¨å¤šä¸ªGradio WebæœåŠ¡å™¨å®ä¾‹ä»¥å¤„ç†å¹¶å‘è¯·æ±‚ã€‚", "example_code": null, "running_command": "python3 -m fastchat.serve.gradio_web_server_multi", "expected_input": null, "expected_output": "å¤šä¸ªGradio WebæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ"}, {"task_title": "è®­ç»ƒæ¨¡å‹", "task_description": "ä½¿ç”¨æŒ‡å®šå‚æ•°è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬æ•°æ®è·¯å¾„ã€è®­ç»ƒå‘¨æœŸå’Œæ‰¹é‡å¤§å°ç­‰ã€‚", "example_code": null, "running_command": "torchrun --nproc_per_node=4 --master_port=20001 fastchat/train/train_mem.py --model_name_or_path meta-llama/Llama-2-7b-hf --data_path data/dummy_conversation.json --bf16 True --output_dir output_vicuna --num_train_epochs 3 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 16 --evaluation_strategy \"no\" --save_strategy \"steps\" --save_steps 1200 --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type \"cosine\" --logging_steps 1 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' --tf32 True --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True", "expected_input": null, "expected_output": "æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¾“å‡ºç»“æœä¿å­˜åœ¨ output_vicuna ç›®å½•"}], "setup": {"setup_commands": ["git clone https://github.com/lm-sys/FastChat.git\ncd FastChat", "brew install rust cmake"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:58:29.536691"}
{"repo_name": "NanmiCoder/MediaCrawler", "stars": 38413, "language": "Python", "tasks": [{"task_title": "ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º", "task_description": "æ­¤ä»»åŠ¡ä½¿ç”¨æŒ‡å®šçš„å¹³å°ï¼ˆå¦‚xhsï¼‰å’ŒäºŒç»´ç ç™»å½•æ–¹å¼ï¼Œä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯å¹¶çˆ¬å–ç›¸å…³çš„å¸–å­ä¿¡æ¯å’Œè¯„è®ºã€‚", "example_code": null, "running_command": "uv run main.py --platform xhs --lt qrcode --type search", "expected_input": "å…³é”®è¯", "expected_output": "çˆ¬å–åˆ°çš„å¸–å­ä¿¡æ¯ä¸è¯„è®º"}, {"task_title": "ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯", "task_description": "æ­¤ä»»åŠ¡ä½¿ç”¨æŒ‡å®šçš„å¹³å°ï¼ˆå¦‚xhsï¼‰å’ŒäºŒç»´ç ç™»å½•æ–¹å¼ï¼Œä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨ï¼Œå¹¶è·å–å¯¹åº”å¸–å­çš„è¯¦ç»†ä¿¡æ¯åŠè¯„è®ºã€‚", "example_code": null, "running_command": "uv run main.py --platform xhs --lt qrcode --type detail", "expected_input": "å¸–å­IDåˆ—è¡¨", "expected_output": "æŒ‡å®šå¸–å­çš„è¯¦ç»†ä¿¡æ¯ä¸è¯„è®º"}, {"task_title": "åˆå§‹åŒ– SQLite æ•°æ®åº“", "task_description": "æ­¤ä»»åŠ¡åˆå§‹åŒ–ä¸€ä¸ªSQLiteæ•°æ®åº“ä»¥å­˜å‚¨çˆ¬å–çš„æ•°æ®ï¼Œæ— éœ€å…¶ä»–å¯é€‰å‚æ•°ã€‚", "example_code": null, "running_command": "uv run main.py --init_db sqlite", "expected_input": null, "expected_output": "SQLiteæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ"}, {"task_title": "ä½¿ç”¨ SQLite å­˜å‚¨æ•°æ®", "task_description": "æ­¤ä»»åŠ¡åœ¨çˆ¬å–ç›¸å…³å¸–å­ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨SQLiteæ•°æ®åº“å­˜å‚¨çˆ¬å–çš„æ•°æ®ï¼Œæ¨èä¸ªäººç”¨æˆ·ä½¿ç”¨ã€‚", "example_code": null, "running_command": "uv run main.py --platform xhs --lt qrcode --type search --save_data_option sqlite", "expected_input": "å…³é”®è¯", "expected_output": "çˆ¬å–åˆ°çš„å¸–å­ä¿¡æ¯ä¸è¯„è®ºå­˜å‚¨åœ¨SQLiteæ•°æ®åº“ä¸­"}, {"task_title": "åˆå§‹åŒ– MySQL æ•°æ®åº“", "task_description": "æ­¤ä»»åŠ¡åˆå§‹åŒ–ä¸€ä¸ªMySQLæ•°æ®åº“ä»¥å­˜å‚¨çˆ¬å–çš„æ•°æ®ã€‚", "example_code": null, "running_command": "uv run main.py --init_db mysql", "expected_input": null, "expected_output": "MySQLæ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ"}, {"task_title": "ä½¿ç”¨ MySQL å­˜å‚¨æ•°æ®", "task_description": "æ­¤ä»»åŠ¡åœ¨çˆ¬å–ç›¸å…³å¸–å­ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨MySQLæ•°æ®åº“å­˜å‚¨çˆ¬å–çš„æ•°æ®ï¼Œé€‚é…å†å²æ›´æ–°ã€‚", "example_code": null, "running_command": "uv run main.py --platform xhs --lt qrcode --type search --save_data_option db", "expected_input": "å…³é”®è¯", "expected_output": "çˆ¬å–åˆ°çš„å¸–å­ä¿¡æ¯ä¸è¯„è®ºå­˜å‚¨åœ¨MySQLæ•°æ®åº“ä¸­"}], "setup": {"setup_commands": ["# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•\ncd MediaCrawler\n\n# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\n# æˆ‘çš„ python ç‰ˆæœ¬æ˜¯ï¼š3.9.6ï¼Œrequirements.txt ä¸­çš„åº“æ˜¯åŸºäºè¿™ä¸ªç‰ˆæœ¬çš„\n# å¦‚æœæ˜¯å…¶ä»– python ç‰ˆæœ¬ï¼Œå¯èƒ½ requirements.txt ä¸­çš„åº“ä¸å…¼å®¹ï¼Œéœ€è‡ªè¡Œè§£å†³\npython -m venv venv\n\n# macOS & Linux æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ\nsource venv/bin/activate\n\n# Windows æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ\nvenv\\Scripts\\activate", "pip install -r requirements.txt"], "docker_commands": [], "has_docker_files": false, "docker_setup_descriptions": null}, "timestamp": "2025-10-30T20:58:53.628339"}
