{
  "repo": {
    "full_name": "xtekky/gpt4free",
    "stars": 65456,
    "forks": 13713,
    "language": "Python",
    "description": "The official gpt4free repository | various collection of powerful language models | o4, o3 and deepseek r1, gpt-4.1, gemini 2.5",
    "topics": [
      "chatbot",
      "chatbots",
      "chatgpt",
      "chatgpt-4",
      "chatgpt-api",
      "chatgpt-free",
      "chatgpt4",
      "deepseek",
      "deepseek-api",
      "deepseek-r1",
      "gpt",
      "gpt-4",
      "gpt-4o",
      "gpt4",
      "gpt4-api",
      "language-model",
      "openai",
      "openai-api",
      "openai-chatgpt",
      "reverse-engineering"
    ],
    "url": "https://github.com/xtekky/gpt4free",
    "created_at": "2023-03-29T17:00:43Z",
    "updated_at": "2025-10-31T02:52:18Z"
  },
  "readme_file": "dataset/readmes/gpt4free_README.md",
  "task_file": "dataset/tasks/gpt4free_tasks.json",
  "num_tasks": 6,
  "num_setup_commands": 3,
  "num_docker_commands": 3,
  "has_docker_files": true,
  "docker_files_list": [
    "docker-compose.yml"
  ],
  "docker_setup_descriptions": {
    "docker-compose.yml": "To build and run the Docker container for the `gpt4free` service from the `xtekky/gpt4free` repository, follow these steps:\n\n### Purpose of the Image\nThe Docker image `hlohaus789/g4f:latest` is designed to provide a lightweight environment for running the `gpt4free` application, which is likely a service that interfaces with the GPT-4 model. This image encapsulates all necessary dependencies and configurations needed to operate the application efficiently.\n\n### Building the Image\n1. **Set Up Your Environment**: Ensure you have Docker and Docker Compose installed on your machine. You can verify this by running `docker --version` and `docker-compose --version`.\n\n2. **Clone the Repository**: Clone the `xtekky/gpt4free` repository to your local machine using Git:\n   ```bash\n   git clone https://github.com/xtekky/gpt4free.git\n   cd gpt4free\n   ```\n\n3. **Prepare Directories**: Create the necessary directories for storing HAR files and generated media. This can be done with the following commands:\n   ```bash\n   mkdir -p har_and_cookies generated_media\n   ```\n\n4. **Set Permissions**: Ensure that the created directories have the correct permissions:\n   ```bash\n   chown -R 1000:1000 har_and_cookies generated_media\n   ```\n\n5. **Build the Docker Image**: Use Docker Compose to build the image defined in the `docker-compose.yml` file. This will compile the application and its dependencies as specified in the Dockerfile referenced in the compose file:\n   ```bash\n   docker-compose build\n   ```\n\n### Running the Container\n1. **Start the Service**: After the build completes successfully, you can start the service using Docker Compose. This will launch the container with the specified configurations, including memory allocation and port mappings:\n   ```bash\n   docker-compose up\n   ```\n\n2. **Access the Application**: Once the container is running, you can access the `gpt4free` service through your web browser or API client at `http://localhost:8080`. The application will also be available at `http://localhost:1337` and `http://localhost:7900` for additional functionalities.\n\n### Environment Configuration\nThe container is configured to use shared memory size of 2GB, which is essential for handling large data processes efficiently. The environment variable `OLLAMA_HOST` is set to `host.docker.internal`, allowing the container to communicate with services running on the host machine.\n\nThis setup provides a robust environment for running the `gpt4free` application, ensuring that all dependencies are managed within the Docker ecosystem, facilitating easier deployment and scalability."
  },
  "processed_at": "2025-10-30T23:35:05.854716"
}