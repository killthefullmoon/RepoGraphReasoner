{
  "repo": {
    "full_name": "unclecode/crawl4ai",
    "stars": 55247,
    "forks": 5520,
    "language": "Python",
    "description": "ðŸš€ðŸ¤– Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN",
    "topics": [],
    "url": "https://github.com/unclecode/crawl4ai",
    "created_at": "2024-05-09T09:48:50Z",
    "updated_at": "2025-10-31T03:19:39Z"
  },
  "readme_file": "dataset/readmes/crawl4ai_README.md",
  "task_file": "dataset/tasks/crawl4ai_tasks.json",
  "num_tasks": 13,
  "num_setup_commands": 9,
  "num_docker_commands": 1,
  "has_docker_files": true,
  "docker_files_list": [
    "Dockerfile",
    "docker-compose.yml"
  ],
  "docker_setup_descriptions": {
    "Dockerfile": "To build and run the Docker image for the \"Crawl4AI\" project from the repository `unclecode/crawl4ai`, follow these steps:\n\n### Building the Docker Image\n\n1. **Base Image**: The Docker image is built on top of a slim version of Python 3.12, ensuring a lightweight environment suitable for running Python applications.\n\n2. **Dependencies**: The image installs essential system packages and libraries required for the application, including:\n   - Development tools (e.g., `build-essential`, `git`, `cmake`)\n   - Libraries for image processing and web scraping (e.g., `libjpeg-dev`, `libglib2.0-0`, `libnss3`)\n   - Redis server for caching and data storage.\n   - Supervisor for process management.\n\n3. **Environment Variables**: Several environment variables are set to optimize the Python environment and configure the application, including settings for Redis connection and Python execution behavior.\n\n4. **User Creation**: A non-root user (`appuser`) is created for enhanced security, and the application is configured to run under this user.\n\n5. **Application Setup**: The image includes logic to install the application either from a local source or directly from the GitHub repository. It also installs necessary Python packages defined in a `requirements.txt` file, along with optional packages based on the installation type specified.\n\n6. **Health Check**: A health check is defined to ensure that the application has sufficient memory and that the Redis service is running.\n\n7. **Final Configuration**: The application is set to run using Supervisor, which manages the application processes and ensures they are restarted if they fail.\n\n### Running the Docker Container\n\n1. **Pull the Image**: Use the following command to pull the latest release of the Crawl4AI image from Docker Hub:\n   ```bash\n   docker pull unclecode/crawl4ai:latest\n   ```\n\n2. **Run the Container**: Start a new container from the pulled image, mapping the necessary ports and allocating shared memory:\n   ```bash\n   docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:latest\n   ```\n\n3. **Access the Application**: Once the container is running, you can access the application playground by navigating to `http://localhost:11235/playground` in your web browser.\n\n### Summary\n\nThe Crawl4AI Docker image is designed to provide a robust environment for running a web crawler and scraper that is compatible with large language models (LLMs). It includes all necessary dependencies, is optimized for security and performance, and is easy to deploy using Docker.",
    "docker-compose.yml": "To build and run the Docker image for the `crawl4ai` project from the `unclecode/crawl4ai` repository using the provided `docker-compose.yml` file, follow these steps:\n\n### Building the Docker Image\n\n1. **Clone the Repository**: Start by cloning the `unclecode/crawl4ai` repository to your local machine. This will give you access to the `docker-compose.yml` file and any other necessary resources.\n\n2. **Prepare Environment Variables**: Create a `.llm.env` file in the root of the cloned repository. This file should contain your API keys for various services (e.g., OpenAI, DeepSeek, Anthropic, etc.). You can use the `.llm.env.example` file as a template to set up your environment variables. Ensure that the keys are correctly populated.\n\n3. **Build the Image**: Navigate to the directory containing the `docker-compose.yml` file in your terminal. Use the Docker Compose command to build the image. The build process will pull the necessary base images and install dependencies required for the application, such as libraries for web serving and any specific tools needed for crawling AI-related tasks. The resulting image will be tagged as `unclecode/crawl4ai` with the specified version.\n\n### Running the Docker Container\n\n4. **Run the Container**: After successfully building the image, you can run the container using Docker Compose. The command will start the `crawl4ai` service, mapping the internal port 11235 to the same port on your host machine. This allows you to access the application via your web browser.\n\n5. **Access the Application**: Once the container is running, open your web browser and navigate to `http://localhost:11235/playground`. This will take you to the application's playground interface, where you can interact with the features provided by the `crawl4ai` service.\n\n### Additional Notes\n\n- **Resource Management**: The Docker Compose configuration includes resource limits to ensure the container does not exceed 4GB of memory, with a reservation of 1GB. This is important for maintaining performance, especially when handling large datasets or multiple requests.\n\n- **Health Checks**: The configuration includes a health check that periodically verifies if the service is running correctly. If the service fails, Docker will attempt to restart it automatically.\n\n- **User Permissions**: The container runs under a non-root user (`appuser`) for improved security.\n\nBy following these steps, you will have a fully operational instance of the `crawl4ai` application running in a Docker container, ready for use and experimentation."
  },
  "processed_at": "2025-10-30T23:42:08.450874"
}